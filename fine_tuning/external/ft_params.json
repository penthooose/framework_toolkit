{
  "0": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/first_stage/combined_datasets",
    "text_column": "input",
    "pre_eval": true,
    "eval_split": 0,
    "max_length": 3000,
    "model_path": "C:/Users/Paul/.cache/huggingface/hub/models--DiscoResearch--Llama3-DiscoLeo-Instruct-8B-v0.1/snapshots/0d7cf14a066a65f037d184d00eddc9d284225d93",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_instruct_stage1",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 3,
      "learning_rate": 0.00002,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_steps": 100,
      "max_grad_norm": 1.0,
      "eval_steps": 301,
      "save_steps": 100,
      "save_total_limit": 3,
      "lr_scheduler_type": "cosine",
      "weight_decay": 0.01,
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.05,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
      ]
    }
  },
  "1": {
    "mode": "unsupervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/unsupervised/mdb_datasets/combined_datasets",
    "text_column": "text",
    "pre_eval": true,
    "eval_split": 0,
    "max_length": 1100,
    "model_path": "C:/Users/Paul/.cache/huggingface/hub/models--DiscoResearch--Llama3-DiscoLeo-Instruct-8B-v0.1/snapshots/0d7cf14a066a65f037d184d00eddc9d284225d93",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_1",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 3,
      "learning_rate": 0.00002,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.03,
      "max_grad_norm": 0.3,
      "eval_steps": 200,
      "save_steps": 100,
      "weight_decay": 0.01
    },
    "peft_config": {
      "r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "2": {
    "mode": "unsupervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/unsupervised/single_chapters/combined_datasets",
    "use_checkpoint": true,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_2/checkpoint-2600",
    "text_column": "text",
    "pre_eval": false,
    "eval_split": 0,
    "max_length": 3000,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_1",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_2",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 3,
      "learning_rate": 0.00002,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.03,
      "max_grad_norm": 0.3,
      "eval_steps": 350,
      "save_steps": 200,
      "weight_decay": 0.01
    },
    "peft_config": {
      "r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w2"
      ]
    }
  },
  "3": {
    "mode": "unsupervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/unsupervised/multiple_chapters/combined_datasets",
    "text_column": "text",
    "pre_eval": true,
    "eval_split": 0,
    "max_length": 3100,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_2",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_3",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 4,
      "learning_rate": 0.00002,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.03,
      "max_grad_norm": 0.3,
      "eval_steps": 45,
      "save_steps": 40,
      "weight_decay": 0.01
    },
    "peft_config": {
      "r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.15,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "4": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format1/combined_datasets",
    "text_column": "input",
    "pre_eval": false,
    "eval_split": 0,
    "max_length": 3200,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 4,
      "save_total_limit": 3,
      "learning_rate": 0.000015,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.05,
      "max_grad_norm": 0.3,
      "eval_steps": 210,
      "save_steps": 200,
      "weight_decay": 0.01
    },
    "peft_config": {
      "r": 24,
      "lora_alpha": 48,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "5": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets",
    "use_checkpoint": true,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200",
    "text_column": "input",
    "pre_eval": false,
    "eval_split": 0,
    "max_length": 3200,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 5,
      "save_total_limit": 5,
      "learning_rate": 0.000015,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.08,
      "max_grad_norm": 0.3,
      "eval_steps": 210,
      "save_steps": 200,
      "weight_decay": 0.01,
      "lr_scheduler_type": "cosine_with_restarts",
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 24,
      "lora_alpha": 48,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "6": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format3/combined_datasets",
    "use_checkpoint": true,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_V2_1/checkpoint-1750",
    "text_column": "input",
    "pre_eval": false,
    "freeze_partly": true,
    "freeze_partly_layers": 16,
    "eval_split": 0,
    "max_length": 3200,
    "model_path": "C:/Users/Paul/.cache/huggingface/hub/models--DiscoResearch--Llama3-DiscoLeo-Instruct-8B-v0.1/snapshots/0d7cf14a066a65f037d184d00eddc9d284225d93",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_V2_1",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 4,
      "save_total_limit": 5,
      "learning_rate": 0.000012,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 12,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.08,
      "max_grad_norm": 0.3,
      "eval_steps": 175,
      "save_steps": 350,
      "weight_decay": 0.01,
      "lr_scheduler_type": "cosine_with_restarts",
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 32,
      "lora_alpha": 72,
      "lora_dropout": 0.08,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "7": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format4/combined_datasets",
    "use_checkpoint": false,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200",
    "text_column": "input",
    "pre_eval": true,
    "freeze_partly": true,
    "freeze_partly_layers": 20,
    "eval_split": 0,
    "max_length": 3200,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_supervised_V2_1",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_V2_2",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 3,
      "save_total_limit": 5,
      "learning_rate": 0.00001,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 12,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.08,
      "max_grad_norm": 0.3,
      "eval_steps": 175,
      "save_steps": 350,
      "weight_decay": 0.02,
      "lr_scheduler_type": "cosine_with_restarts",
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 32,
      "lora_alpha": 72,
      "lora_dropout": 0.12,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "8": {
    "mode": "unsupervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/unsupervised/mixed_chapters/combined_datasets",
    "use_checkpoint": false,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_5/checkpoint-3200",
    "text_column": "text",
    "pre_eval": false,
    "freeze_partly": true,
    "freeze_partly_layers": 22,
    "eval_split": 0,
    "max_length": 2900,
    "model_path": "C:/Users/Paul/.cache/huggingface/hub/models--DiscoResearch--Llama3-DiscoLeo-Instruct-8B-v0.1/snapshots/0d7cf14a066a65f037d184d00eddc9d284225d93",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_unsupervised_V1_1",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 7,
      "save_total_limit": 5,
      "learning_rate": 0.000025,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 6,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.06,
      "max_grad_norm": 0.3,
      "eval_steps": 500,
      "save_steps": 500,
      "weight_decay": 0.01,
      "lr_scheduler_type": "linear",
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 32,
      "lora_alpha": 64,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "9": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised/questions_and_answers/combined_datasets",
    "use_checkpoint": false,
    "checkpoint_path": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised/checkpoint-3200",
    "text_column": "input",
    "pre_eval": false,
    "freeze_partly": true,
    "freeze_partly_layers": 20,
    "eval_split": 0,
    "max_length": 3200,
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_V1_1",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_V1_2",
    "use_flash_attention": true,
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 5,
      "save_total_limit": 5,
      "learning_rate": 0.000022,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.06,
      "max_grad_norm": 0.3,
      "eval_steps": 300,
      "save_steps": 300,
      "weight_decay": 0.01,
      "lr_scheduler_type": "linear",
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 32,
      "lora_alpha": 64,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "10": {
    "mode": "mixed",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/mixed_coherence/combined_datasets",
    "text_column": ["text", "input"],
    "pre_eval": false,
    "eval_split": 0.05,
    "use_flash_attention": true,
    "max_length": 3100,
    "unfreeze_specific": true,
    "unfreeze_specific_layers": [
      8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25
    ],
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_V3",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_coherence",
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 1,
      "save_total_limit": 5,
      "learning_rate": 0.000005,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 12,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.15,
      "lr_scheduler_type": "constant_with_warmup",
      "max_grad_norm": 0.3,
      "save_steps": 125,
      "eval_steps": 50,
      "weight_decay": 0.005,
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 8,
      "lora_alpha": 16,
      "lora_dropout": 0.05,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "11": {
    "mode": "unsupervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/unsupervised_coherence/combined_datasets",
    "text_column": "text",
    "pre_eval": true,
    "eval_split": 0.05,
    "use_flash_attention": true,
    "max_length": 2900,
    "unfreeze_specific": true,
    "unfreeze_specific_layers": [
      8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25
    ],
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_V3",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_1",
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 1,
      "save_total_limit": 5,
      "learning_rate": 0.000005,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.15,
      "lr_scheduler_type": "constant_with_warmup",
      "max_grad_norm": 0.3,
      "save_steps": 100,
      "eval_steps": 50,
      "weight_decay": 0.005,
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 8,
      "lora_alpha": 16,
      "lora_dropout": 0.05,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  },
  "12": {
    "mode": "supervised",
    "data_path": "N:/Thesis/data_prepare/datasets_ready/supervised_coherence/combined_datasets",
    "text_column": "input",
    "pre_eval": true,
    "eval_split": 0.05,
    "use_flash_attention": true,
    "max_length": 3100,
    "unfreeze_specific": true,
    "unfreeze_specific_layers": [
      8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25
    ],
    "model_path": "C:/Users/Paul/.cache/merged_models/llama3_german_merged_V3_1",
    "output_dir": "C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2",
    "quantization_config": 8,
    "training_config": {
      "num_train_epochs": 1,
      "save_total_limit": 5,
      "learning_rate": 0.000005,
      "per_device_train_batch_size": 1,
      "gradient_accumulation_steps": 8,
      "eval_accumulation_steps": 4,
      "warmup_ratio": 0.15,
      "lr_scheduler_type": "constant_with_warmup",
      "max_grad_norm": 0.3,
      "save_steps": 100,
      "eval_steps": 50,
      "weight_decay": 0.005,
      "gradient_checkpointing": true
    },
    "peft_config": {
      "r": 8,
      "lora_alpha": 16,
      "lora_dropout": 0.05,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "w1",
        "w2",
        "w3"
      ]
    }
  }
}
