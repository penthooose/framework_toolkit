2025-04-28 07:21:19,102 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:21:19,102 - INFO - 

==================================================
2025-04-28 07:21:19,102 - INFO - Resuming from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:21:19,102 - INFO - ==================================================

2025-04-28 07:21:19,102 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:21:19,102 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets', 'text_column': 'input', 'use_checkpoint': True, 'checkpoint_path': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000', 'max_samples': None, 'pre_eval': True, 'eval_split': 0, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3200, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 32, 'lora_alpha': 42, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 5, 'learning_rate': 1.5e-05, 'warmup_steps': 100, 'warmup_ratio': 0.08, 'logging_steps': 10, 'save_steps': 200, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 210, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'lr_scheduler_type': 'cosine_with_restarts', 'num_cycles': 2, 'weight_decay': 0.01, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2}}
2025-04-28 07:21:19,102 - INFO - INFO: Loading datasets from individual JSONL files
2025-04-28 07:21:19,102 - INFO - Loading datasets from individual JSONL files
2025-04-28 07:21:19,268 - INFO - INFO: Loaded 8301 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\training_set.jsonl
2025-04-28 07:21:19,535 - INFO - INFO: Loaded 1555 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\validation_set.jsonl
2025-04-28 07:21:19,581 - INFO - INFO: Loaded separate validation set with 1555 examples
2025-04-28 07:21:19,585 - INFO - Loaded separate validation set with 1555 examples
2025-04-28 07:21:19,597 - INFO - INFO: Loaded 522 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\test_set.jsonl
2025-04-28 07:21:19,610 - INFO - INFO: Loaded separate test set with 522 examples
2025-04-28 07:21:19,610 - INFO - Loaded separate test set with 522 examples
2025-04-28 07:21:19,610 - INFO - INFO: Supervised format detected, combining input and output columns
2025-04-28 07:21:19,610 - INFO - Supervised format detected, combining input and output columns
2025-04-28 07:21:20,546 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-04-28 07:21:20,546 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-04-28 07:21:32,035 - INFO - INFO: Dataset prepared with 8301 examples
2025-04-28 07:21:34,331 - INFO - INFO: Dataset prepared with 1555 examples
2025-04-28 07:21:35,166 - INFO - INFO: Dataset prepared with 522 examples
2025-04-28 07:21:35,168 - INFO - INFO: CUDA cache cleared
2025-04-28 07:21:35,289 - INFO - INFO: Garbage collector freed 85 objects
2025-04-28 07:22:06,616 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3
2025-04-28 07:22:06,616 - INFO - INFO: Model has 8030261248 parameters, 0 are trainable (0.00%)
2025-04-28 07:22:07,658 - INFO - INFO: Model has 83886080 trainable parameters after PEFT configuration
2025-04-28 07:27:04,840 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:27:04,840 - INFO - 

==================================================
2025-04-28 07:27:04,840 - INFO - Resuming from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:27:04,840 - INFO - ==================================================

2025-04-28 07:27:04,840 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:27:04,840 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets', 'text_column': 'input', 'use_checkpoint': True, 'checkpoint_path': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000', 'max_samples': None, 'pre_eval': True, 'eval_split': 0, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3200, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 32, 'lora_alpha': 42, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 5, 'learning_rate': 1.5e-05, 'warmup_steps': 100, 'warmup_ratio': 0.08, 'logging_steps': 10, 'save_steps': 200, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 210, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'lr_scheduler_type': 'cosine_with_restarts', 'weight_decay': 0.01, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2}}
2025-04-28 07:27:04,841 - INFO - INFO: Loading datasets from individual JSONL files
2025-04-28 07:27:04,841 - INFO - Loading datasets from individual JSONL files
2025-04-28 07:27:05,009 - INFO - INFO: Loaded 8301 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\training_set.jsonl
2025-04-28 07:27:05,218 - INFO - INFO: Loaded 1555 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\validation_set.jsonl
2025-04-28 07:27:05,257 - INFO - INFO: Loaded separate validation set with 1555 examples
2025-04-28 07:27:05,257 - INFO - Loaded separate validation set with 1555 examples
2025-04-28 07:27:05,270 - INFO - INFO: Loaded 522 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\test_set.jsonl
2025-04-28 07:27:05,283 - INFO - INFO: Loaded separate test set with 522 examples
2025-04-28 07:27:05,284 - INFO - Loaded separate test set with 522 examples
2025-04-28 07:27:05,284 - INFO - INFO: Supervised format detected, combining input and output columns
2025-04-28 07:27:05,284 - INFO - Supervised format detected, combining input and output columns
2025-04-28 07:27:06,108 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-04-28 07:27:06,108 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-04-28 07:27:16,729 - INFO - INFO: Dataset prepared with 8301 examples
2025-04-28 07:27:18,863 - INFO - INFO: Dataset prepared with 1555 examples
2025-04-28 07:27:19,713 - INFO - INFO: Dataset prepared with 522 examples
2025-04-28 07:27:19,717 - INFO - INFO: CUDA cache cleared
2025-04-28 07:27:19,843 - INFO - INFO: Garbage collector freed 85 objects
2025-04-28 07:27:28,635 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3
2025-04-28 07:27:28,635 - INFO - INFO: Model has 8030261248 parameters, 0 are trainable (0.00%)
2025-04-28 07:27:29,527 - INFO - INFO: Model has 83886080 trainable parameters after PEFT configuration
2025-04-28 07:27:29,722 - INFO - Starting model training with 8301 training examples
2025-04-28 07:27:29,722 - INFO - Using 1555 examples for validation during training
2025-04-28 07:27:29,722 - INFO - Using 522 examples for pre/final evaluation
2025-04-28 07:27:29,810 - INFO - INFO: Testing evaluation with current settings...
2025-04-28 07:27:29,810 - INFO - INFO: Test dataset size: 522
2025-04-28 07:27:29,810 - INFO - INFO: CUDA cache cleared
2025-04-28 07:27:29,946 - INFO - INFO: Garbage collector freed 50 objects
2025-04-28 07:32:36,099 - INFO - INFO: Training progress: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,099 - INFO - Training progress: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,099 - INFO - Training metrics: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,135 - INFO - INFO: CUDA cache cleared
2025-04-28 07:32:36,272 - INFO - INFO: Garbage collector freed 19 objects
2025-04-28 07:32:36,272 - INFO - INFO: 
Evaluation successful!
2025-04-28 07:32:36,272 - INFO - INFO: Metrics: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,272 - INFO - INFO: Pre-training evaluation successful. Metrics: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,272 - INFO - Pre-training evaluation successful. Metrics: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,272 - INFO - Pre-training evaluation metrics: {'eval_loss': 1.2580950260162354, 'eval_model_preparation_time': 0.017, 'eval_runtime': 306.136, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705}
2025-04-28 07:32:36,281 - INFO - INFO: 
GPU Memory Summary:
2025-04-28 07:32:36,282 - INFO - INFO: Allocated: 10.77 GB
2025-04-28 07:32:36,282 - INFO - INFO: Cached: 12.52 GB
2025-04-28 07:32:36,282 - INFO - INFO: GPU Memory: Allocated 10.77 GB, Cached 12.52 GB
2025-04-28 07:32:36,283 - INFO - GPU Memory: Allocated 10.77 GB, Cached 12.52 GB
2025-04-28 07:32:36,283 - INFO - GPU Memory: Allocated 10.77 GB, Cached 12.52 GB
2025-04-28 07:32:36,283 - INFO - INFO: Starting training...
2025-04-28 07:32:36,283 - INFO - Starting training...
2025-04-28 07:32:36,283 - INFO - Starting training...
2025-04-28 07:32:36,283 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:32:36,284 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:32:36,284 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:32:36,284 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:32:36,284 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 07:32:36,284 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 07:32:36,285 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 07:32:36,285 - INFO - INFO: Registering specific numpy components
2025-04-28 07:32:36,285 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 07:32:36,286 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 07:32:36,286 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 07:32:36,286 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 07:32:36,286 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 07:32:36,286 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 07:32:36,286 - INFO - INFO: Registering specific numpy components
2025-04-28 07:32:36,286 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 07:32:36,286 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 07:32:36,286 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 07:32:36,876 - INFO - ERROR: Training failed with error: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
2025-04-28 07:32:36,922 - INFO - INFO: Training failed with error: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
2025-04-28 07:32:36,924 - INFO - Training failed with error: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
2025-04-28 07:32:36,924 - INFO - Training failed with error: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([1024, 32]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 4096]) from checkpoint, the shape in current model is torch.Size([32, 4096]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 24]) from checkpoint, the shape in current model is torch.Size([14336, 32]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([24, 14336]) from checkpoint, the shape in current model is torch.Size([32, 14336]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 24]) from checkpoint, the shape in current model is torch.Size([4096, 32]).
2025-04-28 07:36:21,247 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:36:21,247 - INFO - 

==================================================
2025-04-28 07:36:21,247 - INFO - Resuming from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:36:21,247 - INFO - ==================================================

2025-04-28 07:36:21,247 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 07:36:21,247 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets', 'text_column': 'input', 'use_checkpoint': True, 'checkpoint_path': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000', 'max_samples': None, 'pre_eval': False, 'eval_split': 0, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3200, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 24, 'lora_alpha': 48, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 5, 'learning_rate': 1.5e-05, 'warmup_steps': 100, 'warmup_ratio': 0.08, 'logging_steps': 10, 'save_steps': 200, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 210, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'lr_scheduler_type': 'cosine_with_restarts', 'weight_decay': 0.01, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2}}
2025-04-28 07:36:21,247 - INFO - INFO: Loading datasets from individual JSONL files
2025-04-28 07:36:21,247 - INFO - Loading datasets from individual JSONL files
2025-04-28 07:36:21,430 - INFO - INFO: Loaded 8301 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\training_set.jsonl
2025-04-28 07:36:21,643 - INFO - INFO: Loaded 1555 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\validation_set.jsonl
2025-04-28 07:36:21,679 - INFO - INFO: Loaded separate validation set with 1555 examples
2025-04-28 07:36:21,680 - INFO - Loaded separate validation set with 1555 examples
2025-04-28 07:36:21,691 - INFO - INFO: Loaded 522 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\test_set.jsonl
2025-04-28 07:36:21,706 - INFO - INFO: Loaded separate test set with 522 examples
2025-04-28 07:36:21,706 - INFO - Loaded separate test set with 522 examples
2025-04-28 07:36:21,706 - INFO - INFO: Supervised format detected, combining input and output columns
2025-04-28 07:36:21,706 - INFO - Supervised format detected, combining input and output columns
2025-04-28 07:36:22,517 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-04-28 07:36:22,517 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-04-28 07:36:33,322 - INFO - INFO: Dataset prepared with 8301 examples
2025-04-28 07:36:35,521 - INFO - INFO: Dataset prepared with 1555 examples
2025-04-28 07:36:36,373 - INFO - INFO: Dataset prepared with 522 examples
2025-04-28 07:36:36,390 - INFO - INFO: CUDA cache cleared
2025-04-28 07:36:36,496 - INFO - INFO: Garbage collector freed 85 objects
2025-04-28 07:36:45,068 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3
2025-04-28 07:36:45,068 - INFO - INFO: Model has 8030261248 parameters, 0 are trainable (0.00%)
2025-04-28 07:36:45,747 - INFO - INFO: Model has 62914560 trainable parameters after PEFT configuration
2025-04-28 07:36:45,802 - INFO - Starting model training with 8301 training examples
2025-04-28 07:36:45,802 - INFO - Using 1555 examples for validation during training
2025-04-28 07:36:45,802 - INFO - Using 522 examples for pre/final evaluation
2025-04-28 07:36:45,823 - INFO - INFO: Starting training...
2025-04-28 07:36:45,823 - INFO - Starting training...
2025-04-28 07:36:45,823 - INFO - Starting training...
2025-04-28 07:36:45,823 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:36:45,823 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:36:45,823 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:36:45,823 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 07:36:45,823 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 07:36:45,823 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 07:36:45,823 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 07:36:45,823 - INFO - INFO: Registering specific numpy components
2025-04-28 07:36:45,823 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 07:36:45,823 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 07:36:45,823 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 07:36:45,823 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 07:36:45,823 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 07:36:45,823 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 07:36:45,823 - INFO - INFO: Registering specific numpy components
2025-04-28 07:36:45,829 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 07:36:45,829 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 07:36:45,829 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 07:36:47,152 - INFO - INFO: Starting epoch 0.9385265133740028/5
2025-04-28 07:36:47,152 - INFO - Starting epoch 0.9385265133740028/5
2025-04-28 07:36:47,152 - INFO - Starting epoch 0.9385265133740028/5
2025-04-28 07:40:08,709 - INFO - INFO: Training progress: {'loss': 0.57, 'grad_norm': 0.8860024809837341, 'learning_rate': 1.3845584396900316e-05, 'epoch': 0.9733767016022166}
2025-04-28 07:40:08,709 - INFO - Training progress: {'loss': 0.57, 'grad_norm': 0.8860024809837341, 'learning_rate': 1.3845584396900316e-05, 'epoch': 0.9733767016022166}
2025-04-28 07:40:08,709 - INFO - Training metrics: {'loss': 0.57, 'grad_norm': 0.8860024809837341, 'learning_rate': 1.3845584396900316e-05, 'epoch': 0.9733767016022166}
2025-04-28 07:43:21,183 - INFO - INFO: Training progress: {'loss': 0.4731, 'grad_norm': 1.1250381469726562, 'learning_rate': 1.3820763523977277e-05, 'epoch': 0.9830140946873871}
2025-04-28 07:43:21,183 - INFO - Training progress: {'loss': 0.4731, 'grad_norm': 1.1250381469726562, 'learning_rate': 1.3820763523977277e-05, 'epoch': 0.9830140946873871}
2025-04-28 07:43:21,183 - INFO - Training metrics: {'loss': 0.4731, 'grad_norm': 1.1250381469726562, 'learning_rate': 1.3820763523977277e-05, 'epoch': 0.9830140946873871}
2025-04-28 07:46:35,777 - INFO - INFO: Training progress: {'loss': 0.4886, 'grad_norm': 0.7746790647506714, 'learning_rate': 1.379570139068285e-05, 'epoch': 0.9926514877725575}
2025-04-28 07:46:35,777 - INFO - Training progress: {'loss': 0.4886, 'grad_norm': 0.7746790647506714, 'learning_rate': 1.379570139068285e-05, 'epoch': 0.9926514877725575}
2025-04-28 07:46:35,777 - INFO - Training metrics: {'loss': 0.4886, 'grad_norm': 0.7746790647506714, 'learning_rate': 1.379570139068285e-05, 'epoch': 0.9926514877725575}
2025-04-28 07:49:06,232 - INFO - INFO: Starting epoch 0.9993976629321768/5
2025-04-28 07:49:06,232 - INFO - Starting epoch 0.9993976629321768/5
2025-04-28 07:49:06,232 - INFO - Starting epoch 0.9993976629321768/5
2025-04-28 07:50:00,662 - INFO - INFO: Training progress: {'loss': 0.48, 'grad_norm': 0.9903228878974915, 'learning_rate': 1.3770398953626086e-05, 'epoch': 1.002288880857728}
2025-04-28 07:50:00,662 - INFO - Training progress: {'loss': 0.48, 'grad_norm': 0.9903228878974915, 'learning_rate': 1.3770398953626086e-05, 'epoch': 1.002288880857728}
2025-04-28 07:50:00,662 - INFO - Training metrics: {'loss': 0.48, 'grad_norm': 0.9903228878974915, 'learning_rate': 1.3770398953626086e-05, 'epoch': 1.002288880857728}
2025-04-28 07:53:18,215 - INFO - INFO: Training progress: {'loss': 0.4575, 'grad_norm': 0.9845762252807617, 'learning_rate': 1.3744857178588308e-05, 'epoch': 1.0119262739428985}
2025-04-28 07:53:18,215 - INFO - Training progress: {'loss': 0.4575, 'grad_norm': 0.9845762252807617, 'learning_rate': 1.3744857178588308e-05, 'epoch': 1.0119262739428985}
2025-04-28 07:53:18,215 - INFO - Training metrics: {'loss': 0.4575, 'grad_norm': 0.9845762252807617, 'learning_rate': 1.3744857178588308e-05, 'epoch': 1.0119262739428985}
2025-04-28 08:09:01,090 - INFO - INFO: Training progress: {'eval_loss': 0.4520582854747772, 'eval_runtime': 942.8711, 'eval_samples_per_second': 1.649, 'eval_steps_per_second': 1.649, 'epoch': 1.0119262739428985}
2025-04-28 08:09:01,090 - INFO - Training progress: {'eval_loss': 0.4520582854747772, 'eval_runtime': 942.8711, 'eval_samples_per_second': 1.649, 'eval_steps_per_second': 1.649, 'epoch': 1.0119262739428985}
2025-04-28 08:09:01,090 - INFO - Training metrics: {'eval_loss': 0.4520582854747772, 'eval_runtime': 942.8711, 'eval_samples_per_second': 1.649, 'eval_steps_per_second': 1.649, 'epoch': 1.0119262739428985}
2025-04-28 08:12:17,983 - INFO - INFO: Training progress: {'loss': 0.4661, 'grad_norm': 0.9724071025848389, 'learning_rate': 1.3719077040486248e-05, 'epoch': 1.021563667028069}
2025-04-28 08:12:17,984 - INFO - Training progress: {'loss': 0.4661, 'grad_norm': 0.9724071025848389, 'learning_rate': 1.3719077040486248e-05, 'epoch': 1.021563667028069}
2025-04-28 08:12:17,984 - INFO - Training metrics: {'loss': 0.4661, 'grad_norm': 0.9724071025848389, 'learning_rate': 1.3719077040486248e-05, 'epoch': 1.021563667028069}
2025-04-28 08:15:36,450 - INFO - INFO: Training progress: {'loss': 0.4663, 'grad_norm': 1.1212427616119385, 'learning_rate': 1.3693059523334841e-05, 'epoch': 1.0312010601132393}
2025-04-28 08:15:36,450 - INFO - Training progress: {'loss': 0.4663, 'grad_norm': 1.1212427616119385, 'learning_rate': 1.3693059523334841e-05, 'epoch': 1.0312010601132393}
2025-04-28 08:15:36,450 - INFO - Training metrics: {'loss': 0.4663, 'grad_norm': 1.1212427616119385, 'learning_rate': 1.3693059523334841e-05, 'epoch': 1.0312010601132393}
2025-04-28 08:18:53,999 - INFO - INFO: Training progress: {'loss': 0.4514, 'grad_norm': 0.9199048280715942, 'learning_rate': 1.3666805620209658e-05, 'epoch': 1.0408384531984098}
2025-04-28 08:18:53,999 - INFO - Training progress: {'loss': 0.4514, 'grad_norm': 0.9199048280715942, 'learning_rate': 1.3666805620209658e-05, 'epoch': 1.0408384531984098}
2025-04-28 08:18:53,999 - INFO - Training metrics: {'loss': 0.4514, 'grad_norm': 0.9199048280715942, 'learning_rate': 1.3666805620209658e-05, 'epoch': 1.0408384531984098}
2025-04-28 08:22:14,059 - INFO - INFO: Training progress: {'loss': 0.4454, 'grad_norm': 0.7673224210739136, 'learning_rate': 1.3640316333209007e-05, 'epoch': 1.0504758462835804}
2025-04-28 08:22:14,060 - INFO - Training progress: {'loss': 0.4454, 'grad_norm': 0.7673224210739136, 'learning_rate': 1.3640316333209007e-05, 'epoch': 1.0504758462835804}
2025-04-28 08:22:14,060 - INFO - Training metrics: {'loss': 0.4454, 'grad_norm': 0.7673224210739136, 'learning_rate': 1.3640316333209007e-05, 'epoch': 1.0504758462835804}
2025-04-28 08:25:29,342 - INFO - INFO: Training progress: {'loss': 0.4569, 'grad_norm': 0.8685972094535828, 'learning_rate': 1.3613592673415673e-05, 'epoch': 1.0601132393687507}
2025-04-28 08:25:29,342 - INFO - Training progress: {'loss': 0.4569, 'grad_norm': 0.8685972094535828, 'learning_rate': 1.3613592673415673e-05, 'epoch': 1.0601132393687507}
2025-04-28 08:25:29,342 - INFO - Training metrics: {'loss': 0.4569, 'grad_norm': 0.8685972094535828, 'learning_rate': 1.3613592673415673e-05, 'epoch': 1.0601132393687507}
2025-04-28 08:28:42,806 - INFO - INFO: Training progress: {'loss': 0.4464, 'grad_norm': 0.9780226945877075, 'learning_rate': 1.3586635660858343e-05, 'epoch': 1.0697506324539212}
2025-04-28 08:28:42,806 - INFO - Training progress: {'loss': 0.4464, 'grad_norm': 0.9780226945877075, 'learning_rate': 1.3586635660858343e-05, 'epoch': 1.0697506324539212}
2025-04-28 08:28:42,806 - INFO - Training metrics: {'loss': 0.4464, 'grad_norm': 0.9780226945877075, 'learning_rate': 1.3586635660858343e-05, 'epoch': 1.0697506324539212}
2025-04-28 08:31:55,759 - INFO - INFO: Training progress: {'loss': 0.5205, 'grad_norm': 1.0362422466278076, 'learning_rate': 1.3559446324472654e-05, 'epoch': 1.0793880255390917}
2025-04-28 08:31:55,759 - INFO - Training progress: {'loss': 0.5205, 'grad_norm': 1.0362422466278076, 'learning_rate': 1.3559446324472654e-05, 'epoch': 1.0793880255390917}
2025-04-28 08:31:55,759 - INFO - Training metrics: {'loss': 0.5205, 'grad_norm': 1.0362422466278076, 'learning_rate': 1.3559446324472654e-05, 'epoch': 1.0793880255390917}
2025-04-28 08:35:08,924 - INFO - INFO: Training progress: {'loss': 0.433, 'grad_norm': 0.7695164084434509, 'learning_rate': 1.353202570206193e-05, 'epoch': 1.089025418624262}
2025-04-28 08:35:08,924 - INFO - Training progress: {'loss': 0.433, 'grad_norm': 0.7695164084434509, 'learning_rate': 1.353202570206193e-05, 'epoch': 1.089025418624262}
2025-04-28 08:35:08,924 - INFO - Training metrics: {'loss': 0.433, 'grad_norm': 0.7695164084434509, 'learning_rate': 1.353202570206193e-05, 'epoch': 1.089025418624262}
2025-04-28 08:38:22,712 - INFO - INFO: Training progress: {'loss': 0.5182, 'grad_norm': 0.9702129364013672, 'learning_rate': 1.350437484025757e-05, 'epoch': 1.0986628117094326}
2025-04-28 08:38:22,712 - INFO - Training progress: {'loss': 0.5182, 'grad_norm': 0.9702129364013672, 'learning_rate': 1.350437484025757e-05, 'epoch': 1.0986628117094326}
2025-04-28 08:38:22,712 - INFO - Training metrics: {'loss': 0.5182, 'grad_norm': 0.9702129364013672, 'learning_rate': 1.350437484025757e-05, 'epoch': 1.0986628117094326}
2025-04-28 08:41:34,364 - INFO - INFO: Training progress: {'loss': 0.456, 'grad_norm': 0.9509837627410889, 'learning_rate': 1.3476494794479084e-05, 'epoch': 1.1083002047946031}
2025-04-28 08:41:34,364 - INFO - Training progress: {'loss': 0.456, 'grad_norm': 0.9509837627410889, 'learning_rate': 1.3476494794479084e-05, 'epoch': 1.1083002047946031}
2025-04-28 08:41:34,364 - INFO - Training metrics: {'loss': 0.456, 'grad_norm': 0.9509837627410889, 'learning_rate': 1.3476494794479084e-05, 'epoch': 1.1083002047946031}
2025-04-28 08:44:46,469 - INFO - INFO: Training progress: {'loss': 0.4552, 'grad_norm': 0.9967794418334961, 'learning_rate': 1.3448386628893832e-05, 'epoch': 1.1179375978797734}
2025-04-28 08:44:46,469 - INFO - Training progress: {'loss': 0.4552, 'grad_norm': 0.9967794418334961, 'learning_rate': 1.3448386628893832e-05, 'epoch': 1.1179375978797734}
2025-04-28 08:44:46,469 - INFO - Training metrics: {'loss': 0.4552, 'grad_norm': 0.9967794418334961, 'learning_rate': 1.3448386628893832e-05, 'epoch': 1.1179375978797734}
2025-04-28 08:48:02,639 - INFO - INFO: Training progress: {'loss': 0.4753, 'grad_norm': 0.8983170986175537, 'learning_rate': 1.3420051416376387e-05, 'epoch': 1.127574990964944}
2025-04-28 08:48:02,639 - INFO - Training progress: {'loss': 0.4753, 'grad_norm': 0.8983170986175537, 'learning_rate': 1.3420051416376387e-05, 'epoch': 1.127574990964944}
2025-04-28 08:48:02,639 - INFO - Training metrics: {'loss': 0.4753, 'grad_norm': 0.8983170986175537, 'learning_rate': 1.3420051416376387e-05, 'epoch': 1.127574990964944}
2025-04-28 08:51:16,694 - INFO - INFO: Training progress: {'loss': 0.4997, 'grad_norm': 0.8842861652374268, 'learning_rate': 1.339149023846759e-05, 'epoch': 1.1372123840501145}
2025-04-28 08:51:16,694 - INFO - Training progress: {'loss': 0.4997, 'grad_norm': 0.8842861652374268, 'learning_rate': 1.339149023846759e-05, 'epoch': 1.1372123840501145}
2025-04-28 08:51:16,694 - INFO - Training metrics: {'loss': 0.4997, 'grad_norm': 0.8842861652374268, 'learning_rate': 1.339149023846759e-05, 'epoch': 1.1372123840501145}
2025-04-28 08:54:33,197 - INFO - INFO: Training progress: {'loss': 0.478, 'grad_norm': 0.7790980935096741, 'learning_rate': 1.3362704185333266e-05, 'epoch': 1.146849777135285}
2025-04-28 08:54:33,197 - INFO - Training progress: {'loss': 0.478, 'grad_norm': 0.7790980935096741, 'learning_rate': 1.3362704185333266e-05, 'epoch': 1.146849777135285}
2025-04-28 08:54:33,197 - INFO - Training metrics: {'loss': 0.478, 'grad_norm': 0.7790980935096741, 'learning_rate': 1.3362704185333266e-05, 'epoch': 1.146849777135285}
2025-04-28 08:57:48,628 - INFO - INFO: Training progress: {'loss': 0.429, 'grad_norm': 0.7960482239723206, 'learning_rate': 1.3333694355722614e-05, 'epoch': 1.1564871702204553}
2025-04-28 08:57:48,628 - INFO - Training progress: {'loss': 0.429, 'grad_norm': 0.7960482239723206, 'learning_rate': 1.3333694355722614e-05, 'epoch': 1.1564871702204553}
2025-04-28 08:57:48,628 - INFO - Training metrics: {'loss': 0.429, 'grad_norm': 0.7960482239723206, 'learning_rate': 1.3333694355722614e-05, 'epoch': 1.1564871702204553}
2025-04-28 08:57:49,561 - INFO - INFO: Saving checkpoint at step 1200
2025-04-28 08:57:49,561 - INFO - Saving checkpoint at step 1200
2025-04-28 08:57:49,561 - INFO - Saving checkpoint at step 1200
2025-04-28 09:01:01,061 - INFO - INFO: Training progress: {'loss': 0.4281, 'grad_norm': 0.7967208027839661, 'learning_rate': 1.3304461856926275e-05, 'epoch': 1.1661245633056259}
2025-04-28 09:01:01,061 - INFO - Training progress: {'loss': 0.4281, 'grad_norm': 0.7967208027839661, 'learning_rate': 1.3304461856926275e-05, 'epoch': 1.1661245633056259}
2025-04-28 09:01:01,061 - INFO - Training metrics: {'loss': 0.4281, 'grad_norm': 0.7967208027839661, 'learning_rate': 1.3304461856926275e-05, 'epoch': 1.1661245633056259}
2025-04-28 09:04:16,656 - INFO - INFO: Training progress: {'loss': 0.4626, 'grad_norm': 0.8393531441688538, 'learning_rate': 1.3275007804734051e-05, 'epoch': 1.1757619563907964}
2025-04-28 09:04:16,656 - INFO - Training progress: {'loss': 0.4626, 'grad_norm': 0.8393531441688538, 'learning_rate': 1.3275007804734051e-05, 'epoch': 1.1757619563907964}
2025-04-28 09:04:16,656 - INFO - Training metrics: {'loss': 0.4626, 'grad_norm': 0.8393531441688538, 'learning_rate': 1.3275007804734051e-05, 'epoch': 1.1757619563907964}
2025-04-28 09:07:28,803 - INFO - INFO: Training progress: {'loss': 0.4811, 'grad_norm': 0.8663718104362488, 'learning_rate': 1.3245333323392335e-05, 'epoch': 1.1853993494759667}
2025-04-28 09:07:28,803 - INFO - Training progress: {'loss': 0.4811, 'grad_norm': 0.8663718104362488, 'learning_rate': 1.3245333323392335e-05, 'epoch': 1.1853993494759667}
2025-04-28 09:07:28,803 - INFO - Training metrics: {'loss': 0.4811, 'grad_norm': 0.8663718104362488, 'learning_rate': 1.3245333323392335e-05, 'epoch': 1.1853993494759667}
2025-04-28 09:10:40,275 - INFO - INFO: Training progress: {'loss': 0.4599, 'grad_norm': 0.7635533213615417, 'learning_rate': 1.321543954556119e-05, 'epoch': 1.1950367425611372}
2025-04-28 09:10:40,275 - INFO - Training progress: {'loss': 0.4599, 'grad_norm': 0.7635533213615417, 'learning_rate': 1.321543954556119e-05, 'epoch': 1.1950367425611372}
2025-04-28 09:10:40,275 - INFO - Training metrics: {'loss': 0.4599, 'grad_norm': 0.7635533213615417, 'learning_rate': 1.321543954556119e-05, 'epoch': 1.1950367425611372}
2025-04-28 09:13:52,035 - INFO - INFO: Training progress: {'loss': 0.4454, 'grad_norm': 0.8568858504295349, 'learning_rate': 1.3185327612271103e-05, 'epoch': 1.2046741356463078}
2025-04-28 09:13:52,035 - INFO - Training progress: {'loss': 0.4454, 'grad_norm': 0.8568858504295349, 'learning_rate': 1.3185327612271103e-05, 'epoch': 1.2046741356463078}
2025-04-28 09:13:52,035 - INFO - Training metrics: {'loss': 0.4454, 'grad_norm': 0.8568858504295349, 'learning_rate': 1.3185327612271103e-05, 'epoch': 1.2046741356463078}
2025-04-28 09:17:03,070 - INFO - INFO: Training progress: {'loss': 0.441, 'grad_norm': 0.9735889434814453, 'learning_rate': 1.3154998672879461e-05, 'epoch': 1.214311528731478}
2025-04-28 09:17:03,070 - INFO - Training progress: {'loss': 0.441, 'grad_norm': 0.9735889434814453, 'learning_rate': 1.3154998672879461e-05, 'epoch': 1.214311528731478}
2025-04-28 09:17:03,070 - INFO - Training metrics: {'loss': 0.441, 'grad_norm': 0.9735889434814453, 'learning_rate': 1.3154998672879461e-05, 'epoch': 1.214311528731478}
2025-04-28 09:32:05,620 - INFO - INFO: Training progress: {'eval_loss': 0.4339553415775299, 'eval_runtime': 902.55, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.214311528731478}
2025-04-28 09:32:05,620 - INFO - Training progress: {'eval_loss': 0.4339553415775299, 'eval_runtime': 902.55, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.214311528731478}
2025-04-28 09:32:05,620 - INFO - Training metrics: {'eval_loss': 0.4339553415775299, 'eval_runtime': 902.55, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.214311528731478}
2025-04-28 09:35:17,055 - INFO - INFO: Training progress: {'loss': 0.4814, 'grad_norm': 0.8992975950241089, 'learning_rate': 1.312445388502666e-05, 'epoch': 1.2239489218166486}
2025-04-28 09:35:17,055 - INFO - Training progress: {'loss': 0.4814, 'grad_norm': 0.8992975950241089, 'learning_rate': 1.312445388502666e-05, 'epoch': 1.2239489218166486}
2025-04-28 09:35:17,055 - INFO - Training metrics: {'loss': 0.4814, 'grad_norm': 0.8992975950241089, 'learning_rate': 1.312445388502666e-05, 'epoch': 1.2239489218166486}
2025-04-28 09:38:28,415 - INFO - INFO: Training progress: {'loss': 0.4503, 'grad_norm': 1.0565637350082397, 'learning_rate': 1.3093694414591921e-05, 'epoch': 1.2335863149018191}
2025-04-28 09:38:28,415 - INFO - Training progress: {'loss': 0.4503, 'grad_norm': 1.0565637350082397, 'learning_rate': 1.3093694414591921e-05, 'epoch': 1.2335863149018191}
2025-04-28 09:38:28,415 - INFO - Training metrics: {'loss': 0.4503, 'grad_norm': 1.0565637350082397, 'learning_rate': 1.3093694414591921e-05, 'epoch': 1.2335863149018191}
2025-04-28 09:41:39,822 - INFO - INFO: Training progress: {'loss': 0.4941, 'grad_norm': 1.0993531942367554, 'learning_rate': 1.3062721435648791e-05, 'epoch': 1.2432237079869894}
2025-04-28 09:41:39,822 - INFO - Training progress: {'loss': 0.4941, 'grad_norm': 1.0993531942367554, 'learning_rate': 1.3062721435648791e-05, 'epoch': 1.2432237079869894}
2025-04-28 09:41:39,822 - INFO - Training metrics: {'loss': 0.4941, 'grad_norm': 1.0993531942367554, 'learning_rate': 1.3062721435648791e-05, 'epoch': 1.2432237079869894}
2025-04-28 09:44:51,259 - INFO - INFO: Training progress: {'loss': 0.4579, 'grad_norm': 1.0378330945968628, 'learning_rate': 1.3031536130420332e-05, 'epoch': 1.25286110107216}
2025-04-28 09:44:51,259 - INFO - Training progress: {'loss': 0.4579, 'grad_norm': 1.0378330945968628, 'learning_rate': 1.3031536130420332e-05, 'epoch': 1.25286110107216}
2025-04-28 09:44:51,259 - INFO - Training metrics: {'loss': 0.4579, 'grad_norm': 1.0378330945968628, 'learning_rate': 1.3031536130420332e-05, 'epoch': 1.25286110107216}
2025-04-28 09:48:02,551 - INFO - INFO: Training progress: {'loss': 0.4725, 'grad_norm': 0.9145170450210571, 'learning_rate': 1.3000139689233992e-05, 'epoch': 1.2624984941573305}
2025-04-28 09:48:02,551 - INFO - Training progress: {'loss': 0.4725, 'grad_norm': 0.9145170450210571, 'learning_rate': 1.3000139689233992e-05, 'epoch': 1.2624984941573305}
2025-04-28 09:48:02,551 - INFO - Training metrics: {'loss': 0.4725, 'grad_norm': 0.9145170450210571, 'learning_rate': 1.3000139689233992e-05, 'epoch': 1.2624984941573305}
2025-04-28 09:51:14,265 - INFO - INFO: Training progress: {'loss': 0.442, 'grad_norm': 1.2817775011062622, 'learning_rate': 1.2968533310476176e-05, 'epoch': 1.2721358872425008}
2025-04-28 09:51:14,265 - INFO - Training progress: {'loss': 0.442, 'grad_norm': 1.2817775011062622, 'learning_rate': 1.2968533310476176e-05, 'epoch': 1.2721358872425008}
2025-04-28 09:51:14,265 - INFO - Training metrics: {'loss': 0.442, 'grad_norm': 1.2817775011062622, 'learning_rate': 1.2968533310476176e-05, 'epoch': 1.2721358872425008}
2025-04-28 09:54:25,947 - INFO - INFO: Training progress: {'loss': 0.432, 'grad_norm': 1.1135151386260986, 'learning_rate': 1.2936718200546497e-05, 'epoch': 1.2817732803276713}
2025-04-28 09:54:25,947 - INFO - Training progress: {'loss': 0.432, 'grad_norm': 1.1135151386260986, 'learning_rate': 1.2936718200546497e-05, 'epoch': 1.2817732803276713}
2025-04-28 09:54:25,947 - INFO - Training metrics: {'loss': 0.432, 'grad_norm': 1.1135151386260986, 'learning_rate': 1.2936718200546497e-05, 'epoch': 1.2817732803276713}
2025-04-28 09:57:37,437 - INFO - INFO: Training progress: {'loss': 0.4572, 'grad_norm': 1.1609281301498413, 'learning_rate': 1.2904695573811733e-05, 'epoch': 1.2914106734128419}
2025-04-28 09:57:37,437 - INFO - Training progress: {'loss': 0.4572, 'grad_norm': 1.1609281301498413, 'learning_rate': 1.2904695573811733e-05, 'epoch': 1.2914106734128419}
2025-04-28 09:57:37,437 - INFO - Training metrics: {'loss': 0.4572, 'grad_norm': 1.1609281301498413, 'learning_rate': 1.2904695573811733e-05, 'epoch': 1.2914106734128419}
2025-04-28 10:00:48,575 - INFO - INFO: Training progress: {'loss': 0.4256, 'grad_norm': 1.0852867364883423, 'learning_rate': 1.2872466652559476e-05, 'epoch': 1.3010480664980122}
2025-04-28 10:00:48,575 - INFO - Training progress: {'loss': 0.4256, 'grad_norm': 1.0852867364883423, 'learning_rate': 1.2872466652559476e-05, 'epoch': 1.3010480664980122}
2025-04-28 10:00:48,575 - INFO - Training metrics: {'loss': 0.4256, 'grad_norm': 1.0852867364883423, 'learning_rate': 1.2872466652559476e-05, 'epoch': 1.3010480664980122}
2025-04-28 10:04:00,249 - INFO - INFO: Training progress: {'loss': 0.4475, 'grad_norm': 0.8812661170959473, 'learning_rate': 1.2840032666951474e-05, 'epoch': 1.3106854595831827}
2025-04-28 10:04:00,249 - INFO - Training progress: {'loss': 0.4475, 'grad_norm': 0.8812661170959473, 'learning_rate': 1.2840032666951474e-05, 'epoch': 1.3106854595831827}
2025-04-28 10:04:00,249 - INFO - Training metrics: {'loss': 0.4475, 'grad_norm': 0.8812661170959473, 'learning_rate': 1.2840032666951474e-05, 'epoch': 1.3106854595831827}
2025-04-28 10:07:11,443 - INFO - INFO: Training progress: {'loss': 0.4367, 'grad_norm': 1.1162590980529785, 'learning_rate': 1.2807394854976677e-05, 'epoch': 1.3203228526683533}
2025-04-28 10:07:11,443 - INFO - Training progress: {'loss': 0.4367, 'grad_norm': 1.1162590980529785, 'learning_rate': 1.2807394854976677e-05, 'epoch': 1.3203228526683533}
2025-04-28 10:07:11,443 - INFO - Training metrics: {'loss': 0.4367, 'grad_norm': 1.1162590980529785, 'learning_rate': 1.2807394854976677e-05, 'epoch': 1.3203228526683533}
2025-04-28 10:10:23,178 - INFO - INFO: Training progress: {'loss': 0.4304, 'grad_norm': 0.9763501882553101, 'learning_rate': 1.2774554462403992e-05, 'epoch': 1.3299602457535236}
2025-04-28 10:10:23,178 - INFO - Training progress: {'loss': 0.4304, 'grad_norm': 0.9763501882553101, 'learning_rate': 1.2774554462403992e-05, 'epoch': 1.3299602457535236}
2025-04-28 10:10:23,178 - INFO - Training metrics: {'loss': 0.4304, 'grad_norm': 0.9763501882553101, 'learning_rate': 1.2774554462403992e-05, 'epoch': 1.3299602457535236}
2025-04-28 10:13:34,609 - INFO - INFO: Training progress: {'loss': 0.4744, 'grad_norm': 1.160132884979248, 'learning_rate': 1.2741512742734719e-05, 'epoch': 1.339597638838694}
2025-04-28 10:13:34,609 - INFO - Training progress: {'loss': 0.4744, 'grad_norm': 1.160132884979248, 'learning_rate': 1.2741512742734719e-05, 'epoch': 1.339597638838694}
2025-04-28 10:13:34,609 - INFO - Training metrics: {'loss': 0.4744, 'grad_norm': 1.160132884979248, 'learning_rate': 1.2741512742734719e-05, 'epoch': 1.339597638838694}
2025-04-28 10:16:46,296 - INFO - INFO: Training progress: {'loss': 0.4525, 'grad_norm': 1.1124411821365356, 'learning_rate': 1.2708270957154714e-05, 'epoch': 1.3492350319238646}
2025-04-28 10:16:46,296 - INFO - Training progress: {'loss': 0.4525, 'grad_norm': 1.1124411821365356, 'learning_rate': 1.2708270957154714e-05, 'epoch': 1.3492350319238646}
2025-04-28 10:16:46,296 - INFO - Training metrics: {'loss': 0.4525, 'grad_norm': 1.1124411821365356, 'learning_rate': 1.2708270957154714e-05, 'epoch': 1.3492350319238646}
2025-04-28 10:16:47,078 - INFO - INFO: Saving checkpoint at step 1400
2025-04-28 10:16:47,078 - INFO - Saving checkpoint at step 1400
2025-04-28 10:16:47,078 - INFO - Saving checkpoint at step 1400
2025-04-28 10:19:58,673 - INFO - INFO: Training progress: {'loss': 0.465, 'grad_norm': 0.8206895589828491, 'learning_rate': 1.2674830374486249e-05, 'epoch': 1.358872425009035}
2025-04-28 10:19:58,673 - INFO - Training progress: {'loss': 0.465, 'grad_norm': 0.8206895589828491, 'learning_rate': 1.2674830374486249e-05, 'epoch': 1.358872425009035}
2025-04-28 10:19:58,673 - INFO - Training metrics: {'loss': 0.465, 'grad_norm': 0.8206895589828491, 'learning_rate': 1.2674830374486249e-05, 'epoch': 1.358872425009035}
2025-04-28 10:23:10,385 - INFO - INFO: Training progress: {'loss': 0.4325, 'grad_norm': 1.3937091827392578, 'learning_rate': 1.2641192271139582e-05, 'epoch': 1.3685098180942055}
2025-04-28 10:23:10,385 - INFO - Training progress: {'loss': 0.4325, 'grad_norm': 1.3937091827392578, 'learning_rate': 1.2641192271139582e-05, 'epoch': 1.3685098180942055}
2025-04-28 10:23:10,385 - INFO - Training metrics: {'loss': 0.4325, 'grad_norm': 1.3937091827392578, 'learning_rate': 1.2641192271139582e-05, 'epoch': 1.3685098180942055}
2025-04-28 10:26:21,489 - INFO - INFO: Training progress: {'loss': 0.4224, 'grad_norm': 1.219046950340271, 'learning_rate': 1.2607357931064234e-05, 'epoch': 1.378147211179376}
2025-04-28 10:26:21,489 - INFO - Training progress: {'loss': 0.4224, 'grad_norm': 1.219046950340271, 'learning_rate': 1.2607357931064234e-05, 'epoch': 1.378147211179376}
2025-04-28 10:26:21,489 - INFO - Training metrics: {'loss': 0.4224, 'grad_norm': 1.219046950340271, 'learning_rate': 1.2607357931064234e-05, 'epoch': 1.378147211179376}
2025-04-28 10:29:32,756 - INFO - INFO: Training progress: {'loss': 0.429, 'grad_norm': 1.0705993175506592, 'learning_rate': 1.2573328645699985e-05, 'epoch': 1.3877846042645463}
2025-04-28 10:29:32,756 - INFO - Training progress: {'loss': 0.429, 'grad_norm': 1.0705993175506592, 'learning_rate': 1.2573328645699985e-05, 'epoch': 1.3877846042645463}
2025-04-28 10:29:32,756 - INFO - Training metrics: {'loss': 0.429, 'grad_norm': 1.0705993175506592, 'learning_rate': 1.2573328645699985e-05, 'epoch': 1.3877846042645463}
2025-04-28 10:32:44,224 - INFO - INFO: Training progress: {'loss': 0.4168, 'grad_norm': 1.187533974647522, 'learning_rate': 1.253910571392758e-05, 'epoch': 1.3974219973497168}
2025-04-28 10:32:44,224 - INFO - Training progress: {'loss': 0.4168, 'grad_norm': 1.187533974647522, 'learning_rate': 1.253910571392758e-05, 'epoch': 1.3974219973497168}
2025-04-28 10:32:44,224 - INFO - Training metrics: {'loss': 0.4168, 'grad_norm': 1.187533974647522, 'learning_rate': 1.253910571392758e-05, 'epoch': 1.3974219973497168}
2025-04-28 10:35:55,779 - INFO - INFO: Training progress: {'loss': 0.4225, 'grad_norm': 0.8284773230552673, 'learning_rate': 1.2504690442019146e-05, 'epoch': 1.4070593904348874}
2025-04-28 10:35:55,779 - INFO - Training progress: {'loss': 0.4225, 'grad_norm': 0.8284773230552673, 'learning_rate': 1.2504690442019146e-05, 'epoch': 1.4070593904348874}
2025-04-28 10:35:55,779 - INFO - Training metrics: {'loss': 0.4225, 'grad_norm': 0.8284773230552673, 'learning_rate': 1.2504690442019146e-05, 'epoch': 1.4070593904348874}
2025-04-28 10:39:07,255 - INFO - INFO: Training progress: {'loss': 0.4882, 'grad_norm': 1.2313987016677856, 'learning_rate': 1.2470084143588343e-05, 'epoch': 1.416696783520058}
2025-04-28 10:39:07,255 - INFO - Training progress: {'loss': 0.4882, 'grad_norm': 1.2313987016677856, 'learning_rate': 1.2470084143588343e-05, 'epoch': 1.416696783520058}
2025-04-28 10:39:07,255 - INFO - Training metrics: {'loss': 0.4882, 'grad_norm': 1.2313987016677856, 'learning_rate': 1.2470084143588343e-05, 'epoch': 1.416696783520058}
2025-04-28 10:54:09,524 - INFO - INFO: Training progress: {'eval_loss': 0.4222733676433563, 'eval_runtime': 902.2526, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.416696783520058}
2025-04-28 10:54:09,524 - INFO - Training progress: {'eval_loss': 0.4222733676433563, 'eval_runtime': 902.2526, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.416696783520058}
2025-04-28 10:54:09,524 - INFO - Training metrics: {'eval_loss': 0.4222733676433563, 'eval_runtime': 902.2526, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.416696783520058}
2025-04-28 10:57:21,064 - INFO - INFO: Training progress: {'loss': 0.4324, 'grad_norm': 0.9579139947891235, 'learning_rate': 1.2435288139540208e-05, 'epoch': 1.4263341766052282}
2025-04-28 10:57:21,064 - INFO - Training progress: {'loss': 0.4324, 'grad_norm': 0.9579139947891235, 'learning_rate': 1.2435288139540208e-05, 'epoch': 1.4263341766052282}
2025-04-28 10:57:21,064 - INFO - Training metrics: {'loss': 0.4324, 'grad_norm': 0.9579139947891235, 'learning_rate': 1.2435288139540208e-05, 'epoch': 1.4263341766052282}
2025-04-28 11:00:32,309 - INFO - INFO: Training progress: {'loss': 0.4604, 'grad_norm': 1.113262414932251, 'learning_rate': 1.2400303758020754e-05, 'epoch': 1.4359715696903987}
2025-04-28 11:00:32,309 - INFO - Training progress: {'loss': 0.4604, 'grad_norm': 1.113262414932251, 'learning_rate': 1.2400303758020754e-05, 'epoch': 1.4359715696903987}
2025-04-28 11:00:32,309 - INFO - Training metrics: {'loss': 0.4604, 'grad_norm': 1.113262414932251, 'learning_rate': 1.2400303758020754e-05, 'epoch': 1.4359715696903987}
2025-04-28 11:03:44,014 - INFO - INFO: Training progress: {'loss': 0.4749, 'grad_norm': 1.0756504535675049, 'learning_rate': 1.2365132334366268e-05, 'epoch': 1.4456089627755693}
2025-04-28 11:03:44,014 - INFO - Training progress: {'loss': 0.4749, 'grad_norm': 1.0756504535675049, 'learning_rate': 1.2365132334366268e-05, 'epoch': 1.4456089627755693}
2025-04-28 11:03:44,014 - INFO - Training metrics: {'loss': 0.4749, 'grad_norm': 1.0756504535675049, 'learning_rate': 1.2365132334366268e-05, 'epoch': 1.4456089627755693}
2025-04-28 11:06:55,438 - INFO - INFO: Training progress: {'loss': 0.4731, 'grad_norm': 1.0540659427642822, 'learning_rate': 1.2329775211052335e-05, 'epoch': 1.4552463558607398}
2025-04-28 11:06:55,438 - INFO - Training progress: {'loss': 0.4731, 'grad_norm': 1.0540659427642822, 'learning_rate': 1.2329775211052335e-05, 'epoch': 1.4552463558607398}
2025-04-28 11:06:55,438 - INFO - Training metrics: {'loss': 0.4731, 'grad_norm': 1.0540659427642822, 'learning_rate': 1.2329775211052335e-05, 'epoch': 1.4552463558607398}
2025-04-28 11:10:06,854 - INFO - INFO: Training progress: {'loss': 0.4359, 'grad_norm': 0.9568279385566711, 'learning_rate': 1.229423373764261e-05, 'epoch': 1.4648837489459101}
2025-04-28 11:10:06,854 - INFO - Training progress: {'loss': 0.4359, 'grad_norm': 0.9568279385566711, 'learning_rate': 1.229423373764261e-05, 'epoch': 1.4648837489459101}
2025-04-28 11:10:06,854 - INFO - Training metrics: {'loss': 0.4359, 'grad_norm': 0.9568279385566711, 'learning_rate': 1.229423373764261e-05, 'epoch': 1.4648837489459101}
2025-04-28 11:13:18,221 - INFO - INFO: Training progress: {'loss': 0.4741, 'grad_norm': 0.9534175992012024, 'learning_rate': 1.2258509270737293e-05, 'epoch': 1.4745211420310806}
2025-04-28 11:13:18,221 - INFO - Training progress: {'loss': 0.4741, 'grad_norm': 0.9534175992012024, 'learning_rate': 1.2258509270737293e-05, 'epoch': 1.4745211420310806}
2025-04-28 11:13:18,221 - INFO - Training metrics: {'loss': 0.4741, 'grad_norm': 0.9534175992012024, 'learning_rate': 1.2258509270737293e-05, 'epoch': 1.4745211420310806}
2025-04-28 11:16:29,639 - INFO - INFO: Training progress: {'loss': 0.4175, 'grad_norm': 0.9420526027679443, 'learning_rate': 1.2222603173921357e-05, 'epoch': 1.4841585351162512}
2025-04-28 11:16:29,639 - INFO - Training progress: {'loss': 0.4175, 'grad_norm': 0.9420526027679443, 'learning_rate': 1.2222603173921357e-05, 'epoch': 1.4841585351162512}
2025-04-28 11:16:29,639 - INFO - Training metrics: {'loss': 0.4175, 'grad_norm': 0.9420526027679443, 'learning_rate': 1.2222603173921357e-05, 'epoch': 1.4841585351162512}
2025-04-28 11:19:41,461 - INFO - INFO: Training progress: {'loss': 0.4297, 'grad_norm': 1.299355149269104, 'learning_rate': 1.2186516817712493e-05, 'epoch': 1.4937959282014215}
2025-04-28 11:19:41,461 - INFO - Training progress: {'loss': 0.4297, 'grad_norm': 1.299355149269104, 'learning_rate': 1.2186516817712493e-05, 'epoch': 1.4937959282014215}
2025-04-28 11:19:41,461 - INFO - Training metrics: {'loss': 0.4297, 'grad_norm': 1.299355149269104, 'learning_rate': 1.2186516817712493e-05, 'epoch': 1.4937959282014215}
2025-04-28 11:22:53,327 - INFO - INFO: Training progress: {'loss': 0.4238, 'grad_norm': 1.1407408714294434, 'learning_rate': 1.215025157950881e-05, 'epoch': 1.503433321286592}
2025-04-28 11:22:53,327 - INFO - Training progress: {'loss': 0.4238, 'grad_norm': 1.1407408714294434, 'learning_rate': 1.215025157950881e-05, 'epoch': 1.503433321286592}
2025-04-28 11:22:53,327 - INFO - Training metrics: {'loss': 0.4238, 'grad_norm': 1.1407408714294434, 'learning_rate': 1.215025157950881e-05, 'epoch': 1.503433321286592}
2025-04-28 11:26:04,587 - INFO - INFO: Training progress: {'loss': 0.4258, 'grad_norm': 1.1721693277359009, 'learning_rate': 1.211380884353625e-05, 'epoch': 1.5130707143717625}
2025-04-28 11:26:04,587 - INFO - Training progress: {'loss': 0.4258, 'grad_norm': 1.1721693277359009, 'learning_rate': 1.211380884353625e-05, 'epoch': 1.5130707143717625}
2025-04-28 11:26:04,587 - INFO - Training metrics: {'loss': 0.4258, 'grad_norm': 1.1721693277359009, 'learning_rate': 1.211380884353625e-05, 'epoch': 1.5130707143717625}
2025-04-28 11:29:16,054 - INFO - INFO: Training progress: {'loss': 0.45, 'grad_norm': 1.1074864864349365, 'learning_rate': 1.2077190000795753e-05, 'epoch': 1.5227081074569329}
2025-04-28 11:29:16,054 - INFO - Training progress: {'loss': 0.45, 'grad_norm': 1.1074864864349365, 'learning_rate': 1.2077190000795753e-05, 'epoch': 1.5227081074569329}
2025-04-28 11:29:16,054 - INFO - Training metrics: {'loss': 0.45, 'grad_norm': 1.1074864864349365, 'learning_rate': 1.2077190000795753e-05, 'epoch': 1.5227081074569329}
2025-04-28 11:32:27,396 - INFO - INFO: Training progress: {'loss': 0.4526, 'grad_norm': 1.1800923347473145, 'learning_rate': 1.2040396449010166e-05, 'epoch': 1.5323455005421034}
2025-04-28 11:32:27,412 - INFO - Training progress: {'loss': 0.4526, 'grad_norm': 1.1800923347473145, 'learning_rate': 1.2040396449010166e-05, 'epoch': 1.5323455005421034}
2025-04-28 11:32:27,412 - INFO - Training metrics: {'loss': 0.4526, 'grad_norm': 1.1800923347473145, 'learning_rate': 1.2040396449010166e-05, 'epoch': 1.5323455005421034}
2025-04-28 11:35:38,470 - INFO - INFO: Training progress: {'loss': 0.4937, 'grad_norm': 1.140028715133667, 'learning_rate': 1.2003429592570901e-05, 'epoch': 1.541982893627274}
2025-04-28 11:35:38,470 - INFO - Training progress: {'loss': 0.4937, 'grad_norm': 1.140028715133667, 'learning_rate': 1.2003429592570901e-05, 'epoch': 1.541982893627274}
2025-04-28 11:35:38,470 - INFO - Training metrics: {'loss': 0.4937, 'grad_norm': 1.140028715133667, 'learning_rate': 1.2003429592570901e-05, 'epoch': 1.541982893627274}
2025-04-28 11:35:39,265 - INFO - INFO: Saving checkpoint at step 1600
2025-04-28 11:35:39,265 - INFO - Saving checkpoint at step 1600
2025-04-28 11:35:39,265 - INFO - Saving checkpoint at step 1600
2025-04-28 11:38:50,454 - INFO - INFO: Training progress: {'loss': 0.4808, 'grad_norm': 1.3538618087768555, 'learning_rate': 1.1966290842484306e-05, 'epoch': 1.5516202867124442}
2025-04-28 11:38:50,454 - INFO - Training progress: {'loss': 0.4808, 'grad_norm': 1.3538618087768555, 'learning_rate': 1.1966290842484306e-05, 'epoch': 1.5516202867124442}
2025-04-28 11:38:50,454 - INFO - Training metrics: {'loss': 0.4808, 'grad_norm': 1.3538618087768555, 'learning_rate': 1.1966290842484306e-05, 'epoch': 1.5516202867124442}
2025-04-28 11:42:02,146 - INFO - INFO: Training progress: {'loss': 0.47, 'grad_norm': 1.561150074005127, 'learning_rate': 1.1928981616317837e-05, 'epoch': 1.5612576797976148}
2025-04-28 11:42:02,146 - INFO - Training progress: {'loss': 0.47, 'grad_norm': 1.561150074005127, 'learning_rate': 1.1928981616317837e-05, 'epoch': 1.5612576797976148}
2025-04-28 11:42:02,146 - INFO - Training metrics: {'loss': 0.47, 'grad_norm': 1.561150074005127, 'learning_rate': 1.1928981616317837e-05, 'epoch': 1.5612576797976148}
2025-04-28 11:45:13,603 - INFO - INFO: Training progress: {'loss': 0.4221, 'grad_norm': 1.4480100870132446, 'learning_rate': 1.1891503338145933e-05, 'epoch': 1.5708950728827853}
2025-04-28 11:45:13,603 - INFO - Training progress: {'loss': 0.4221, 'grad_norm': 1.4480100870132446, 'learning_rate': 1.1891503338145933e-05, 'epoch': 1.5708950728827853}
2025-04-28 11:45:13,603 - INFO - Training metrics: {'loss': 0.4221, 'grad_norm': 1.4480100870132446, 'learning_rate': 1.1891503338145933e-05, 'epoch': 1.5708950728827853}
2025-04-28 11:48:25,219 - INFO - INFO: Training progress: {'loss': 0.3994, 'grad_norm': 1.0267366170883179, 'learning_rate': 1.185385743849566e-05, 'epoch': 1.5805324659679556}
2025-04-28 11:48:25,219 - INFO - Training progress: {'loss': 0.3994, 'grad_norm': 1.0267366170883179, 'learning_rate': 1.185385743849566e-05, 'epoch': 1.5805324659679556}
2025-04-28 11:48:25,219 - INFO - Training metrics: {'loss': 0.3994, 'grad_norm': 1.0267366170883179, 'learning_rate': 1.185385743849566e-05, 'epoch': 1.5805324659679556}
2025-04-28 11:51:36,866 - INFO - INFO: Training progress: {'loss': 0.3968, 'grad_norm': 1.1660469770431519, 'learning_rate': 1.1816045354292113e-05, 'epoch': 1.5901698590531261}
2025-04-28 11:51:36,866 - INFO - Training progress: {'loss': 0.3968, 'grad_norm': 1.1660469770431519, 'learning_rate': 1.1816045354292113e-05, 'epoch': 1.5901698590531261}
2025-04-28 11:51:36,866 - INFO - Training metrics: {'loss': 0.3968, 'grad_norm': 1.1660469770431519, 'learning_rate': 1.1816045354292113e-05, 'epoch': 1.5901698590531261}
2025-04-28 11:54:48,394 - INFO - INFO: Training progress: {'loss': 0.4178, 'grad_norm': 1.4787617921829224, 'learning_rate': 1.1778068528803568e-05, 'epoch': 1.5998072521382967}
2025-04-28 11:54:48,394 - INFO - Training progress: {'loss': 0.4178, 'grad_norm': 1.4787617921829224, 'learning_rate': 1.1778068528803568e-05, 'epoch': 1.5998072521382967}
2025-04-28 11:54:48,394 - INFO - Training metrics: {'loss': 0.4178, 'grad_norm': 1.4787617921829224, 'learning_rate': 1.1778068528803568e-05, 'epoch': 1.5998072521382967}
2025-04-28 11:58:00,110 - INFO - INFO: Training progress: {'loss': 0.434, 'grad_norm': 1.0878082513809204, 'learning_rate': 1.1739928411586397e-05, 'epoch': 1.609444645223467}
2025-04-28 11:58:00,110 - INFO - Training progress: {'loss': 0.434, 'grad_norm': 1.0878082513809204, 'learning_rate': 1.1739928411586397e-05, 'epoch': 1.609444645223467}
2025-04-28 11:58:00,110 - INFO - Training metrics: {'loss': 0.434, 'grad_norm': 1.0878082513809204, 'learning_rate': 1.1739928411586397e-05, 'epoch': 1.609444645223467}
2025-04-28 12:01:11,861 - INFO - INFO: Training progress: {'loss': 0.4808, 'grad_norm': 1.4069480895996094, 'learning_rate': 1.1701626458429728e-05, 'epoch': 1.6190820383086375}
2025-04-28 12:01:11,861 - INFO - Training progress: {'loss': 0.4808, 'grad_norm': 1.4069480895996094, 'learning_rate': 1.1701626458429728e-05, 'epoch': 1.6190820383086375}
2025-04-28 12:01:11,861 - INFO - Training metrics: {'loss': 0.4808, 'grad_norm': 1.4069480895996094, 'learning_rate': 1.1701626458429728e-05, 'epoch': 1.6190820383086375}
2025-04-28 12:16:14,246 - INFO - INFO: Training progress: {'eval_loss': 0.41216111183166504, 'eval_runtime': 902.3854, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.6190820383086375}
2025-04-28 12:16:14,246 - INFO - Training progress: {'eval_loss': 0.41216111183166504, 'eval_runtime': 902.3854, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.6190820383086375}
2025-04-28 12:16:14,246 - INFO - Training metrics: {'eval_loss': 0.41216111183166504, 'eval_runtime': 902.3854, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.6190820383086375}
2025-04-28 12:19:25,776 - INFO - INFO: Training progress: {'loss': 0.4292, 'grad_norm': 1.0397324562072754, 'learning_rate': 1.1663164131299892e-05, 'epoch': 1.628719431393808}
2025-04-28 12:19:25,776 - INFO - Training progress: {'loss': 0.4292, 'grad_norm': 1.0397324562072754, 'learning_rate': 1.1663164131299892e-05, 'epoch': 1.628719431393808}
2025-04-28 12:19:25,776 - INFO - Training metrics: {'loss': 0.4292, 'grad_norm': 1.0397324562072754, 'learning_rate': 1.1663164131299892e-05, 'epoch': 1.628719431393808}
2025-04-28 12:22:36,903 - INFO - INFO: Training progress: {'loss': 0.4754, 'grad_norm': 0.9851894378662109, 'learning_rate': 1.1624542898284614e-05, 'epoch': 1.6383568244789783}
2025-04-28 12:22:36,903 - INFO - Training progress: {'loss': 0.4754, 'grad_norm': 0.9851894378662109, 'learning_rate': 1.1624542898284614e-05, 'epoch': 1.6383568244789783}
2025-04-28 12:22:36,903 - INFO - Training metrics: {'loss': 0.4754, 'grad_norm': 0.9851894378662109, 'learning_rate': 1.1624542898284614e-05, 'epoch': 1.6383568244789783}
2025-04-28 12:25:48,511 - INFO - INFO: Training progress: {'loss': 0.439, 'grad_norm': 1.1772056818008423, 'learning_rate': 1.158576423353697e-05, 'epoch': 1.6479942175641489}
2025-04-28 12:25:48,511 - INFO - Training progress: {'loss': 0.439, 'grad_norm': 1.1772056818008423, 'learning_rate': 1.158576423353697e-05, 'epoch': 1.6479942175641489}
2025-04-28 12:25:48,511 - INFO - Training metrics: {'loss': 0.439, 'grad_norm': 1.1772056818008423, 'learning_rate': 1.158576423353697e-05, 'epoch': 1.6479942175641489}
2025-04-28 12:28:59,700 - INFO - INFO: Training progress: {'loss': 0.3988, 'grad_norm': 1.2767263650894165, 'learning_rate': 1.1546829617219133e-05, 'epoch': 1.6576316106493194}
2025-04-28 12:28:59,700 - INFO - Training progress: {'loss': 0.3988, 'grad_norm': 1.2767263650894165, 'learning_rate': 1.1546829617219133e-05, 'epoch': 1.6576316106493194}
2025-04-28 12:28:59,700 - INFO - Training metrics: {'loss': 0.3988, 'grad_norm': 1.2767263650894165, 'learning_rate': 1.1546829617219133e-05, 'epoch': 1.6576316106493194}
2025-04-28 12:32:11,383 - INFO - INFO: Training progress: {'loss': 0.4215, 'grad_norm': 1.3753904104232788, 'learning_rate': 1.150774053544587e-05, 'epoch': 1.6672690037344897}
2025-04-28 12:32:11,383 - INFO - Training progress: {'loss': 0.4215, 'grad_norm': 1.3753904104232788, 'learning_rate': 1.150774053544587e-05, 'epoch': 1.6672690037344897}
2025-04-28 12:32:11,383 - INFO - Training metrics: {'loss': 0.4215, 'grad_norm': 1.3753904104232788, 'learning_rate': 1.150774053544587e-05, 'epoch': 1.6672690037344897}
2025-04-28 12:35:22,685 - INFO - INFO: Training progress: {'loss': 0.4749, 'grad_norm': 1.1582131385803223, 'learning_rate': 1.1468498480227808e-05, 'epoch': 1.6769063968196602}
2025-04-28 12:35:22,685 - INFO - Training progress: {'loss': 0.4749, 'grad_norm': 1.1582131385803223, 'learning_rate': 1.1468498480227808e-05, 'epoch': 1.6769063968196602}
2025-04-28 12:35:22,685 - INFO - Training metrics: {'loss': 0.4749, 'grad_norm': 1.1582131385803223, 'learning_rate': 1.1468498480227808e-05, 'epoch': 1.6769063968196602}
2025-04-28 12:38:34,013 - INFO - INFO: Training progress: {'loss': 0.4463, 'grad_norm': 1.1030272245407104, 'learning_rate': 1.1429104949414502e-05, 'epoch': 1.6865437899048308}
2025-04-28 12:38:34,013 - INFO - Training progress: {'loss': 0.4463, 'grad_norm': 1.1030272245407104, 'learning_rate': 1.1429104949414502e-05, 'epoch': 1.6865437899048308}
2025-04-28 12:38:34,013 - INFO - Training metrics: {'loss': 0.4463, 'grad_norm': 1.1030272245407104, 'learning_rate': 1.1429104949414502e-05, 'epoch': 1.6865437899048308}
2025-04-28 12:41:45,291 - INFO - INFO: Training progress: {'loss': 0.4326, 'grad_norm': 1.3288941383361816, 'learning_rate': 1.1389561446637253e-05, 'epoch': 1.696181182990001}
2025-04-28 12:41:45,291 - INFO - Training progress: {'loss': 0.4326, 'grad_norm': 1.3288941383361816, 'learning_rate': 1.1389561446637253e-05, 'epoch': 1.696181182990001}
2025-04-28 12:41:45,291 - INFO - Training metrics: {'loss': 0.4326, 'grad_norm': 1.3288941383361816, 'learning_rate': 1.1389561446637253e-05, 'epoch': 1.696181182990001}
2025-04-28 12:44:56,880 - INFO - INFO: Training progress: {'loss': 0.4331, 'grad_norm': 1.2544018030166626, 'learning_rate': 1.1349869481251715e-05, 'epoch': 1.7058185760751716}
2025-04-28 12:44:56,880 - INFO - Training progress: {'loss': 0.4331, 'grad_norm': 1.2544018030166626, 'learning_rate': 1.1349869481251715e-05, 'epoch': 1.7058185760751716}
2025-04-28 12:44:56,880 - INFO - Training metrics: {'loss': 0.4331, 'grad_norm': 1.2544018030166626, 'learning_rate': 1.1349869481251715e-05, 'epoch': 1.7058185760751716}
2025-04-28 12:48:08,389 - INFO - INFO: Training progress: {'loss': 0.4515, 'grad_norm': 1.350550651550293, 'learning_rate': 1.1310030568280283e-05, 'epoch': 1.7154559691603422}
2025-04-28 12:48:08,389 - INFO - Training progress: {'loss': 0.4515, 'grad_norm': 1.350550651550293, 'learning_rate': 1.1310030568280283e-05, 'epoch': 1.7154559691603422}
2025-04-28 12:48:08,389 - INFO - Training metrics: {'loss': 0.4515, 'grad_norm': 1.350550651550293, 'learning_rate': 1.1310030568280283e-05, 'epoch': 1.7154559691603422}
2025-04-28 12:51:20,070 - INFO - INFO: Training progress: {'loss': 0.4101, 'grad_norm': 1.1225541830062866, 'learning_rate': 1.1270046228354278e-05, 'epoch': 1.7250933622455125}
2025-04-28 12:51:20,070 - INFO - Training progress: {'loss': 0.4101, 'grad_norm': 1.1225541830062866, 'learning_rate': 1.1270046228354278e-05, 'epoch': 1.7250933622455125}
2025-04-28 12:51:20,070 - INFO - Training metrics: {'loss': 0.4101, 'grad_norm': 1.1225541830062866, 'learning_rate': 1.1270046228354278e-05, 'epoch': 1.7250933622455125}
2025-04-28 12:54:31,515 - INFO - INFO: Training progress: {'loss': 0.4325, 'grad_norm': 1.1514934301376343, 'learning_rate': 1.1229917987655879e-05, 'epoch': 1.7347307553306832}
2025-04-28 12:54:31,515 - INFO - Training progress: {'loss': 0.4325, 'grad_norm': 1.1514934301376343, 'learning_rate': 1.1229917987655879e-05, 'epoch': 1.7347307553306832}
2025-04-28 12:54:31,515 - INFO - Training metrics: {'loss': 0.4325, 'grad_norm': 1.1514934301376343, 'learning_rate': 1.1229917987655879e-05, 'epoch': 1.7347307553306832}
2025-04-28 12:54:32,304 - INFO - INFO: Saving checkpoint at step 1800
2025-04-28 12:54:32,304 - INFO - Saving checkpoint at step 1800
2025-04-28 12:54:32,304 - INFO - Saving checkpoint at step 1800
2025-04-28 12:57:44,047 - INFO - INFO: Training progress: {'loss': 0.4322, 'grad_norm': 1.443568468093872, 'learning_rate': 1.1189647377859896e-05, 'epoch': 1.7443681484158535}
2025-04-28 12:57:44,047 - INFO - Training progress: {'loss': 0.4322, 'grad_norm': 1.443568468093872, 'learning_rate': 1.1189647377859896e-05, 'epoch': 1.7443681484158535}
2025-04-28 12:57:44,047 - INFO - Training metrics: {'loss': 0.4322, 'grad_norm': 1.443568468093872, 'learning_rate': 1.1189647377859896e-05, 'epoch': 1.7443681484158535}
2025-04-28 13:00:55,719 - INFO - INFO: Training progress: {'loss': 0.3758, 'grad_norm': 1.1933387517929077, 'learning_rate': 1.1149235936075298e-05, 'epoch': 1.7540055415010238}
2025-04-28 13:00:55,719 - INFO - Training progress: {'loss': 0.3758, 'grad_norm': 1.1933387517929077, 'learning_rate': 1.1149235936075298e-05, 'epoch': 1.7540055415010238}
2025-04-28 13:00:55,719 - INFO - Training metrics: {'loss': 0.3758, 'grad_norm': 1.1933387517929077, 'learning_rate': 1.1149235936075298e-05, 'epoch': 1.7540055415010238}
2025-04-28 13:04:07,154 - INFO - INFO: Training progress: {'loss': 0.4488, 'grad_norm': 1.1318975687026978, 'learning_rate': 1.1108685204786529e-05, 'epoch': 1.7636429345861946}
2025-04-28 13:04:07,154 - INFO - Training progress: {'loss': 0.4488, 'grad_norm': 1.1318975687026978, 'learning_rate': 1.1108685204786529e-05, 'epoch': 1.7636429345861946}
2025-04-28 13:04:07,154 - INFO - Training metrics: {'loss': 0.4488, 'grad_norm': 1.1318975687026978, 'learning_rate': 1.1108685204786529e-05, 'epoch': 1.7636429345861946}
2025-04-28 13:07:18,545 - INFO - INFO: Training progress: {'loss': 0.3692, 'grad_norm': 1.419608473777771, 'learning_rate': 1.1067996731794654e-05, 'epoch': 1.773280327671365}
2025-04-28 13:07:18,545 - INFO - Training progress: {'loss': 0.3692, 'grad_norm': 1.419608473777771, 'learning_rate': 1.1067996731794654e-05, 'epoch': 1.773280327671365}
2025-04-28 13:07:18,545 - INFO - Training metrics: {'loss': 0.3692, 'grad_norm': 1.419608473777771, 'learning_rate': 1.1067996731794654e-05, 'epoch': 1.773280327671365}
2025-04-28 13:10:30,328 - INFO - INFO: Training progress: {'loss': 0.4491, 'grad_norm': 1.2168093919754028, 'learning_rate': 1.1027172070158267e-05, 'epoch': 1.7829177207565352}
2025-04-28 13:10:30,328 - INFO - Training progress: {'loss': 0.4491, 'grad_norm': 1.2168093919754028, 'learning_rate': 1.1027172070158267e-05, 'epoch': 1.7829177207565352}
2025-04-28 13:10:30,328 - INFO - Training metrics: {'loss': 0.4491, 'grad_norm': 1.2168093919754028, 'learning_rate': 1.1027172070158267e-05, 'epoch': 1.7829177207565352}
2025-04-28 13:13:41,839 - INFO - INFO: Training progress: {'loss': 0.4561, 'grad_norm': 1.4199035167694092, 'learning_rate': 1.0986212778134208e-05, 'epoch': 1.792555113841706}
2025-04-28 13:13:41,839 - INFO - Training progress: {'loss': 0.4561, 'grad_norm': 1.4199035167694092, 'learning_rate': 1.0986212778134208e-05, 'epoch': 1.792555113841706}
2025-04-28 13:13:41,839 - INFO - Training metrics: {'loss': 0.4561, 'grad_norm': 1.4199035167694092, 'learning_rate': 1.0986212778134208e-05, 'epoch': 1.792555113841706}
2025-04-28 13:16:54,007 - INFO - INFO: Training progress: {'loss': 0.4557, 'grad_norm': 1.074397087097168, 'learning_rate': 1.0945120419118104e-05, 'epoch': 1.8021925069268763}
2025-04-28 13:16:54,007 - INFO - Training progress: {'loss': 0.4557, 'grad_norm': 1.074397087097168, 'learning_rate': 1.0945120419118104e-05, 'epoch': 1.8021925069268763}
2025-04-28 13:16:54,007 - INFO - Training metrics: {'loss': 0.4557, 'grad_norm': 1.074397087097168, 'learning_rate': 1.0945120419118104e-05, 'epoch': 1.8021925069268763}
2025-04-28 13:20:05,868 - INFO - INFO: Training progress: {'loss': 0.3971, 'grad_norm': 1.1748688220977783, 'learning_rate': 1.090389656158467e-05, 'epoch': 1.8118299000120466}
2025-04-28 13:20:05,868 - INFO - Training progress: {'loss': 0.3971, 'grad_norm': 1.1748688220977783, 'learning_rate': 1.090389656158467e-05, 'epoch': 1.8118299000120466}
2025-04-28 13:20:05,868 - INFO - Training metrics: {'loss': 0.3971, 'grad_norm': 1.1748688220977783, 'learning_rate': 1.090389656158467e-05, 'epoch': 1.8118299000120466}
2025-04-28 13:23:17,330 - INFO - INFO: Training progress: {'loss': 0.4083, 'grad_norm': 1.3298122882843018, 'learning_rate': 1.086254277902786e-05, 'epoch': 1.8214672930972173}
2025-04-28 13:23:17,330 - INFO - Training progress: {'loss': 0.4083, 'grad_norm': 1.3298122882843018, 'learning_rate': 1.086254277902786e-05, 'epoch': 1.8214672930972173}
2025-04-28 13:23:17,330 - INFO - Training metrics: {'loss': 0.4083, 'grad_norm': 1.3298122882843018, 'learning_rate': 1.086254277902786e-05, 'epoch': 1.8214672930972173}
2025-04-28 13:38:23,672 - INFO - INFO: Training progress: {'eval_loss': 0.40130898356437683, 'eval_runtime': 906.3415, 'eval_samples_per_second': 1.716, 'eval_steps_per_second': 1.716, 'epoch': 1.8214672930972173}
2025-04-28 13:38:23,688 - INFO - Training progress: {'eval_loss': 0.40130898356437683, 'eval_runtime': 906.3415, 'eval_samples_per_second': 1.716, 'eval_steps_per_second': 1.716, 'epoch': 1.8214672930972173}
2025-04-28 13:38:23,688 - INFO - Training metrics: {'eval_loss': 0.40130898356437683, 'eval_runtime': 906.3415, 'eval_samples_per_second': 1.716, 'eval_steps_per_second': 1.716, 'epoch': 1.8214672930972173}
2025-04-28 13:41:35,446 - INFO - INFO: Training progress: {'loss': 0.4258, 'grad_norm': 1.0566327571868896, 'learning_rate': 1.0821060649900797e-05, 'epoch': 1.8311046861823876}
2025-04-28 13:41:35,446 - INFO - Training progress: {'loss': 0.4258, 'grad_norm': 1.0566327571868896, 'learning_rate': 1.0821060649900797e-05, 'epoch': 1.8311046861823876}
2025-04-28 13:41:35,446 - INFO - Training metrics: {'loss': 0.4258, 'grad_norm': 1.0566327571868896, 'learning_rate': 1.0821060649900797e-05, 'epoch': 1.8311046861823876}
2025-04-28 13:44:47,303 - INFO - INFO: Training progress: {'loss': 0.4348, 'grad_norm': 0.984944760799408, 'learning_rate': 1.0779451757555533e-05, 'epoch': 1.8407420792675582}
2025-04-28 13:44:47,303 - INFO - Training progress: {'loss': 0.4348, 'grad_norm': 0.984944760799408, 'learning_rate': 1.0779451757555533e-05, 'epoch': 1.8407420792675582}
2025-04-28 13:44:47,303 - INFO - Training metrics: {'loss': 0.4348, 'grad_norm': 0.984944760799408, 'learning_rate': 1.0779451757555533e-05, 'epoch': 1.8407420792675582}
2025-04-28 13:47:59,234 - INFO - INFO: Training progress: {'loss': 0.4189, 'grad_norm': 1.0635192394256592, 'learning_rate': 1.0737717690182602e-05, 'epoch': 1.8503794723527287}
2025-04-28 13:47:59,234 - INFO - Training progress: {'loss': 0.4189, 'grad_norm': 1.0635192394256592, 'learning_rate': 1.0737717690182602e-05, 'epoch': 1.8503794723527287}
2025-04-28 13:47:59,234 - INFO - Training metrics: {'loss': 0.4189, 'grad_norm': 1.0635192394256592, 'learning_rate': 1.0737717690182602e-05, 'epoch': 1.8503794723527287}
2025-04-28 13:51:11,015 - INFO - INFO: Training progress: {'loss': 0.4085, 'grad_norm': 0.9394209980964661, 'learning_rate': 1.0695860040750416e-05, 'epoch': 1.860016865437899}
2025-04-28 13:51:11,015 - INFO - Training progress: {'loss': 0.4085, 'grad_norm': 0.9394209980964661, 'learning_rate': 1.0695860040750416e-05, 'epoch': 1.860016865437899}
2025-04-28 13:51:11,015 - INFO - Training metrics: {'loss': 0.4085, 'grad_norm': 0.9394209980964661, 'learning_rate': 1.0695860040750416e-05, 'epoch': 1.860016865437899}
2025-04-28 13:54:22,953 - INFO - INFO: Training progress: {'loss': 0.4206, 'grad_norm': 1.3600109815597534, 'learning_rate': 1.065388040694444e-05, 'epoch': 1.8696542585230695}
2025-04-28 13:54:22,953 - INFO - Training progress: {'loss': 0.4206, 'grad_norm': 1.3600109815597534, 'learning_rate': 1.065388040694444e-05, 'epoch': 1.8696542585230695}
2025-04-28 13:54:22,953 - INFO - Training metrics: {'loss': 0.4206, 'grad_norm': 1.3600109815597534, 'learning_rate': 1.065388040694444e-05, 'epoch': 1.8696542585230695}
2025-04-28 13:57:34,866 - INFO - INFO: Training progress: {'loss': 0.4082, 'grad_norm': 1.2315527200698853, 'learning_rate': 1.0611780391106231e-05, 'epoch': 1.87929165160824}
2025-04-28 13:57:34,866 - INFO - Training progress: {'loss': 0.4082, 'grad_norm': 1.2315527200698853, 'learning_rate': 1.0611780391106231e-05, 'epoch': 1.87929165160824}
2025-04-28 13:57:34,866 - INFO - Training metrics: {'loss': 0.4082, 'grad_norm': 1.2315527200698853, 'learning_rate': 1.0611780391106231e-05, 'epoch': 1.87929165160824}
2025-04-28 14:00:46,561 - INFO - INFO: Training progress: {'loss': 0.4258, 'grad_norm': 1.2771981954574585, 'learning_rate': 1.0569561600172262e-05, 'epoch': 1.8889290446934104}
2025-04-28 14:00:46,561 - INFO - Training progress: {'loss': 0.4258, 'grad_norm': 1.2771981954574585, 'learning_rate': 1.0569561600172262e-05, 'epoch': 1.8889290446934104}
2025-04-28 14:00:46,561 - INFO - Training metrics: {'loss': 0.4258, 'grad_norm': 1.2771981954574585, 'learning_rate': 1.0569561600172262e-05, 'epoch': 1.8889290446934104}
2025-04-28 14:03:58,552 - INFO - INFO: Training progress: {'loss': 0.4712, 'grad_norm': 1.0940158367156982, 'learning_rate': 1.0527225645612592e-05, 'epoch': 1.898566437778581}
2025-04-28 14:03:58,552 - INFO - Training progress: {'loss': 0.4712, 'grad_norm': 1.0940158367156982, 'learning_rate': 1.0527225645612592e-05, 'epoch': 1.898566437778581}
2025-04-28 14:03:58,552 - INFO - Training metrics: {'loss': 0.4712, 'grad_norm': 1.0940158367156982, 'learning_rate': 1.0527225645612592e-05, 'epoch': 1.898566437778581}
2025-04-28 14:07:10,621 - INFO - INFO: Training progress: {'loss': 0.4242, 'grad_norm': 1.4098376035690308, 'learning_rate': 1.0484774143369361e-05, 'epoch': 1.9082038308637514}
2025-04-28 14:07:10,621 - INFO - Training progress: {'loss': 0.4242, 'grad_norm': 1.4098376035690308, 'learning_rate': 1.0484774143369361e-05, 'epoch': 1.9082038308637514}
2025-04-28 14:07:10,621 - INFO - Training metrics: {'loss': 0.4242, 'grad_norm': 1.4098376035690308, 'learning_rate': 1.0484774143369361e-05, 'epoch': 1.9082038308637514}
2025-04-28 14:10:22,539 - INFO - INFO: Training progress: {'loss': 0.414, 'grad_norm': 1.2631596326828003, 'learning_rate': 1.0442208713795108e-05, 'epoch': 1.9178412239489218}
2025-04-28 14:10:22,539 - INFO - Training progress: {'loss': 0.414, 'grad_norm': 1.2631596326828003, 'learning_rate': 1.0442208713795108e-05, 'epoch': 1.9178412239489218}
2025-04-28 14:10:22,539 - INFO - Training metrics: {'loss': 0.414, 'grad_norm': 1.2631596326828003, 'learning_rate': 1.0442208713795108e-05, 'epoch': 1.9178412239489218}
2025-04-28 14:13:34,138 - INFO - INFO: Training progress: {'loss': 0.4278, 'grad_norm': 1.2415930032730103, 'learning_rate': 1.039953098159091e-05, 'epoch': 1.9274786170340923}
2025-04-28 14:13:34,138 - INFO - Training progress: {'loss': 0.4278, 'grad_norm': 1.2415930032730103, 'learning_rate': 1.039953098159091e-05, 'epoch': 1.9274786170340923}
2025-04-28 14:13:34,138 - INFO - Training metrics: {'loss': 0.4278, 'grad_norm': 1.2415930032730103, 'learning_rate': 1.039953098159091e-05, 'epoch': 1.9274786170340923}
2025-04-28 14:13:34,943 - INFO - INFO: Saving checkpoint at step 2000
2025-04-28 14:13:34,943 - INFO - Saving checkpoint at step 2000
2025-04-28 14:13:34,943 - INFO - Saving checkpoint at step 2000
2025-04-28 14:16:46,946 - INFO - INFO: Training progress: {'loss': 0.4499, 'grad_norm': 1.179002285003662, 'learning_rate': 1.0356742575744386e-05, 'epoch': 1.9371160101192628}
2025-04-28 14:16:46,946 - INFO - Training progress: {'loss': 0.4499, 'grad_norm': 1.179002285003662, 'learning_rate': 1.0356742575744386e-05, 'epoch': 1.9371160101192628}
2025-04-28 14:16:46,946 - INFO - Training metrics: {'loss': 0.4499, 'grad_norm': 1.179002285003662, 'learning_rate': 1.0356742575744386e-05, 'epoch': 1.9371160101192628}
2025-04-28 14:19:58,592 - INFO - INFO: Training progress: {'loss': 0.4209, 'grad_norm': 1.301835060119629, 'learning_rate': 1.0313845129467516e-05, 'epoch': 1.9467534032044331}
2025-04-28 14:19:58,592 - INFO - Training progress: {'loss': 0.4209, 'grad_norm': 1.301835060119629, 'learning_rate': 1.0313845129467516e-05, 'epoch': 1.9467534032044331}
2025-04-28 14:19:58,592 - INFO - Training metrics: {'loss': 0.4209, 'grad_norm': 1.301835060119629, 'learning_rate': 1.0313845129467516e-05, 'epoch': 1.9467534032044331}
2025-04-28 14:23:09,998 - INFO - INFO: Training progress: {'loss': 0.4213, 'grad_norm': 1.2381173372268677, 'learning_rate': 1.0270840280134293e-05, 'epoch': 1.9563907962896037}
2025-04-28 14:23:09,998 - INFO - Training progress: {'loss': 0.4213, 'grad_norm': 1.2381173372268677, 'learning_rate': 1.0270840280134293e-05, 'epoch': 1.9563907962896037}
2025-04-28 14:23:09,998 - INFO - Training metrics: {'loss': 0.4213, 'grad_norm': 1.2381173372268677, 'learning_rate': 1.0270840280134293e-05, 'epoch': 1.9563907962896037}
2025-04-28 14:26:22,011 - INFO - INFO: Training progress: {'loss': 0.4061, 'grad_norm': 1.3219218254089355, 'learning_rate': 1.0227729669218229e-05, 'epoch': 1.9660281893747742}
2025-04-28 14:26:22,011 - INFO - Training progress: {'loss': 0.4061, 'grad_norm': 1.3219218254089355, 'learning_rate': 1.0227729669218229e-05, 'epoch': 1.9660281893747742}
2025-04-28 14:26:22,011 - INFO - Training metrics: {'loss': 0.4061, 'grad_norm': 1.3219218254089355, 'learning_rate': 1.0227729669218229e-05, 'epoch': 1.9660281893747742}
2025-04-28 14:29:33,686 - INFO - INFO: Training progress: {'loss': 0.4187, 'grad_norm': 1.3005452156066895, 'learning_rate': 1.0184514942229715e-05, 'epoch': 1.9756655824599445}
2025-04-28 14:29:33,686 - INFO - Training progress: {'loss': 0.4187, 'grad_norm': 1.3005452156066895, 'learning_rate': 1.0184514942229715e-05, 'epoch': 1.9756655824599445}
2025-04-28 14:29:33,686 - INFO - Training metrics: {'loss': 0.4187, 'grad_norm': 1.3005452156066895, 'learning_rate': 1.0184514942229715e-05, 'epoch': 1.9756655824599445}
2025-04-28 14:32:45,135 - INFO - INFO: Training progress: {'loss': 0.4597, 'grad_norm': 1.183774709701538, 'learning_rate': 1.0141197748653181e-05, 'epoch': 1.985302975545115}
2025-04-28 14:32:45,135 - INFO - Training progress: {'loss': 0.4597, 'grad_norm': 1.183774709701538, 'learning_rate': 1.0141197748653181e-05, 'epoch': 1.985302975545115}
2025-04-28 14:32:45,135 - INFO - Training metrics: {'loss': 0.4597, 'grad_norm': 1.183774709701538, 'learning_rate': 1.0141197748653181e-05, 'epoch': 1.985302975545115}
2025-04-28 14:35:57,023 - INFO - INFO: Training progress: {'loss': 0.4244, 'grad_norm': 1.2374149560928345, 'learning_rate': 1.009777974188417e-05, 'epoch': 1.9949403686302856}
2025-04-28 14:35:57,023 - INFO - Training progress: {'loss': 0.4244, 'grad_norm': 1.2374149560928345, 'learning_rate': 1.009777974188417e-05, 'epoch': 1.9949403686302856}
2025-04-28 14:35:57,023 - INFO - Training metrics: {'loss': 0.4244, 'grad_norm': 1.2374149560928345, 'learning_rate': 1.009777974188417e-05, 'epoch': 1.9949403686302856}
2025-04-28 14:37:38,074 - INFO - INFO: Starting epoch 1.9997590651728707/5
2025-04-28 14:37:38,074 - INFO - Starting epoch 1.9997590651728707/5
2025-04-28 14:37:38,074 - INFO - Starting epoch 1.9997590651728707/5
2025-04-28 14:39:17,283 - INFO - INFO: Training progress: {'loss': 0.3756, 'grad_norm': 1.523115634918213, 'learning_rate': 1.005426257916621e-05, 'epoch': 2.004577761715456}
2025-04-28 14:39:17,283 - INFO - Training progress: {'loss': 0.3756, 'grad_norm': 1.523115634918213, 'learning_rate': 1.005426257916621e-05, 'epoch': 2.004577761715456}
2025-04-28 14:39:17,283 - INFO - Training metrics: {'loss': 0.3756, 'grad_norm': 1.523115634918213, 'learning_rate': 1.005426257916621e-05, 'epoch': 2.004577761715456}
2025-04-28 14:42:28,802 - INFO - INFO: Training progress: {'loss': 0.3971, 'grad_norm': 1.0879586935043335, 'learning_rate': 1.001064792152756e-05, 'epoch': 2.0142151548006266}
2025-04-28 14:42:28,802 - INFO - Training progress: {'loss': 0.3971, 'grad_norm': 1.0879586935043335, 'learning_rate': 1.001064792152756e-05, 'epoch': 2.0142151548006266}
2025-04-28 14:42:28,802 - INFO - Training metrics: {'loss': 0.3971, 'grad_norm': 1.0879586935043335, 'learning_rate': 1.001064792152756e-05, 'epoch': 2.0142151548006266}
2025-04-28 14:45:40,291 - INFO - INFO: Training progress: {'loss': 0.422, 'grad_norm': 1.3849713802337646, 'learning_rate': 9.96693743371781e-06, 'epoch': 2.023852547885797}
2025-04-28 14:45:40,291 - INFO - Training progress: {'loss': 0.422, 'grad_norm': 1.3849713802337646, 'learning_rate': 9.96693743371781e-06, 'epoch': 2.023852547885797}
2025-04-28 14:45:40,291 - INFO - Training metrics: {'loss': 0.422, 'grad_norm': 1.3849713802337646, 'learning_rate': 9.96693743371781e-06, 'epoch': 2.023852547885797}
2025-04-28 15:00:45,954 - INFO - INFO: Training progress: {'eval_loss': 0.3907662332057953, 'eval_runtime': 905.6628, 'eval_samples_per_second': 1.717, 'eval_steps_per_second': 1.717, 'epoch': 2.023852547885797}
2025-04-28 15:00:45,954 - INFO - Training progress: {'eval_loss': 0.3907662332057953, 'eval_runtime': 905.6628, 'eval_samples_per_second': 1.717, 'eval_steps_per_second': 1.717, 'epoch': 2.023852547885797}
2025-04-28 15:00:45,954 - INFO - Training metrics: {'eval_loss': 0.3907662332057953, 'eval_runtime': 905.6628, 'eval_samples_per_second': 1.717, 'eval_steps_per_second': 1.717, 'epoch': 2.023852547885797}
2025-04-28 15:03:57,875 - INFO - INFO: Training progress: {'loss': 0.3914, 'grad_norm': 1.0108722448349, 'learning_rate': 9.923132784144347e-06, 'epoch': 2.0334899409709672}
2025-04-28 15:03:57,875 - INFO - Training progress: {'loss': 0.3914, 'grad_norm': 1.0108722448349, 'learning_rate': 9.923132784144347e-06, 'epoch': 2.0334899409709672}
2025-04-28 15:03:57,875 - INFO - Training metrics: {'loss': 0.3914, 'grad_norm': 1.0108722448349, 'learning_rate': 9.923132784144347e-06, 'epoch': 2.0334899409709672}
2025-04-28 15:07:09,750 - INFO - INFO: Training progress: {'loss': 0.4036, 'grad_norm': 1.19637930393219, 'learning_rate': 9.879235644808657e-06, 'epoch': 2.043127334056138}
2025-04-28 15:07:09,750 - INFO - Training progress: {'loss': 0.4036, 'grad_norm': 1.19637930393219, 'learning_rate': 9.879235644808657e-06, 'epoch': 2.043127334056138}
2025-04-28 15:07:09,750 - INFO - Training metrics: {'loss': 0.4036, 'grad_norm': 1.19637930393219, 'learning_rate': 9.879235644808657e-06, 'epoch': 2.043127334056138}
2025-04-28 15:10:21,447 - INFO - INFO: Training progress: {'loss': 0.3822, 'grad_norm': 1.4140247106552124, 'learning_rate': 9.835247691242516e-06, 'epoch': 2.0527647271413083}
2025-04-28 15:10:21,447 - INFO - Training progress: {'loss': 0.3822, 'grad_norm': 1.4140247106552124, 'learning_rate': 9.835247691242516e-06, 'epoch': 2.0527647271413083}
2025-04-28 15:10:21,447 - INFO - Training metrics: {'loss': 0.3822, 'grad_norm': 1.4140247106552124, 'learning_rate': 9.835247691242516e-06, 'epoch': 2.0527647271413083}
2025-04-28 15:13:33,394 - INFO - INFO: Training progress: {'loss': 0.4015, 'grad_norm': 1.1344894170761108, 'learning_rate': 9.791170602444039e-06, 'epoch': 2.0624021202264786}
2025-04-28 15:13:33,394 - INFO - Training progress: {'loss': 0.4015, 'grad_norm': 1.1344894170761108, 'learning_rate': 9.791170602444039e-06, 'epoch': 2.0624021202264786}
2025-04-28 15:13:33,394 - INFO - Training metrics: {'loss': 0.4015, 'grad_norm': 1.1344894170761108, 'learning_rate': 9.791170602444039e-06, 'epoch': 2.0624021202264786}
2025-04-28 15:16:45,118 - INFO - INFO: Training progress: {'loss': 0.3791, 'grad_norm': 1.3176078796386719, 'learning_rate': 9.747006060813577e-06, 'epoch': 2.0720395133116494}
2025-04-28 15:16:45,118 - INFO - Training progress: {'loss': 0.3791, 'grad_norm': 1.3176078796386719, 'learning_rate': 9.747006060813577e-06, 'epoch': 2.0720395133116494}
2025-04-28 15:16:45,118 - INFO - Training metrics: {'loss': 0.3791, 'grad_norm': 1.3176078796386719, 'learning_rate': 9.747006060813577e-06, 'epoch': 2.0720395133116494}
2025-04-28 15:19:57,225 - INFO - INFO: Training progress: {'loss': 0.397, 'grad_norm': 1.2753797769546509, 'learning_rate': 9.702755752089524e-06, 'epoch': 2.0816769063968197}
2025-04-28 15:19:57,225 - INFO - Training progress: {'loss': 0.397, 'grad_norm': 1.2753797769546509, 'learning_rate': 9.702755752089524e-06, 'epoch': 2.0816769063968197}
2025-04-28 15:19:57,225 - INFO - Training metrics: {'loss': 0.397, 'grad_norm': 1.2753797769546509, 'learning_rate': 9.702755752089524e-06, 'epoch': 2.0816769063968197}
2025-04-28 15:23:08,641 - INFO - INFO: Training progress: {'loss': 0.3921, 'grad_norm': 1.7036898136138916, 'learning_rate': 9.658421365283951e-06, 'epoch': 2.09131429948199}
2025-04-28 15:23:08,641 - INFO - Training progress: {'loss': 0.3921, 'grad_norm': 1.7036898136138916, 'learning_rate': 9.658421365283951e-06, 'epoch': 2.09131429948199}
2025-04-28 15:23:08,641 - INFO - Training metrics: {'loss': 0.3921, 'grad_norm': 1.7036898136138916, 'learning_rate': 9.658421365283951e-06, 'epoch': 2.09131429948199}
2025-04-28 15:26:19,906 - INFO - INFO: Training progress: {'loss': 0.3616, 'grad_norm': 1.000430941581726, 'learning_rate': 9.614004592618156e-06, 'epoch': 2.1009516925671607}
2025-04-28 15:26:19,906 - INFO - Training progress: {'loss': 0.3616, 'grad_norm': 1.000430941581726, 'learning_rate': 9.614004592618156e-06, 'epoch': 2.1009516925671607}
2025-04-28 15:26:19,906 - INFO - Training metrics: {'loss': 0.3616, 'grad_norm': 1.000430941581726, 'learning_rate': 9.614004592618156e-06, 'epoch': 2.1009516925671607}
2025-04-28 15:29:31,951 - INFO - INFO: Training progress: {'loss': 0.3756, 'grad_norm': 1.4380372762680054, 'learning_rate': 9.569507129458054e-06, 'epoch': 2.110589085652331}
2025-04-28 15:29:31,951 - INFO - Training progress: {'loss': 0.3756, 'grad_norm': 1.4380372762680054, 'learning_rate': 9.569507129458054e-06, 'epoch': 2.110589085652331}
2025-04-28 15:29:31,951 - INFO - Training metrics: {'loss': 0.3756, 'grad_norm': 1.4380372762680054, 'learning_rate': 9.569507129458054e-06, 'epoch': 2.110589085652331}
2025-04-28 15:32:44,009 - INFO - INFO: Training progress: {'loss': 0.3895, 'grad_norm': 1.3925182819366455, 'learning_rate': 9.524930674249486e-06, 'epoch': 2.1202264787375014}
2025-04-28 15:32:44,009 - INFO - Training progress: {'loss': 0.3895, 'grad_norm': 1.3925182819366455, 'learning_rate': 9.524930674249486e-06, 'epoch': 2.1202264787375014}
2025-04-28 15:32:44,009 - INFO - Training metrics: {'loss': 0.3895, 'grad_norm': 1.3925182819366455, 'learning_rate': 9.524930674249486e-06, 'epoch': 2.1202264787375014}
2025-04-28 15:32:45,013 - INFO - INFO: Saving checkpoint at step 2200
2025-04-28 15:32:45,013 - INFO - Saving checkpoint at step 2200
2025-04-28 15:32:45,013 - INFO - Saving checkpoint at step 2200
2025-04-28 15:35:56,258 - INFO - INFO: Training progress: {'loss': 0.3991, 'grad_norm': 1.2937397956848145, 'learning_rate': 9.480276928453372e-06, 'epoch': 2.129863871822672}
2025-04-28 15:35:56,258 - INFO - Training progress: {'loss': 0.3991, 'grad_norm': 1.2937397956848145, 'learning_rate': 9.480276928453372e-06, 'epoch': 2.129863871822672}
2025-04-28 15:35:56,258 - INFO - Training metrics: {'loss': 0.3991, 'grad_norm': 1.2937397956848145, 'learning_rate': 9.480276928453372e-06, 'epoch': 2.129863871822672}
2025-04-28 15:39:07,512 - INFO - INFO: Training progress: {'loss': 0.3963, 'grad_norm': 0.9777045845985413, 'learning_rate': 9.435547596480781e-06, 'epoch': 2.1395012649078424}
2025-04-28 15:39:07,512 - INFO - Training progress: {'loss': 0.3963, 'grad_norm': 0.9777045845985413, 'learning_rate': 9.435547596480781e-06, 'epoch': 2.1395012649078424}
2025-04-28 15:39:07,512 - INFO - Training metrics: {'loss': 0.3963, 'grad_norm': 0.9777045845985413, 'learning_rate': 9.435547596480781e-06, 'epoch': 2.1395012649078424}
2025-04-28 15:42:19,402 - INFO - INFO: Training progress: {'loss': 0.4089, 'grad_norm': 1.2983026504516602, 'learning_rate': 9.39074438562786e-06, 'epoch': 2.1491386579930127}
2025-04-28 15:42:19,402 - INFO - Training progress: {'loss': 0.4089, 'grad_norm': 1.2983026504516602, 'learning_rate': 9.39074438562786e-06, 'epoch': 2.1491386579930127}
2025-04-28 15:42:19,402 - INFO - Training metrics: {'loss': 0.4089, 'grad_norm': 1.2983026504516602, 'learning_rate': 9.39074438562786e-06, 'epoch': 2.1491386579930127}
2025-04-28 15:45:31,238 - INFO - INFO: Training progress: {'loss': 0.3967, 'grad_norm': 1.5242210626602173, 'learning_rate': 9.345869006010685e-06, 'epoch': 2.1587760510781835}
2025-04-28 15:45:31,238 - INFO - Training progress: {'loss': 0.3967, 'grad_norm': 1.5242210626602173, 'learning_rate': 9.345869006010685e-06, 'epoch': 2.1587760510781835}
2025-04-28 15:45:31,238 - INFO - Training metrics: {'loss': 0.3967, 'grad_norm': 1.5242210626602173, 'learning_rate': 9.345869006010685e-06, 'epoch': 2.1587760510781835}
2025-04-28 15:48:43,267 - INFO - INFO: Training progress: {'loss': 0.3737, 'grad_norm': 1.4490841627120972, 'learning_rate': 9.300923170499972e-06, 'epoch': 2.168413444163354}
2025-04-28 15:48:43,267 - INFO - Training progress: {'loss': 0.3737, 'grad_norm': 1.4490841627120972, 'learning_rate': 9.300923170499972e-06, 'epoch': 2.168413444163354}
2025-04-28 15:48:43,267 - INFO - Training metrics: {'loss': 0.3737, 'grad_norm': 1.4490841627120972, 'learning_rate': 9.300923170499972e-06, 'epoch': 2.168413444163354}
2025-04-28 15:51:58,104 - INFO - INFO: Training progress: {'loss': 0.3967, 'grad_norm': 1.4210548400878906, 'learning_rate': 9.255908594655705e-06, 'epoch': 2.178050837248524}
2025-04-28 15:51:58,104 - INFO - Training progress: {'loss': 0.3967, 'grad_norm': 1.4210548400878906, 'learning_rate': 9.255908594655705e-06, 'epoch': 2.178050837248524}
2025-04-28 15:51:58,104 - INFO - Training metrics: {'loss': 0.3967, 'grad_norm': 1.4210548400878906, 'learning_rate': 9.255908594655705e-06, 'epoch': 2.178050837248524}
2025-04-28 15:55:26,556 - INFO - INFO: Training progress: {'loss': 0.3428, 'grad_norm': 1.6794819831848145, 'learning_rate': 9.21082699666165e-06, 'epoch': 2.187688230333695}
2025-04-28 15:55:26,558 - INFO - Training progress: {'loss': 0.3428, 'grad_norm': 1.6794819831848145, 'learning_rate': 9.21082699666165e-06, 'epoch': 2.187688230333695}
2025-04-28 15:55:26,558 - INFO - Training metrics: {'loss': 0.3428, 'grad_norm': 1.6794819831848145, 'learning_rate': 9.21082699666165e-06, 'epoch': 2.187688230333695}
2025-04-28 15:58:54,151 - INFO - INFO: Training progress: {'loss': 0.3972, 'grad_norm': 1.4553663730621338, 'learning_rate': 9.16568009725978e-06, 'epoch': 2.197325623418865}
2025-04-28 15:58:54,151 - INFO - Training progress: {'loss': 0.3972, 'grad_norm': 1.4553663730621338, 'learning_rate': 9.16568009725978e-06, 'epoch': 2.197325623418865}
2025-04-28 15:58:54,151 - INFO - Training metrics: {'loss': 0.3972, 'grad_norm': 1.4553663730621338, 'learning_rate': 9.16568009725978e-06, 'epoch': 2.197325623418865}
2025-04-28 16:02:21,916 - INFO - INFO: Training progress: {'loss': 0.3944, 'grad_norm': 1.5170314311981201, 'learning_rate': 9.120469619684585e-06, 'epoch': 2.2069630165040355}
2025-04-28 16:02:21,916 - INFO - Training progress: {'loss': 0.3944, 'grad_norm': 1.5170314311981201, 'learning_rate': 9.120469619684585e-06, 'epoch': 2.2069630165040355}
2025-04-28 16:02:21,916 - INFO - Training metrics: {'loss': 0.3944, 'grad_norm': 1.5170314311981201, 'learning_rate': 9.120469619684585e-06, 'epoch': 2.2069630165040355}
2025-04-28 16:05:49,266 - INFO - INFO: Training progress: {'loss': 0.3818, 'grad_norm': 1.5312427282333374, 'learning_rate': 9.075197289597303e-06, 'epoch': 2.2166004095892062}
2025-04-28 16:05:49,266 - INFO - Training progress: {'loss': 0.3818, 'grad_norm': 1.5312427282333374, 'learning_rate': 9.075197289597303e-06, 'epoch': 2.2166004095892062}
2025-04-28 16:05:49,266 - INFO - Training metrics: {'loss': 0.3818, 'grad_norm': 1.5312427282333374, 'learning_rate': 9.075197289597303e-06, 'epoch': 2.2166004095892062}
2025-04-28 16:09:16,642 - INFO - INFO: Training progress: {'loss': 0.3343, 'grad_norm': 1.4611351490020752, 'learning_rate': 9.02986483502005e-06, 'epoch': 2.2262378026743765}
2025-04-28 16:09:16,642 - INFO - Training progress: {'loss': 0.3343, 'grad_norm': 1.4611351490020752, 'learning_rate': 9.02986483502005e-06, 'epoch': 2.2262378026743765}
2025-04-28 16:09:16,642 - INFO - Training metrics: {'loss': 0.3343, 'grad_norm': 1.4611351490020752, 'learning_rate': 9.02986483502005e-06, 'epoch': 2.2262378026743765}
2025-04-28 16:26:04,590 - INFO - INFO: Training progress: {'eval_loss': 0.38458508253097534, 'eval_runtime': 1007.9487, 'eval_samples_per_second': 1.543, 'eval_steps_per_second': 1.543, 'epoch': 2.2262378026743765}
2025-04-28 16:26:04,591 - INFO - Training progress: {'eval_loss': 0.38458508253097534, 'eval_runtime': 1007.9487, 'eval_samples_per_second': 1.543, 'eval_steps_per_second': 1.543, 'epoch': 2.2262378026743765}
2025-04-28 16:26:04,591 - INFO - Training metrics: {'eval_loss': 0.38458508253097534, 'eval_runtime': 1007.9487, 'eval_samples_per_second': 1.543, 'eval_steps_per_second': 1.543, 'epoch': 2.2262378026743765}
2025-04-28 16:29:20,105 - INFO - INFO: Training progress: {'loss': 0.4036, 'grad_norm': 1.491249918937683, 'learning_rate': 8.984473986269866e-06, 'epoch': 2.235875195759547}
2025-04-28 16:29:20,105 - INFO - Training progress: {'loss': 0.4036, 'grad_norm': 1.491249918937683, 'learning_rate': 8.984473986269866e-06, 'epoch': 2.235875195759547}
2025-04-28 16:29:20,105 - INFO - Training metrics: {'loss': 0.4036, 'grad_norm': 1.491249918937683, 'learning_rate': 8.984473986269866e-06, 'epoch': 2.235875195759547}
2025-04-28 16:32:38,812 - INFO - INFO: Training progress: {'loss': 0.3518, 'grad_norm': 1.5760401487350464, 'learning_rate': 8.939026475892668e-06, 'epoch': 2.2455125888447176}
2025-04-28 16:32:38,812 - INFO - Training progress: {'loss': 0.3518, 'grad_norm': 1.5760401487350464, 'learning_rate': 8.939026475892668e-06, 'epoch': 2.2455125888447176}
2025-04-28 16:32:38,812 - INFO - Training metrics: {'loss': 0.3518, 'grad_norm': 1.5760401487350464, 'learning_rate': 8.939026475892668e-06, 'epoch': 2.2455125888447176}
2025-04-28 16:35:54,147 - INFO - INFO: Training progress: {'loss': 0.362, 'grad_norm': 1.2841768264770508, 'learning_rate': 8.893524038597119e-06, 'epoch': 2.255149981929888}
2025-04-28 16:35:54,147 - INFO - Training progress: {'loss': 0.362, 'grad_norm': 1.2841768264770508, 'learning_rate': 8.893524038597119e-06, 'epoch': 2.255149981929888}
2025-04-28 16:35:54,147 - INFO - Training metrics: {'loss': 0.362, 'grad_norm': 1.2841768264770508, 'learning_rate': 8.893524038597119e-06, 'epoch': 2.255149981929888}
2025-04-28 16:39:08,018 - INFO - INFO: Training progress: {'loss': 0.3889, 'grad_norm': 1.2841733694076538, 'learning_rate': 8.84796841118841e-06, 'epoch': 2.2647873750150582}
2025-04-28 16:39:08,018 - INFO - Training progress: {'loss': 0.3889, 'grad_norm': 1.2841733694076538, 'learning_rate': 8.84796841118841e-06, 'epoch': 2.2647873750150582}
2025-04-28 16:39:08,018 - INFO - Training metrics: {'loss': 0.3889, 'grad_norm': 1.2841733694076538, 'learning_rate': 8.84796841118841e-06, 'epoch': 2.2647873750150582}
2025-04-28 16:42:19,839 - INFO - INFO: Training progress: {'loss': 0.3444, 'grad_norm': 1.337464451789856, 'learning_rate': 8.80236133250198e-06, 'epoch': 2.274424768100229}
2025-04-28 16:42:19,839 - INFO - Training progress: {'loss': 0.3444, 'grad_norm': 1.337464451789856, 'learning_rate': 8.80236133250198e-06, 'epoch': 2.274424768100229}
2025-04-28 16:42:19,839 - INFO - Training metrics: {'loss': 0.3444, 'grad_norm': 1.337464451789856, 'learning_rate': 8.80236133250198e-06, 'epoch': 2.274424768100229}
2025-04-28 16:45:30,561 - INFO - INFO: Training progress: {'loss': 0.3903, 'grad_norm': 1.567448377609253, 'learning_rate': 8.756704543337125e-06, 'epoch': 2.2840621611853993}
2025-04-28 16:45:30,561 - INFO - Training progress: {'loss': 0.3903, 'grad_norm': 1.567448377609253, 'learning_rate': 8.756704543337125e-06, 'epoch': 2.2840621611853993}
2025-04-28 16:45:30,561 - INFO - Training metrics: {'loss': 0.3903, 'grad_norm': 1.567448377609253, 'learning_rate': 8.756704543337125e-06, 'epoch': 2.2840621611853993}
2025-04-28 16:48:42,262 - INFO - INFO: Training progress: {'loss': 0.4373, 'grad_norm': 1.7776185274124146, 'learning_rate': 8.71099978639058e-06, 'epoch': 2.29369955427057}
2025-04-28 16:48:42,262 - INFO - Training progress: {'loss': 0.4373, 'grad_norm': 1.7776185274124146, 'learning_rate': 8.71099978639058e-06, 'epoch': 2.29369955427057}
2025-04-28 16:48:42,262 - INFO - Training metrics: {'loss': 0.4373, 'grad_norm': 1.7776185274124146, 'learning_rate': 8.71099978639058e-06, 'epoch': 2.29369955427057}
2025-04-28 16:51:56,044 - INFO - INFO: Training progress: {'loss': 0.4152, 'grad_norm': 1.740295648574829, 'learning_rate': 8.665248806189973e-06, 'epoch': 2.3033369473557403}
2025-04-28 16:51:56,044 - INFO - Training progress: {'loss': 0.4152, 'grad_norm': 1.740295648574829, 'learning_rate': 8.665248806189973e-06, 'epoch': 2.3033369473557403}
2025-04-28 16:51:56,044 - INFO - Training metrics: {'loss': 0.4152, 'grad_norm': 1.740295648574829, 'learning_rate': 8.665248806189973e-06, 'epoch': 2.3033369473557403}
2025-04-28 16:55:21,552 - INFO - INFO: Training progress: {'loss': 0.3728, 'grad_norm': 1.3024989366531372, 'learning_rate': 8.619453349027264e-06, 'epoch': 2.3129743404409107}
2025-04-28 16:55:21,552 - INFO - Training progress: {'loss': 0.3728, 'grad_norm': 1.3024989366531372, 'learning_rate': 8.619453349027264e-06, 'epoch': 2.3129743404409107}
2025-04-28 16:55:21,553 - INFO - Training metrics: {'loss': 0.3728, 'grad_norm': 1.3024989366531372, 'learning_rate': 8.619453349027264e-06, 'epoch': 2.3129743404409107}
2025-04-28 16:55:22,907 - INFO - INFO: Saving checkpoint at step 2400
2025-04-28 16:55:22,907 - INFO - Saving checkpoint at step 2400
2025-04-28 16:55:22,907 - INFO - Saving checkpoint at step 2400
2025-04-28 16:58:39,353 - INFO - INFO: Training progress: {'loss': 0.408, 'grad_norm': 1.5195761919021606, 'learning_rate': 8.57361516289206e-06, 'epoch': 2.322611733526081}
2025-04-28 16:58:39,360 - INFO - Training progress: {'loss': 0.408, 'grad_norm': 1.5195761919021606, 'learning_rate': 8.57361516289206e-06, 'epoch': 2.322611733526081}
2025-04-28 16:58:39,360 - INFO - Training metrics: {'loss': 0.408, 'grad_norm': 1.5195761919021606, 'learning_rate': 8.57361516289206e-06, 'epoch': 2.322611733526081}
2025-04-28 17:02:02,791 - INFO - INFO: Training progress: {'loss': 0.4353, 'grad_norm': 1.6732326745986938, 'learning_rate': 8.527735997404926e-06, 'epoch': 2.3322491266112517}
2025-04-28 17:02:02,792 - INFO - Training progress: {'loss': 0.4353, 'grad_norm': 1.6732326745986938, 'learning_rate': 8.527735997404926e-06, 'epoch': 2.3322491266112517}
2025-04-28 17:02:02,792 - INFO - Training metrics: {'loss': 0.4353, 'grad_norm': 1.6732326745986938, 'learning_rate': 8.527735997404926e-06, 'epoch': 2.3322491266112517}
2025-04-28 17:05:31,293 - INFO - INFO: Training progress: {'loss': 0.3659, 'grad_norm': 1.427865743637085, 'learning_rate': 8.481817603750582e-06, 'epoch': 2.341886519696422}
2025-04-28 17:05:31,294 - INFO - Training progress: {'loss': 0.3659, 'grad_norm': 1.427865743637085, 'learning_rate': 8.481817603750582e-06, 'epoch': 2.341886519696422}
2025-04-28 17:05:31,294 - INFO - Training metrics: {'loss': 0.3659, 'grad_norm': 1.427865743637085, 'learning_rate': 8.481817603750582e-06, 'epoch': 2.341886519696422}
2025-04-28 17:08:59,383 - INFO - INFO: Training progress: {'loss': 0.432, 'grad_norm': 1.2979830503463745, 'learning_rate': 8.435861734611065e-06, 'epoch': 2.351523912781593}
2025-04-28 17:08:59,383 - INFO - Training progress: {'loss': 0.432, 'grad_norm': 1.2979830503463745, 'learning_rate': 8.435861734611065e-06, 'epoch': 2.351523912781593}
2025-04-28 17:08:59,383 - INFO - Training metrics: {'loss': 0.432, 'grad_norm': 1.2979830503463745, 'learning_rate': 8.435861734611065e-06, 'epoch': 2.351523912781593}
2025-04-28 17:12:27,389 - INFO - INFO: Training progress: {'loss': 0.3573, 'grad_norm': 1.5365549325942993, 'learning_rate': 8.389870144098836e-06, 'epoch': 2.361161305866763}
2025-04-28 17:12:27,389 - INFO - Training progress: {'loss': 0.3573, 'grad_norm': 1.5365549325942993, 'learning_rate': 8.389870144098836e-06, 'epoch': 2.361161305866763}
2025-04-28 17:12:27,389 - INFO - Training metrics: {'loss': 0.3573, 'grad_norm': 1.5365549325942993, 'learning_rate': 8.389870144098836e-06, 'epoch': 2.361161305866763}
2025-04-28 17:15:55,854 - INFO - INFO: Training progress: {'loss': 0.3911, 'grad_norm': 1.3164472579956055, 'learning_rate': 8.343844587689826e-06, 'epoch': 2.3707986989519334}
2025-04-28 17:15:55,855 - INFO - Training progress: {'loss': 0.3911, 'grad_norm': 1.3164472579956055, 'learning_rate': 8.343844587689826e-06, 'epoch': 2.3707986989519334}
2025-04-28 17:15:55,855 - INFO - Training metrics: {'loss': 0.3911, 'grad_norm': 1.3164472579956055, 'learning_rate': 8.343844587689826e-06, 'epoch': 2.3707986989519334}
2025-04-28 17:19:23,804 - INFO - INFO: Training progress: {'loss': 0.3612, 'grad_norm': 1.6015496253967285, 'learning_rate': 8.297786822156418e-06, 'epoch': 2.380436092037104}
2025-04-28 17:19:23,804 - INFO - Training progress: {'loss': 0.3612, 'grad_norm': 1.6015496253967285, 'learning_rate': 8.297786822156418e-06, 'epoch': 2.380436092037104}
2025-04-28 17:19:23,804 - INFO - Training metrics: {'loss': 0.3612, 'grad_norm': 1.6015496253967285, 'learning_rate': 8.297786822156418e-06, 'epoch': 2.380436092037104}
2025-04-28 17:22:52,004 - INFO - INFO: Training progress: {'loss': 0.3842, 'grad_norm': 1.6552799940109253, 'learning_rate': 8.25169860550041e-06, 'epoch': 2.3900734851222745}
2025-04-28 17:22:52,005 - INFO - Training progress: {'loss': 0.3842, 'grad_norm': 1.6552799940109253, 'learning_rate': 8.25169860550041e-06, 'epoch': 2.3900734851222745}
2025-04-28 17:22:52,005 - INFO - Training metrics: {'loss': 0.3842, 'grad_norm': 1.6552799940109253, 'learning_rate': 8.25169860550041e-06, 'epoch': 2.3900734851222745}
2025-04-28 17:26:20,175 - INFO - INFO: Training progress: {'loss': 0.3623, 'grad_norm': 1.8586647510528564, 'learning_rate': 8.205581696885901e-06, 'epoch': 2.3997108782074448}
2025-04-28 17:26:20,177 - INFO - Training progress: {'loss': 0.3623, 'grad_norm': 1.8586647510528564, 'learning_rate': 8.205581696885901e-06, 'epoch': 2.3997108782074448}
2025-04-28 17:26:20,177 - INFO - Training metrics: {'loss': 0.3623, 'grad_norm': 1.8586647510528564, 'learning_rate': 8.205581696885901e-06, 'epoch': 2.3997108782074448}
2025-04-28 17:29:48,636 - INFO - INFO: Training progress: {'loss': 0.3897, 'grad_norm': 1.9576674699783325, 'learning_rate': 8.159437856572142e-06, 'epoch': 2.4093482712926155}
2025-04-28 17:29:48,637 - INFO - Training progress: {'loss': 0.3897, 'grad_norm': 1.9576674699783325, 'learning_rate': 8.159437856572142e-06, 'epoch': 2.4093482712926155}
2025-04-28 17:29:48,637 - INFO - Training metrics: {'loss': 0.3897, 'grad_norm': 1.9576674699783325, 'learning_rate': 8.159437856572142e-06, 'epoch': 2.4093482712926155}
2025-04-28 17:33:19,406 - INFO - INFO: Training progress: {'loss': 0.3902, 'grad_norm': 1.8222092390060425, 'learning_rate': 8.113268845846362e-06, 'epoch': 2.418985664377786}
2025-04-28 17:33:19,406 - INFO - Training progress: {'loss': 0.3902, 'grad_norm': 1.8222092390060425, 'learning_rate': 8.113268845846362e-06, 'epoch': 2.418985664377786}
2025-04-28 17:33:19,406 - INFO - Training metrics: {'loss': 0.3902, 'grad_norm': 1.8222092390060425, 'learning_rate': 8.113268845846362e-06, 'epoch': 2.418985664377786}
2025-04-28 17:36:41,560 - INFO - INFO: Training progress: {'loss': 0.391, 'grad_norm': 1.5464457273483276, 'learning_rate': 8.067076426956532e-06, 'epoch': 2.428623057462956}
2025-04-28 17:36:41,560 - INFO - Training progress: {'loss': 0.391, 'grad_norm': 1.5464457273483276, 'learning_rate': 8.067076426956532e-06, 'epoch': 2.428623057462956}
2025-04-28 17:36:41,560 - INFO - Training metrics: {'loss': 0.391, 'grad_norm': 1.5464457273483276, 'learning_rate': 8.067076426956532e-06, 'epoch': 2.428623057462956}
2025-04-28 17:52:15,020 - INFO - INFO: Training progress: {'eval_loss': 0.37626591324806213, 'eval_runtime': 933.4568, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 1.666, 'epoch': 2.428623057462956}
2025-04-28 17:52:15,020 - INFO - Training progress: {'eval_loss': 0.37626591324806213, 'eval_runtime': 933.4568, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 1.666, 'epoch': 2.428623057462956}
2025-04-28 17:52:15,020 - INFO - Training metrics: {'eval_loss': 0.37626591324806213, 'eval_runtime': 933.4568, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 1.666, 'epoch': 2.428623057462956}
2025-04-28 17:55:29,627 - INFO - INFO: Training progress: {'loss': 0.3917, 'grad_norm': 1.7172513008117676, 'learning_rate': 8.020862363044094e-06, 'epoch': 2.438260450548127}
2025-04-28 17:55:29,627 - INFO - Training progress: {'loss': 0.3917, 'grad_norm': 1.7172513008117676, 'learning_rate': 8.020862363044094e-06, 'epoch': 2.438260450548127}
2025-04-28 17:55:29,627 - INFO - Training metrics: {'loss': 0.3917, 'grad_norm': 1.7172513008117676, 'learning_rate': 8.020862363044094e-06, 'epoch': 2.438260450548127}
2025-04-28 17:58:45,915 - INFO - INFO: Training progress: {'loss': 0.3383, 'grad_norm': 1.831292748451233, 'learning_rate': 7.974628418076674e-06, 'epoch': 2.447897843633297}
2025-04-28 17:58:45,915 - INFO - Training progress: {'loss': 0.3383, 'grad_norm': 1.831292748451233, 'learning_rate': 7.974628418076674e-06, 'epoch': 2.447897843633297}
2025-04-28 17:58:45,915 - INFO - Training metrics: {'loss': 0.3383, 'grad_norm': 1.831292748451233, 'learning_rate': 7.974628418076674e-06, 'epoch': 2.447897843633297}
2025-04-28 18:02:01,141 - INFO - INFO: Training progress: {'loss': 0.3904, 'grad_norm': 1.7663754224777222, 'learning_rate': 7.928376356780748e-06, 'epoch': 2.4575352367184675}
2025-04-28 18:02:01,141 - INFO - Training progress: {'loss': 0.3904, 'grad_norm': 1.7663754224777222, 'learning_rate': 7.928376356780748e-06, 'epoch': 2.4575352367184675}
2025-04-28 18:02:01,141 - INFO - Training metrics: {'loss': 0.3904, 'grad_norm': 1.7663754224777222, 'learning_rate': 7.928376356780748e-06, 'epoch': 2.4575352367184675}
2025-04-28 18:05:18,366 - INFO - INFO: Training progress: {'loss': 0.3698, 'grad_norm': 1.391501545906067, 'learning_rate': 7.88210794457428e-06, 'epoch': 2.4671726298036383}
2025-04-28 18:05:18,366 - INFO - Training progress: {'loss': 0.3698, 'grad_norm': 1.391501545906067, 'learning_rate': 7.88210794457428e-06, 'epoch': 2.4671726298036383}
2025-04-28 18:05:18,366 - INFO - Training metrics: {'loss': 0.3698, 'grad_norm': 1.391501545906067, 'learning_rate': 7.88210794457428e-06, 'epoch': 2.4671726298036383}
2025-04-28 18:08:35,862 - INFO - INFO: Training progress: {'loss': 0.3492, 'grad_norm': 1.621911883354187, 'learning_rate': 7.835824947499342e-06, 'epoch': 2.4768100228888086}
2025-04-28 18:08:35,862 - INFO - Training progress: {'loss': 0.3492, 'grad_norm': 1.621911883354187, 'learning_rate': 7.835824947499342e-06, 'epoch': 2.4768100228888086}
2025-04-28 18:08:35,862 - INFO - Training metrics: {'loss': 0.3492, 'grad_norm': 1.621911883354187, 'learning_rate': 7.835824947499342e-06, 'epoch': 2.4768100228888086}
2025-04-28 18:11:51,747 - INFO - INFO: Training progress: {'loss': 0.4139, 'grad_norm': 1.5482254028320312, 'learning_rate': 7.789529132154705e-06, 'epoch': 2.486447415973979}
2025-04-28 18:11:51,747 - INFO - Training progress: {'loss': 0.4139, 'grad_norm': 1.5482254028320312, 'learning_rate': 7.789529132154705e-06, 'epoch': 2.486447415973979}
2025-04-28 18:11:51,747 - INFO - Training metrics: {'loss': 0.4139, 'grad_norm': 1.5482254028320312, 'learning_rate': 7.789529132154705e-06, 'epoch': 2.486447415973979}
2025-04-28 18:15:07,430 - INFO - INFO: Training progress: {'loss': 0.3836, 'grad_norm': 1.9446661472320557, 'learning_rate': 7.743222265628407e-06, 'epoch': 2.4960848090591496}
2025-04-28 18:15:07,430 - INFO - Training progress: {'loss': 0.3836, 'grad_norm': 1.9446661472320557, 'learning_rate': 7.743222265628407e-06, 'epoch': 2.4960848090591496}
2025-04-28 18:15:07,430 - INFO - Training metrics: {'loss': 0.3836, 'grad_norm': 1.9446661472320557, 'learning_rate': 7.743222265628407e-06, 'epoch': 2.4960848090591496}
2025-04-28 18:18:24,190 - INFO - INFO: Training progress: {'loss': 0.3756, 'grad_norm': 1.6624606847763062, 'learning_rate': 7.6969061154303e-06, 'epoch': 2.50572220214432}
2025-04-28 18:18:24,190 - INFO - Training progress: {'loss': 0.3756, 'grad_norm': 1.6624606847763062, 'learning_rate': 7.6969061154303e-06, 'epoch': 2.50572220214432}
2025-04-28 18:18:24,190 - INFO - Training metrics: {'loss': 0.3756, 'grad_norm': 1.6624606847763062, 'learning_rate': 7.6969061154303e-06, 'epoch': 2.50572220214432}
2025-04-28 18:18:25,218 - INFO - INFO: Saving checkpoint at step 2600
2025-04-28 18:18:25,218 - INFO - Saving checkpoint at step 2600
2025-04-28 18:18:25,218 - INFO - Saving checkpoint at step 2600
2025-04-28 18:21:42,159 - INFO - INFO: Training progress: {'loss': 0.3682, 'grad_norm': 1.8195877075195312, 'learning_rate': 7.65058244942459e-06, 'epoch': 2.5153595952294903}
2025-04-28 18:21:42,159 - INFO - Training progress: {'loss': 0.3682, 'grad_norm': 1.8195877075195312, 'learning_rate': 7.65058244942459e-06, 'epoch': 2.5153595952294903}
2025-04-28 18:21:42,159 - INFO - Training metrics: {'loss': 0.3682, 'grad_norm': 1.8195877075195312, 'learning_rate': 7.65058244942459e-06, 'epoch': 2.5153595952294903}
2025-04-28 18:24:58,468 - INFO - INFO: Training progress: {'loss': 0.3873, 'grad_norm': 1.3693127632141113, 'learning_rate': 7.604253035762364e-06, 'epoch': 2.524996988314661}
2025-04-28 18:24:58,468 - INFO - Training progress: {'loss': 0.3873, 'grad_norm': 1.3693127632141113, 'learning_rate': 7.604253035762364e-06, 'epoch': 2.524996988314661}
2025-04-28 18:24:58,468 - INFO - Training metrics: {'loss': 0.3873, 'grad_norm': 1.3693127632141113, 'learning_rate': 7.604253035762364e-06, 'epoch': 2.524996988314661}
2025-04-28 18:28:14,971 - INFO - INFO: Training progress: {'loss': 0.3861, 'grad_norm': 1.3162235021591187, 'learning_rate': 7.557919642814087e-06, 'epoch': 2.5346343813998313}
2025-04-28 18:28:14,971 - INFO - Training progress: {'loss': 0.3861, 'grad_norm': 1.3162235021591187, 'learning_rate': 7.557919642814087e-06, 'epoch': 2.5346343813998313}
2025-04-28 18:28:14,971 - INFO - Training metrics: {'loss': 0.3861, 'grad_norm': 1.3162235021591187, 'learning_rate': 7.557919642814087e-06, 'epoch': 2.5346343813998313}
2025-04-28 18:31:30,522 - INFO - INFO: Training progress: {'loss': 0.3377, 'grad_norm': 1.6131192445755005, 'learning_rate': 7.511584039102109e-06, 'epoch': 2.5442717744850016}
2025-04-28 18:31:30,522 - INFO - Training progress: {'loss': 0.3377, 'grad_norm': 1.6131192445755005, 'learning_rate': 7.511584039102109e-06, 'epoch': 2.5442717744850016}
2025-04-28 18:31:30,522 - INFO - Training metrics: {'loss': 0.3377, 'grad_norm': 1.6131192445755005, 'learning_rate': 7.511584039102109e-06, 'epoch': 2.5442717744850016}
2025-04-28 18:34:46,375 - INFO - INFO: Training progress: {'loss': 0.392, 'grad_norm': 1.6422626972198486, 'learning_rate': 7.465247993233176e-06, 'epoch': 2.5539091675701724}
2025-04-28 18:34:46,375 - INFO - Training progress: {'loss': 0.392, 'grad_norm': 1.6422626972198486, 'learning_rate': 7.465247993233176e-06, 'epoch': 2.5539091675701724}
2025-04-28 18:34:46,375 - INFO - Training metrics: {'loss': 0.392, 'grad_norm': 1.6422626972198486, 'learning_rate': 7.465247993233176e-06, 'epoch': 2.5539091675701724}
2025-04-28 18:37:57,747 - INFO - INFO: Training progress: {'loss': 0.3888, 'grad_norm': 1.5951379537582397, 'learning_rate': 7.418913273830898e-06, 'epoch': 2.5635465606553427}
2025-04-28 18:37:57,747 - INFO - Training progress: {'loss': 0.3888, 'grad_norm': 1.5951379537582397, 'learning_rate': 7.418913273830898e-06, 'epoch': 2.5635465606553427}
2025-04-28 18:37:57,747 - INFO - Training metrics: {'loss': 0.3888, 'grad_norm': 1.5951379537582397, 'learning_rate': 7.418913273830898e-06, 'epoch': 2.5635465606553427}
2025-04-28 18:41:10,735 - INFO - INFO: Training progress: {'loss': 0.4102, 'grad_norm': 1.5622713565826416, 'learning_rate': 7.372581649468262e-06, 'epoch': 2.5731839537405135}
2025-04-28 18:41:10,735 - INFO - Training progress: {'loss': 0.4102, 'grad_norm': 1.5622713565826416, 'learning_rate': 7.372581649468262e-06, 'epoch': 2.5731839537405135}
2025-04-28 18:41:10,735 - INFO - Training metrics: {'loss': 0.4102, 'grad_norm': 1.5622713565826416, 'learning_rate': 7.372581649468262e-06, 'epoch': 2.5731839537405135}
2025-04-28 18:44:28,338 - INFO - INFO: Training progress: {'loss': 0.3491, 'grad_norm': 1.1820484399795532, 'learning_rate': 7.326254888600114e-06, 'epoch': 2.5828213468256838}
2025-04-28 18:44:28,339 - INFO - Training progress: {'loss': 0.3491, 'grad_norm': 1.1820484399795532, 'learning_rate': 7.326254888600114e-06, 'epoch': 2.5828213468256838}
2025-04-28 18:44:28,339 - INFO - Training metrics: {'loss': 0.3491, 'grad_norm': 1.1820484399795532, 'learning_rate': 7.326254888600114e-06, 'epoch': 2.5828213468256838}
2025-04-28 18:47:40,787 - INFO - INFO: Training progress: {'loss': 0.4417, 'grad_norm': 2.084650754928589, 'learning_rate': 7.279934759495672e-06, 'epoch': 2.592458739910854}
2025-04-28 18:47:40,787 - INFO - Training progress: {'loss': 0.4417, 'grad_norm': 2.084650754928589, 'learning_rate': 7.279934759495672e-06, 'epoch': 2.592458739910854}
2025-04-28 18:47:40,787 - INFO - Training metrics: {'loss': 0.4417, 'grad_norm': 2.084650754928589, 'learning_rate': 7.279934759495672e-06, 'epoch': 2.592458739910854}
2025-04-28 18:50:53,818 - INFO - INFO: Training progress: {'loss': 0.3867, 'grad_norm': 1.8017500638961792, 'learning_rate': 7.233623030171009e-06, 'epoch': 2.6020961329960244}
2025-04-28 18:50:53,818 - INFO - Training progress: {'loss': 0.3867, 'grad_norm': 1.8017500638961792, 'learning_rate': 7.233623030171009e-06, 'epoch': 2.6020961329960244}
2025-04-28 18:50:53,818 - INFO - Training metrics: {'loss': 0.3867, 'grad_norm': 1.8017500638961792, 'learning_rate': 7.233623030171009e-06, 'epoch': 2.6020961329960244}
2025-04-28 18:54:11,857 - INFO - INFO: Training progress: {'loss': 0.3892, 'grad_norm': 1.334867238998413, 'learning_rate': 7.18732146832159e-06, 'epoch': 2.611733526081195}
2025-04-28 18:54:11,857 - INFO - Training progress: {'loss': 0.3892, 'grad_norm': 1.334867238998413, 'learning_rate': 7.18732146832159e-06, 'epoch': 2.611733526081195}
2025-04-28 18:54:11,857 - INFO - Training metrics: {'loss': 0.3892, 'grad_norm': 1.334867238998413, 'learning_rate': 7.18732146832159e-06, 'epoch': 2.611733526081195}
2025-04-28 18:57:35,027 - INFO - INFO: Training progress: {'loss': 0.3748, 'grad_norm': 1.5057039260864258, 'learning_rate': 7.1410318412548e-06, 'epoch': 2.6213709191663654}
2025-04-28 18:57:35,027 - INFO - Training progress: {'loss': 0.3748, 'grad_norm': 1.5057039260864258, 'learning_rate': 7.1410318412548e-06, 'epoch': 2.6213709191663654}
2025-04-28 18:57:35,027 - INFO - Training metrics: {'loss': 0.3748, 'grad_norm': 1.5057039260864258, 'learning_rate': 7.1410318412548e-06, 'epoch': 2.6213709191663654}
2025-04-28 19:00:51,233 - INFO - INFO: Training progress: {'loss': 0.3832, 'grad_norm': 1.145434856414795, 'learning_rate': 7.094755915822462e-06, 'epoch': 2.631008312251536}
2025-04-28 19:00:51,234 - INFO - Training progress: {'loss': 0.3832, 'grad_norm': 1.145434856414795, 'learning_rate': 7.094755915822462e-06, 'epoch': 2.631008312251536}
2025-04-28 19:00:51,234 - INFO - Training metrics: {'loss': 0.3832, 'grad_norm': 1.145434856414795, 'learning_rate': 7.094755915822462e-06, 'epoch': 2.631008312251536}
2025-04-28 19:16:13,462 - INFO - INFO: Training progress: {'eval_loss': 0.36883941292762756, 'eval_runtime': 922.2266, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 1.686, 'epoch': 2.631008312251536}
2025-04-28 19:16:13,463 - INFO - Training progress: {'eval_loss': 0.36883941292762756, 'eval_runtime': 922.2266, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 1.686, 'epoch': 2.631008312251536}
2025-04-28 19:16:13,463 - INFO - Training metrics: {'eval_loss': 0.36883941292762756, 'eval_runtime': 922.2266, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 1.686, 'epoch': 2.631008312251536}
2025-04-28 19:19:26,239 - INFO - INFO: Training progress: {'loss': 0.4003, 'grad_norm': 2.161520481109619, 'learning_rate': 7.048495458353433e-06, 'epoch': 2.6406457053367065}
2025-04-28 19:19:26,239 - INFO - Training progress: {'loss': 0.4003, 'grad_norm': 2.161520481109619, 'learning_rate': 7.048495458353433e-06, 'epoch': 2.6406457053367065}
2025-04-28 19:19:26,240 - INFO - Training metrics: {'loss': 0.4003, 'grad_norm': 2.161520481109619, 'learning_rate': 7.048495458353433e-06, 'epoch': 2.6406457053367065}
2025-04-28 19:22:38,927 - INFO - INFO: Training progress: {'loss': 0.3808, 'grad_norm': 1.8756444454193115, 'learning_rate': 7.002252234586153e-06, 'epoch': 2.650283098421877}
2025-04-28 19:22:38,928 - INFO - Training progress: {'loss': 0.3808, 'grad_norm': 1.8756444454193115, 'learning_rate': 7.002252234586153e-06, 'epoch': 2.650283098421877}
2025-04-28 19:22:38,928 - INFO - Training metrics: {'loss': 0.3808, 'grad_norm': 1.8756444454193115, 'learning_rate': 7.002252234586153e-06, 'epoch': 2.650283098421877}
2025-04-28 19:25:51,745 - INFO - INFO: Training progress: {'loss': 0.3876, 'grad_norm': 1.7312289476394653, 'learning_rate': 6.956028009601262e-06, 'epoch': 2.659920491507047}
2025-04-28 19:25:51,745 - INFO - Training progress: {'loss': 0.3876, 'grad_norm': 1.7312289476394653, 'learning_rate': 6.956028009601262e-06, 'epoch': 2.659920491507047}
2025-04-28 19:25:51,745 - INFO - Training metrics: {'loss': 0.3876, 'grad_norm': 1.7312289476394653, 'learning_rate': 6.956028009601262e-06, 'epoch': 2.659920491507047}
2025-04-28 19:29:05,635 - INFO - INFO: Training progress: {'loss': 0.3617, 'grad_norm': 1.5798085927963257, 'learning_rate': 6.909824547754235e-06, 'epoch': 2.669557884592218}
2025-04-28 19:29:05,636 - INFO - Training progress: {'loss': 0.3617, 'grad_norm': 1.5798085927963257, 'learning_rate': 6.909824547754235e-06, 'epoch': 2.669557884592218}
2025-04-28 19:29:05,636 - INFO - Training metrics: {'loss': 0.3617, 'grad_norm': 1.5798085927963257, 'learning_rate': 6.909824547754235e-06, 'epoch': 2.669557884592218}
2025-04-28 19:32:18,230 - INFO - INFO: Training progress: {'loss': 0.4096, 'grad_norm': 1.4897435903549194, 'learning_rate': 6.863643612608015e-06, 'epoch': 2.679195277677388}
2025-04-28 19:32:18,231 - INFO - Training progress: {'loss': 0.4096, 'grad_norm': 1.4897435903549194, 'learning_rate': 6.863643612608015e-06, 'epoch': 2.679195277677388}
2025-04-28 19:32:18,231 - INFO - Training metrics: {'loss': 0.4096, 'grad_norm': 1.4897435903549194, 'learning_rate': 6.863643612608015e-06, 'epoch': 2.679195277677388}
2025-04-28 19:35:30,009 - INFO - INFO: Training progress: {'loss': 0.3539, 'grad_norm': 1.5500324964523315, 'learning_rate': 6.817486966865722e-06, 'epoch': 2.688832670762559}
2025-04-28 19:35:30,009 - INFO - Training progress: {'loss': 0.3539, 'grad_norm': 1.5500324964523315, 'learning_rate': 6.817486966865722e-06, 'epoch': 2.688832670762559}
2025-04-28 19:35:30,009 - INFO - Training metrics: {'loss': 0.3539, 'grad_norm': 1.5500324964523315, 'learning_rate': 6.817486966865722e-06, 'epoch': 2.688832670762559}
2025-04-28 19:38:42,788 - INFO - INFO: Training progress: {'loss': 0.3683, 'grad_norm': 1.465395212173462, 'learning_rate': 6.771356372303356e-06, 'epoch': 2.6984700638477293}
2025-04-28 19:38:42,788 - INFO - Training progress: {'loss': 0.3683, 'grad_norm': 1.465395212173462, 'learning_rate': 6.771356372303356e-06, 'epoch': 2.6984700638477293}
2025-04-28 19:38:42,788 - INFO - Training metrics: {'loss': 0.3683, 'grad_norm': 1.465395212173462, 'learning_rate': 6.771356372303356e-06, 'epoch': 2.6984700638477293}
2025-04-28 19:38:43,779 - INFO - INFO: Saving checkpoint at step 2800
2025-04-28 19:38:43,780 - INFO - Saving checkpoint at step 2800
2025-04-28 19:38:43,780 - INFO - Saving checkpoint at step 2800
2025-04-28 19:41:56,210 - INFO - INFO: Training progress: {'loss': 0.3222, 'grad_norm': 1.5404071807861328, 'learning_rate': 6.725253589702563e-06, 'epoch': 2.7081074569328996}
2025-04-28 19:41:56,210 - INFO - Training progress: {'loss': 0.3222, 'grad_norm': 1.5404071807861328, 'learning_rate': 6.725253589702563e-06, 'epoch': 2.7081074569328996}
2025-04-28 19:41:56,210 - INFO - Training metrics: {'loss': 0.3222, 'grad_norm': 1.5404071807861328, 'learning_rate': 6.725253589702563e-06, 'epoch': 2.7081074569328996}
2025-04-28 19:45:08,010 - INFO - INFO: Training progress: {'loss': 0.4402, 'grad_norm': 2.1353328227996826, 'learning_rate': 6.679180378783415e-06, 'epoch': 2.71774485001807}
2025-04-28 19:45:08,011 - INFO - Training progress: {'loss': 0.4402, 'grad_norm': 2.1353328227996826, 'learning_rate': 6.679180378783415e-06, 'epoch': 2.71774485001807}
2025-04-28 19:45:08,011 - INFO - Training metrics: {'loss': 0.4402, 'grad_norm': 2.1353328227996826, 'learning_rate': 6.679180378783415e-06, 'epoch': 2.71774485001807}
2025-04-28 19:48:20,972 - INFO - INFO: Training progress: {'loss': 0.4007, 'grad_norm': 1.65959632396698, 'learning_rate': 6.6331384981372445e-06, 'epoch': 2.7273822431032406}
2025-04-28 19:48:20,972 - INFO - Training progress: {'loss': 0.4007, 'grad_norm': 1.65959632396698, 'learning_rate': 6.6331384981372445e-06, 'epoch': 2.7273822431032406}
2025-04-28 19:48:20,972 - INFO - Training metrics: {'loss': 0.4007, 'grad_norm': 1.65959632396698, 'learning_rate': 6.6331384981372445e-06, 'epoch': 2.7273822431032406}
2025-04-28 19:51:32,553 - INFO - INFO: Training progress: {'loss': 0.3586, 'grad_norm': 2.2534162998199463, 'learning_rate': 6.587129705159536e-06, 'epoch': 2.737019636188411}
2025-04-28 19:51:32,554 - INFO - Training progress: {'loss': 0.3586, 'grad_norm': 2.2534162998199463, 'learning_rate': 6.587129705159536e-06, 'epoch': 2.737019636188411}
2025-04-28 19:51:32,554 - INFO - Training metrics: {'loss': 0.3586, 'grad_norm': 2.2534162998199463, 'learning_rate': 6.587129705159536e-06, 'epoch': 2.737019636188411}
2025-04-28 19:54:46,834 - INFO - INFO: Training progress: {'loss': 0.3795, 'grad_norm': 1.6651653051376343, 'learning_rate': 6.541155755982817e-06, 'epoch': 2.7466570292735817}
2025-04-28 19:54:46,834 - INFO - Training progress: {'loss': 0.3795, 'grad_norm': 1.6651653051376343, 'learning_rate': 6.541155755982817e-06, 'epoch': 2.7466570292735817}
2025-04-28 19:54:46,834 - INFO - Training metrics: {'loss': 0.3795, 'grad_norm': 1.6651653051376343, 'learning_rate': 6.541155755982817e-06, 'epoch': 2.7466570292735817}
2025-04-28 19:58:04,245 - INFO - INFO: Training progress: {'loss': 0.367, 'grad_norm': 1.5980265140533447, 'learning_rate': 6.495218405409662e-06, 'epoch': 2.756294422358752}
2025-04-28 19:58:04,245 - INFO - Training progress: {'loss': 0.367, 'grad_norm': 1.5980265140533447, 'learning_rate': 6.495218405409662e-06, 'epoch': 2.756294422358752}
2025-04-28 19:58:04,245 - INFO - Training metrics: {'loss': 0.367, 'grad_norm': 1.5980265140533447, 'learning_rate': 6.495218405409662e-06, 'epoch': 2.756294422358752}
2025-04-28 20:01:19,942 - INFO - INFO: Training progress: {'loss': 0.4177, 'grad_norm': 1.1458477973937988, 'learning_rate': 6.449319406845686e-06, 'epoch': 2.7659318154439223}
2025-04-28 20:01:19,942 - INFO - Training progress: {'loss': 0.4177, 'grad_norm': 1.1458477973937988, 'learning_rate': 6.449319406845686e-06, 'epoch': 2.7659318154439223}
2025-04-28 20:01:19,942 - INFO - Training metrics: {'loss': 0.4177, 'grad_norm': 1.1458477973937988, 'learning_rate': 6.449319406845686e-06, 'epoch': 2.7659318154439223}
2025-04-28 20:04:33,537 - INFO - INFO: Training progress: {'loss': 0.4014, 'grad_norm': 1.6038774251937866, 'learning_rate': 6.4034605122326195e-06, 'epoch': 2.7755692085290926}
2025-04-28 20:04:33,537 - INFO - Training progress: {'loss': 0.4014, 'grad_norm': 1.6038774251937866, 'learning_rate': 6.4034605122326195e-06, 'epoch': 2.7755692085290926}
2025-04-28 20:04:33,538 - INFO - Training metrics: {'loss': 0.4014, 'grad_norm': 1.6038774251937866, 'learning_rate': 6.4034605122326195e-06, 'epoch': 2.7755692085290926}
2025-04-28 20:07:46,343 - INFO - INFO: Training progress: {'loss': 0.3521, 'grad_norm': 1.6893351078033447, 'learning_rate': 6.357643471981463e-06, 'epoch': 2.7852066016142634}
2025-04-28 20:07:46,343 - INFO - Training progress: {'loss': 0.3521, 'grad_norm': 1.6893351078033447, 'learning_rate': 6.357643471981463e-06, 'epoch': 2.7852066016142634}
2025-04-28 20:07:46,343 - INFO - Training metrics: {'loss': 0.3521, 'grad_norm': 1.6893351078033447, 'learning_rate': 6.357643471981463e-06, 'epoch': 2.7852066016142634}
2025-04-28 20:11:07,782 - INFO - INFO: Training progress: {'loss': 0.4106, 'grad_norm': 1.7440049648284912, 'learning_rate': 6.3118700349056426e-06, 'epoch': 2.7948439946994337}
2025-04-28 20:11:07,782 - INFO - Training progress: {'loss': 0.4106, 'grad_norm': 1.7440049648284912, 'learning_rate': 6.3118700349056426e-06, 'epoch': 2.7948439946994337}
2025-04-28 20:11:07,782 - INFO - Training metrics: {'loss': 0.4106, 'grad_norm': 1.7440049648284912, 'learning_rate': 6.3118700349056426e-06, 'epoch': 2.7948439946994337}
2025-04-28 20:14:27,231 - INFO - INFO: Training progress: {'loss': 0.339, 'grad_norm': 1.598537564277649, 'learning_rate': 6.266141948154284e-06, 'epoch': 2.8044813877846044}
2025-04-28 20:14:27,231 - INFO - Training progress: {'loss': 0.339, 'grad_norm': 1.598537564277649, 'learning_rate': 6.266141948154284e-06, 'epoch': 2.8044813877846044}
2025-04-28 20:14:27,231 - INFO - Training metrics: {'loss': 0.339, 'grad_norm': 1.598537564277649, 'learning_rate': 6.266141948154284e-06, 'epoch': 2.8044813877846044}
2025-04-28 20:17:44,887 - INFO - INFO: Training progress: {'loss': 0.364, 'grad_norm': 1.7756798267364502, 'learning_rate': 6.2204609571455005e-06, 'epoch': 2.8141187808697747}
2025-04-28 20:17:44,888 - INFO - Training progress: {'loss': 0.364, 'grad_norm': 1.7756798267364502, 'learning_rate': 6.2204609571455005e-06, 'epoch': 2.8141187808697747}
2025-04-28 20:17:44,888 - INFO - Training metrics: {'loss': 0.364, 'grad_norm': 1.7756798267364502, 'learning_rate': 6.2204609571455005e-06, 'epoch': 2.8141187808697747}
2025-04-28 20:21:02,358 - INFO - INFO: Training progress: {'loss': 0.3733, 'grad_norm': 1.7105122804641724, 'learning_rate': 6.174828805499798e-06, 'epoch': 2.823756173954945}
2025-04-28 20:21:02,358 - INFO - Training progress: {'loss': 0.3733, 'grad_norm': 1.7105122804641724, 'learning_rate': 6.174828805499798e-06, 'epoch': 2.823756173954945}
2025-04-28 20:21:02,358 - INFO - Training metrics: {'loss': 0.3733, 'grad_norm': 1.7105122804641724, 'learning_rate': 6.174828805499798e-06, 'epoch': 2.823756173954945}
2025-04-28 20:24:19,772 - INFO - INFO: Training progress: {'loss': 0.3585, 'grad_norm': 1.797337293624878, 'learning_rate': 6.1292472349735e-06, 'epoch': 2.833393567040116}
2025-04-28 20:24:19,772 - INFO - Training progress: {'loss': 0.3585, 'grad_norm': 1.797337293624878, 'learning_rate': 6.1292472349735e-06, 'epoch': 2.833393567040116}
2025-04-28 20:24:19,772 - INFO - Training metrics: {'loss': 0.3585, 'grad_norm': 1.797337293624878, 'learning_rate': 6.1292472349735e-06, 'epoch': 2.833393567040116}
2025-04-28 20:39:42,657 - INFO - INFO: Training progress: {'eval_loss': 0.3622659146785736, 'eval_runtime': 922.883, 'eval_samples_per_second': 1.685, 'eval_steps_per_second': 1.685, 'epoch': 2.833393567040116}
2025-04-28 20:39:42,658 - INFO - Training progress: {'eval_loss': 0.3622659146785736, 'eval_runtime': 922.883, 'eval_samples_per_second': 1.685, 'eval_steps_per_second': 1.685, 'epoch': 2.833393567040116}
2025-04-28 20:39:42,658 - INFO - Training metrics: {'eval_loss': 0.3622659146785736, 'eval_runtime': 922.883, 'eval_samples_per_second': 1.685, 'eval_steps_per_second': 1.685, 'epoch': 2.833393567040116}
2025-04-28 20:42:59,771 - INFO - INFO: Training progress: {'loss': 0.4211, 'grad_norm': 1.6743158102035522, 'learning_rate': 6.083717985392276e-06, 'epoch': 2.843030960125286}
2025-04-28 20:42:59,771 - INFO - Training progress: {'loss': 0.4211, 'grad_norm': 1.6743158102035522, 'learning_rate': 6.083717985392276e-06, 'epoch': 2.843030960125286}
2025-04-28 20:42:59,771 - INFO - Training metrics: {'loss': 0.4211, 'grad_norm': 1.6743158102035522, 'learning_rate': 6.083717985392276e-06, 'epoch': 2.843030960125286}
2025-04-28 20:46:17,213 - INFO - INFO: Training progress: {'loss': 0.3667, 'grad_norm': 1.8627533912658691, 'learning_rate': 6.038242794584735e-06, 'epoch': 2.8526683532104564}
2025-04-28 20:46:17,213 - INFO - Training progress: {'loss': 0.3667, 'grad_norm': 1.8627533912658691, 'learning_rate': 6.038242794584735e-06, 'epoch': 2.8526683532104564}
2025-04-28 20:46:17,213 - INFO - Training metrics: {'loss': 0.3667, 'grad_norm': 1.8627533912658691, 'learning_rate': 6.038242794584735e-06, 'epoch': 2.8526683532104564}
2025-04-28 20:49:37,762 - INFO - INFO: Training progress: {'loss': 0.345, 'grad_norm': 1.4167299270629883, 'learning_rate': 5.992823398316081e-06, 'epoch': 2.862305746295627}
2025-04-28 20:49:37,762 - INFO - Training progress: {'loss': 0.345, 'grad_norm': 1.4167299270629883, 'learning_rate': 5.992823398316081e-06, 'epoch': 2.862305746295627}
2025-04-28 20:49:37,762 - INFO - Training metrics: {'loss': 0.345, 'grad_norm': 1.4167299270629883, 'learning_rate': 5.992823398316081e-06, 'epoch': 2.862305746295627}
2025-04-28 20:52:52,726 - INFO - INFO: Training progress: {'loss': 0.3675, 'grad_norm': 1.622780203819275, 'learning_rate': 5.947461530221881e-06, 'epoch': 2.8719431393807975}
2025-04-28 20:52:52,726 - INFO - Training progress: {'loss': 0.3675, 'grad_norm': 1.622780203819275, 'learning_rate': 5.947461530221881e-06, 'epoch': 2.8719431393807975}
2025-04-28 20:52:52,726 - INFO - Training metrics: {'loss': 0.3675, 'grad_norm': 1.622780203819275, 'learning_rate': 5.947461530221881e-06, 'epoch': 2.8719431393807975}
2025-04-28 20:56:07,911 - INFO - INFO: Training progress: {'loss': 0.41, 'grad_norm': 1.5781055688858032, 'learning_rate': 5.902158921741871e-06, 'epoch': 2.881580532465968}
2025-04-28 20:56:07,911 - INFO - Training progress: {'loss': 0.41, 'grad_norm': 1.5781055688858032, 'learning_rate': 5.902158921741871e-06, 'epoch': 2.881580532465968}
2025-04-28 20:56:07,911 - INFO - Training metrics: {'loss': 0.41, 'grad_norm': 1.5781055688858032, 'learning_rate': 5.902158921741871e-06, 'epoch': 2.881580532465968}
2025-04-28 20:59:21,966 - INFO - INFO: Training progress: {'loss': 0.3387, 'grad_norm': 1.27122163772583, 'learning_rate': 5.856917302053875e-06, 'epoch': 2.8912179255511385}
2025-04-28 20:59:21,966 - INFO - Training progress: {'loss': 0.3387, 'grad_norm': 1.27122163772583, 'learning_rate': 5.856917302053875e-06, 'epoch': 2.8912179255511385}
2025-04-28 20:59:21,966 - INFO - Training metrics: {'loss': 0.3387, 'grad_norm': 1.27122163772583, 'learning_rate': 5.856917302053875e-06, 'epoch': 2.8912179255511385}
2025-04-28 20:59:23,210 - INFO - INFO: Saving checkpoint at step 3000
2025-04-28 20:59:23,210 - INFO - Saving checkpoint at step 3000
2025-04-28 20:59:23,210 - INFO - Saving checkpoint at step 3000
2025-04-28 21:02:36,051 - INFO - INFO: Training progress: {'loss': 0.3465, 'grad_norm': 1.5429377555847168, 'learning_rate': 5.811738398007816e-06, 'epoch': 2.900855318636309}
2025-04-28 21:02:36,052 - INFO - Training progress: {'loss': 0.3465, 'grad_norm': 1.5429377555847168, 'learning_rate': 5.811738398007816e-06, 'epoch': 2.900855318636309}
2025-04-28 21:02:36,052 - INFO - Training metrics: {'loss': 0.3465, 'grad_norm': 1.5429377555847168, 'learning_rate': 5.811738398007816e-06, 'epoch': 2.900855318636309}
2025-04-28 21:05:47,450 - INFO - INFO: Training progress: {'loss': 0.3951, 'grad_norm': 1.537725806236267, 'learning_rate': 5.766623934059783e-06, 'epoch': 2.9104927117214796}
2025-04-28 21:05:47,450 - INFO - Training progress: {'loss': 0.3951, 'grad_norm': 1.537725806236267, 'learning_rate': 5.766623934059783e-06, 'epoch': 2.9104927117214796}
2025-04-28 21:05:47,450 - INFO - Training metrics: {'loss': 0.3951, 'grad_norm': 1.537725806236267, 'learning_rate': 5.766623934059783e-06, 'epoch': 2.9104927117214796}
2025-04-28 21:08:59,240 - INFO - INFO: Training progress: {'loss': 0.3577, 'grad_norm': 1.6407270431518555, 'learning_rate': 5.721575632206227e-06, 'epoch': 2.92013010480665}
2025-04-28 21:08:59,240 - INFO - Training progress: {'loss': 0.3577, 'grad_norm': 1.6407270431518555, 'learning_rate': 5.721575632206227e-06, 'epoch': 2.92013010480665}
2025-04-28 21:08:59,241 - INFO - Training metrics: {'loss': 0.3577, 'grad_norm': 1.6407270431518555, 'learning_rate': 5.721575632206227e-06, 'epoch': 2.92013010480665}
2025-04-28 21:12:13,135 - INFO - INFO: Training progress: {'loss': 0.3623, 'grad_norm': 1.9362009763717651, 'learning_rate': 5.6765952119182156e-06, 'epoch': 2.9297674978918202}
2025-04-28 21:12:13,135 - INFO - Training progress: {'loss': 0.3623, 'grad_norm': 1.9362009763717651, 'learning_rate': 5.6765952119182156e-06, 'epoch': 2.9297674978918202}
2025-04-28 21:12:13,135 - INFO - Training metrics: {'loss': 0.3623, 'grad_norm': 1.9362009763717651, 'learning_rate': 5.6765952119182156e-06, 'epoch': 2.9297674978918202}
2025-04-28 21:15:27,550 - INFO - INFO: Training progress: {'loss': 0.3727, 'grad_norm': 1.3597328662872314, 'learning_rate': 5.631684390075821e-06, 'epoch': 2.9394048909769905}
2025-04-28 21:15:27,551 - INFO - Training progress: {'loss': 0.3727, 'grad_norm': 1.3597328662872314, 'learning_rate': 5.631684390075821e-06, 'epoch': 2.9394048909769905}
2025-04-28 21:15:27,551 - INFO - Training metrics: {'loss': 0.3727, 'grad_norm': 1.3597328662872314, 'learning_rate': 5.631684390075821e-06, 'epoch': 2.9394048909769905}
2025-04-28 21:18:40,147 - INFO - INFO: Training progress: {'loss': 0.358, 'grad_norm': 1.9292327165603638, 'learning_rate': 5.586844880902568e-06, 'epoch': 2.9490422840621613}
2025-04-28 21:18:40,147 - INFO - Training progress: {'loss': 0.358, 'grad_norm': 1.9292327165603638, 'learning_rate': 5.586844880902568e-06, 'epoch': 2.9490422840621613}
2025-04-28 21:18:40,147 - INFO - Training metrics: {'loss': 0.358, 'grad_norm': 1.9292327165603638, 'learning_rate': 5.586844880902568e-06, 'epoch': 2.9490422840621613}
2025-04-28 21:21:53,661 - INFO - INFO: Training progress: {'loss': 0.3815, 'grad_norm': 1.7717760801315308, 'learning_rate': 5.542078395900018e-06, 'epoch': 2.9586796771473316}
2025-04-28 21:21:53,661 - INFO - Training progress: {'loss': 0.3815, 'grad_norm': 1.7717760801315308, 'learning_rate': 5.542078395900018e-06, 'epoch': 2.9586796771473316}
2025-04-28 21:21:53,662 - INFO - Training metrics: {'loss': 0.3815, 'grad_norm': 1.7717760801315308, 'learning_rate': 5.542078395900018e-06, 'epoch': 2.9586796771473316}
2025-04-28 21:25:06,965 - INFO - INFO: Training progress: {'loss': 0.3381, 'grad_norm': 1.4546427726745605, 'learning_rate': 5.497386643782438e-06, 'epoch': 2.9683170702325024}
2025-04-28 21:25:06,965 - INFO - Training progress: {'loss': 0.3381, 'grad_norm': 1.4546427726745605, 'learning_rate': 5.497386643782438e-06, 'epoch': 2.9683170702325024}
2025-04-28 21:25:06,965 - INFO - Training metrics: {'loss': 0.3381, 'grad_norm': 1.4546427726745605, 'learning_rate': 5.497386643782438e-06, 'epoch': 2.9683170702325024}
2025-04-28 21:28:20,125 - INFO - INFO: Training progress: {'loss': 0.36, 'grad_norm': 1.4083720445632935, 'learning_rate': 5.452771330411572e-06, 'epoch': 2.9779544633176727}
2025-04-28 21:28:20,126 - INFO - Training progress: {'loss': 0.36, 'grad_norm': 1.4083720445632935, 'learning_rate': 5.452771330411572e-06, 'epoch': 2.9779544633176727}
2025-04-28 21:28:20,126 - INFO - Training metrics: {'loss': 0.36, 'grad_norm': 1.4083720445632935, 'learning_rate': 5.452771330411572e-06, 'epoch': 2.9779544633176727}
2025-04-28 21:31:32,472 - INFO - INFO: Training progress: {'loss': 0.4025, 'grad_norm': 1.6041314601898193, 'learning_rate': 5.408234158731547e-06, 'epoch': 2.987591856402843}
2025-04-28 21:31:32,472 - INFO - Training progress: {'loss': 0.4025, 'grad_norm': 1.6041314601898193, 'learning_rate': 5.408234158731547e-06, 'epoch': 2.987591856402843}
2025-04-28 21:31:32,472 - INFO - Training metrics: {'loss': 0.4025, 'grad_norm': 1.6041314601898193, 'learning_rate': 5.408234158731547e-06, 'epoch': 2.987591856402843}
2025-04-28 21:34:45,148 - INFO - INFO: Training progress: {'loss': 0.3597, 'grad_norm': 1.3854589462280273, 'learning_rate': 5.363776828703847e-06, 'epoch': 2.9972292494880133}
2025-04-28 21:34:45,148 - INFO - Training progress: {'loss': 0.3597, 'grad_norm': 1.3854589462280273, 'learning_rate': 5.363776828703847e-06, 'epoch': 2.9972292494880133}
2025-04-28 21:34:45,148 - INFO - Training metrics: {'loss': 0.3597, 'grad_norm': 1.3854589462280273, 'learning_rate': 5.363776828703847e-06, 'epoch': 2.9972292494880133}
2025-04-28 21:35:42,002 - INFO - INFO: Starting epoch 2.9991567281050475/5
2025-04-28 21:35:42,002 - INFO - Starting epoch 2.9991567281050475/5
2025-04-28 21:35:42,002 - INFO - Starting epoch 2.9991567281050475/5
2025-04-28 21:38:09,130 - INFO - INFO: Training progress: {'loss': 0.3782, 'grad_norm': 1.8386534452438354, 'learning_rate': 5.319401037242444e-06, 'epoch': 3.006866642573184}
2025-04-28 21:38:09,130 - INFO - Training progress: {'loss': 0.3782, 'grad_norm': 1.8386534452438354, 'learning_rate': 5.319401037242444e-06, 'epoch': 3.006866642573184}
2025-04-28 21:38:09,130 - INFO - Training metrics: {'loss': 0.3782, 'grad_norm': 1.8386534452438354, 'learning_rate': 5.319401037242444e-06, 'epoch': 3.006866642573184}
2025-04-28 21:41:26,960 - INFO - INFO: Training progress: {'loss': 0.3443, 'grad_norm': 1.9427156448364258, 'learning_rate': 5.275108478149036e-06, 'epoch': 3.0165040356583543}
2025-04-28 21:41:26,960 - INFO - Training progress: {'loss': 0.3443, 'grad_norm': 1.9427156448364258, 'learning_rate': 5.275108478149036e-06, 'epoch': 3.0165040356583543}
2025-04-28 21:41:26,960 - INFO - Training metrics: {'loss': 0.3443, 'grad_norm': 1.9427156448364258, 'learning_rate': 5.275108478149036e-06, 'epoch': 3.0165040356583543}
2025-04-28 21:44:38,250 - INFO - INFO: Training progress: {'loss': 0.3296, 'grad_norm': 1.814099907875061, 'learning_rate': 5.2309008420483635e-06, 'epoch': 3.026141428743525}
2025-04-28 21:44:38,251 - INFO - Training progress: {'loss': 0.3296, 'grad_norm': 1.814099907875061, 'learning_rate': 5.2309008420483635e-06, 'epoch': 3.026141428743525}
2025-04-28 21:44:38,251 - INFO - Training metrics: {'loss': 0.3296, 'grad_norm': 1.814099907875061, 'learning_rate': 5.2309008420483635e-06, 'epoch': 3.026141428743525}
2025-04-28 21:47:50,298 - INFO - INFO: Training progress: {'loss': 0.3274, 'grad_norm': 1.629737138748169, 'learning_rate': 5.186779816323715e-06, 'epoch': 3.0357788218286954}
2025-04-28 21:47:50,298 - INFO - Training progress: {'loss': 0.3274, 'grad_norm': 1.629737138748169, 'learning_rate': 5.186779816323715e-06, 'epoch': 3.0357788218286954}
2025-04-28 21:47:50,298 - INFO - Training metrics: {'loss': 0.3274, 'grad_norm': 1.629737138748169, 'learning_rate': 5.186779816323715e-06, 'epoch': 3.0357788218286954}
2025-04-28 22:03:08,292 - INFO - INFO: Training progress: {'eval_loss': 0.35635918378829956, 'eval_runtime': 917.9915, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 1.694, 'epoch': 3.0357788218286954}
2025-04-28 22:03:08,292 - INFO - Training progress: {'eval_loss': 0.35635918378829956, 'eval_runtime': 917.9915, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 1.694, 'epoch': 3.0357788218286954}
2025-04-28 22:03:08,292 - INFO - Training metrics: {'eval_loss': 0.35635918378829956, 'eval_runtime': 917.9915, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 1.694, 'epoch': 3.0357788218286954}
2025-04-28 22:06:22,371 - INFO - INFO: Training progress: {'loss': 0.3932, 'grad_norm': 1.6884121894836426, 'learning_rate': 5.142747085052498e-06, 'epoch': 3.0454162149138657}
2025-04-28 22:06:22,372 - INFO - Training progress: {'loss': 0.3932, 'grad_norm': 1.6884121894836426, 'learning_rate': 5.142747085052498e-06, 'epoch': 3.0454162149138657}
2025-04-28 22:06:22,372 - INFO - Training metrics: {'loss': 0.3932, 'grad_norm': 1.6884121894836426, 'learning_rate': 5.142747085052498e-06, 'epoch': 3.0454162149138657}
2025-04-28 22:09:39,775 - INFO - INFO: Training progress: {'loss': 0.3638, 'grad_norm': 1.6494451761245728, 'learning_rate': 5.098804328941965e-06, 'epoch': 3.0550536079990365}
2025-04-28 22:09:39,775 - INFO - Training progress: {'loss': 0.3638, 'grad_norm': 1.6494451761245728, 'learning_rate': 5.098804328941965e-06, 'epoch': 3.0550536079990365}
2025-04-28 22:09:39,775 - INFO - Training metrics: {'loss': 0.3638, 'grad_norm': 1.6494451761245728, 'learning_rate': 5.098804328941965e-06, 'epoch': 3.0550536079990365}
2025-04-28 22:12:58,430 - INFO - INFO: Training progress: {'loss': 0.3353, 'grad_norm': 1.8601130247116089, 'learning_rate': 5.054953225265062e-06, 'epoch': 3.064691001084207}
2025-04-28 22:12:58,430 - INFO - Training progress: {'loss': 0.3353, 'grad_norm': 1.8601130247116089, 'learning_rate': 5.054953225265062e-06, 'epoch': 3.064691001084207}
2025-04-28 22:12:58,430 - INFO - Training metrics: {'loss': 0.3353, 'grad_norm': 1.8601130247116089, 'learning_rate': 5.054953225265062e-06, 'epoch': 3.064691001084207}
2025-04-28 22:16:18,586 - INFO - INFO: Training progress: {'loss': 0.3528, 'grad_norm': 1.8047007322311401, 'learning_rate': 5.011195447796406e-06, 'epoch': 3.074328394169377}
2025-04-28 22:16:18,586 - INFO - Training progress: {'loss': 0.3528, 'grad_norm': 1.8047007322311401, 'learning_rate': 5.011195447796406e-06, 'epoch': 3.074328394169377}
2025-04-28 22:16:18,586 - INFO - Training metrics: {'loss': 0.3528, 'grad_norm': 1.8047007322311401, 'learning_rate': 5.011195447796406e-06, 'epoch': 3.074328394169377}
2025-04-28 22:19:39,476 - INFO - INFO: Training progress: {'loss': 0.3474, 'grad_norm': 2.081810712814331, 'learning_rate': 4.967532666748405e-06, 'epoch': 3.083965787254548}
2025-04-28 22:19:39,476 - INFO - Training progress: {'loss': 0.3474, 'grad_norm': 2.081810712814331, 'learning_rate': 4.967532666748405e-06, 'epoch': 3.083965787254548}
2025-04-28 22:19:39,476 - INFO - Training metrics: {'loss': 0.3474, 'grad_norm': 2.081810712814331, 'learning_rate': 4.967532666748405e-06, 'epoch': 3.083965787254548}
2025-04-28 22:19:40,487 - INFO - INFO: Saving checkpoint at step 3200
2025-04-28 22:19:40,487 - INFO - Saving checkpoint at step 3200
2025-04-28 22:19:40,487 - INFO - Saving checkpoint at step 3200
2025-04-28 22:23:02,210 - INFO - INFO: Training progress: {'loss': 0.3782, 'grad_norm': 1.7509011030197144, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.093603180339718}
2025-04-28 22:23:02,211 - INFO - Training progress: {'loss': 0.3782, 'grad_norm': 1.7509011030197144, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.093603180339718}
2025-04-28 22:23:02,211 - INFO - Training metrics: {'loss': 0.3782, 'grad_norm': 1.7509011030197144, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.093603180339718}
2025-04-28 22:26:22,156 - INFO - INFO: Training progress: {'loss': 0.3727, 'grad_norm': 2.0904927253723145, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.1032405734248885}
2025-04-28 22:26:22,156 - INFO - Training progress: {'loss': 0.3727, 'grad_norm': 2.0904927253723145, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.1032405734248885}
2025-04-28 22:26:22,156 - INFO - Training metrics: {'loss': 0.3727, 'grad_norm': 2.0904927253723145, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.1032405734248885}
2025-04-28 22:29:37,899 - INFO - INFO: Training progress: {'loss': 0.3557, 'grad_norm': 1.5863687992095947, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.112877966510059}
2025-04-28 22:29:37,899 - INFO - Training progress: {'loss': 0.3557, 'grad_norm': 1.5863687992095947, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.112877966510059}
2025-04-28 22:29:37,899 - INFO - Training metrics: {'loss': 0.3557, 'grad_norm': 1.5863687992095947, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.112877966510059}
2025-04-28 22:32:50,548 - INFO - INFO: Training progress: {'loss': 0.36, 'grad_norm': 1.8940666913986206, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.1225153595952295}
2025-04-28 22:32:50,549 - INFO - Training progress: {'loss': 0.36, 'grad_norm': 1.8940666913986206, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.1225153595952295}
2025-04-28 22:32:50,549 - INFO - Training metrics: {'loss': 0.36, 'grad_norm': 1.8940666913986206, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.1225153595952295}
2025-04-28 22:36:01,721 - INFO - INFO: Training progress: {'loss': 0.3466, 'grad_norm': 1.8719751834869385, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1321527526804}
2025-04-28 22:36:01,721 - INFO - Training progress: {'loss': 0.3466, 'grad_norm': 1.8719751834869385, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1321527526804}
2025-04-28 22:36:01,721 - INFO - Training metrics: {'loss': 0.3466, 'grad_norm': 1.8719751834869385, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1321527526804}
2025-04-28 22:39:21,243 - INFO - INFO: Training progress: {'loss': 0.3427, 'grad_norm': 1.9825544357299805, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1417901457655706}
2025-04-28 22:39:21,243 - INFO - Training progress: {'loss': 0.3427, 'grad_norm': 1.9825544357299805, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1417901457655706}
2025-04-28 22:39:21,243 - INFO - Training metrics: {'loss': 0.3427, 'grad_norm': 1.9825544357299805, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1417901457655706}
2025-04-28 22:42:37,034 - INFO - INFO: Training progress: {'loss': 0.3466, 'grad_norm': 1.7473214864730835, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.151427538850741}
2025-04-28 22:42:37,034 - INFO - Training progress: {'loss': 0.3466, 'grad_norm': 1.7473214864730835, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.151427538850741}
2025-04-28 22:42:37,034 - INFO - Training metrics: {'loss': 0.3466, 'grad_norm': 1.7473214864730835, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.151427538850741}
2025-04-28 22:45:52,010 - INFO - INFO: Training progress: {'loss': 0.3481, 'grad_norm': 1.5467908382415771, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.161064931935911}
2025-04-28 22:45:52,010 - INFO - Training progress: {'loss': 0.3481, 'grad_norm': 1.5467908382415771, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.161064931935911}
2025-04-28 22:45:52,010 - INFO - Training metrics: {'loss': 0.3481, 'grad_norm': 1.5467908382415771, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.161064931935911}
2025-04-28 22:49:05,366 - INFO - INFO: Training progress: {'loss': 0.3389, 'grad_norm': 2.172705888748169, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.170702325021082}
2025-04-28 22:49:05,366 - INFO - Training progress: {'loss': 0.3389, 'grad_norm': 2.172705888748169, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.170702325021082}
2025-04-28 22:49:05,366 - INFO - Training metrics: {'loss': 0.3389, 'grad_norm': 2.172705888748169, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.170702325021082}
2025-04-28 22:52:18,352 - INFO - INFO: Training progress: {'loss': 0.382, 'grad_norm': 2.1216366291046143, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.1803397181062523}
2025-04-28 22:52:18,352 - INFO - Training progress: {'loss': 0.382, 'grad_norm': 2.1216366291046143, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.1803397181062523}
2025-04-28 22:52:18,352 - INFO - Training metrics: {'loss': 0.382, 'grad_norm': 2.1216366291046143, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.1803397181062523}
2025-04-28 22:55:32,204 - INFO - INFO: Training progress: {'loss': 0.3281, 'grad_norm': 1.9066791534423828, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.1899771111914226}
2025-04-28 22:55:32,204 - INFO - Training progress: {'loss': 0.3281, 'grad_norm': 1.9066791534423828, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.1899771111914226}
2025-04-28 22:55:32,204 - INFO - Training metrics: {'loss': 0.3281, 'grad_norm': 1.9066791534423828, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.1899771111914226}
2025-04-28 22:58:49,950 - INFO - INFO: Training progress: {'loss': 0.3514, 'grad_norm': 1.5920486450195312, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.1996145042765933}
2025-04-28 22:58:49,950 - INFO - Training progress: {'loss': 0.3514, 'grad_norm': 1.5920486450195312, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.1996145042765933}
2025-04-28 22:58:49,950 - INFO - Training metrics: {'loss': 0.3514, 'grad_norm': 1.5920486450195312, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.1996145042765933}
2025-04-28 23:02:03,342 - INFO - INFO: Training progress: {'loss': 0.3714, 'grad_norm': 2.1426947116851807, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2092518973617636}
2025-04-28 23:02:03,342 - INFO - Training progress: {'loss': 0.3714, 'grad_norm': 2.1426947116851807, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2092518973617636}
2025-04-28 23:02:03,342 - INFO - Training metrics: {'loss': 0.3714, 'grad_norm': 2.1426947116851807, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2092518973617636}
2025-04-28 23:05:17,459 - INFO - INFO: Training progress: {'loss': 0.3169, 'grad_norm': 1.917595624923706, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.218889290446934}
2025-04-28 23:05:17,459 - INFO - Training progress: {'loss': 0.3169, 'grad_norm': 1.917595624923706, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.218889290446934}
2025-04-28 23:05:17,459 - INFO - Training metrics: {'loss': 0.3169, 'grad_norm': 1.917595624923706, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.218889290446934}
2025-04-28 23:08:42,154 - INFO - INFO: Training progress: {'loss': 0.3475, 'grad_norm': 1.7236214876174927, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.2285266835321047}
2025-04-28 23:08:42,154 - INFO - Training progress: {'loss': 0.3475, 'grad_norm': 1.7236214876174927, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.2285266835321047}
2025-04-28 23:08:42,154 - INFO - Training metrics: {'loss': 0.3475, 'grad_norm': 1.7236214876174927, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.2285266835321047}
2025-04-28 23:12:07,816 - INFO - INFO: Training progress: {'loss': 0.4074, 'grad_norm': 2.02567720413208, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.238164076617275}
2025-04-28 23:12:07,816 - INFO - Training progress: {'loss': 0.4074, 'grad_norm': 2.02567720413208, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.238164076617275}
2025-04-28 23:12:07,816 - INFO - Training metrics: {'loss': 0.4074, 'grad_norm': 2.02567720413208, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.238164076617275}
2025-04-28 23:27:00,277 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 23:27:00,277 - INFO - 

==================================================
2025-04-28 23:27:00,277 - INFO - Resuming from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 23:27:00,277 - INFO - ==================================================

2025-04-28 23:27:00,277 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 23:27:00,278 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets', 'text_column': 'input', 'use_checkpoint': True, 'checkpoint_path': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000', 'max_samples': None, 'pre_eval': False, 'eval_split': 0, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3200, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 24, 'lora_alpha': 48, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 5, 'learning_rate': 1.5e-05, 'warmup_steps': 100, 'warmup_ratio': 0.08, 'logging_steps': 10, 'save_steps': 200, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 210, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'lr_scheduler_type': 'cosine_with_restarts', 'weight_decay': 0.01, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2}}
2025-04-28 23:27:00,278 - INFO - INFO: Loading datasets from individual JSONL files
2025-04-28 23:27:00,278 - INFO - Loading datasets from individual JSONL files
2025-04-28 23:27:00,547 - INFO - INFO: Loaded 8301 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\training_set.jsonl
2025-04-28 23:27:00,777 - INFO - INFO: Loaded 1555 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\validation_set.jsonl
2025-04-28 23:27:00,807 - INFO - INFO: Loaded separate validation set with 1555 examples
2025-04-28 23:27:00,807 - INFO - Loaded separate validation set with 1555 examples
2025-04-28 23:27:00,832 - INFO - INFO: Loaded 522 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\test_set.jsonl
2025-04-28 23:27:00,843 - INFO - INFO: Loaded separate test set with 522 examples
2025-04-28 23:27:00,844 - INFO - Loaded separate test set with 522 examples
2025-04-28 23:27:00,844 - INFO - INFO: Supervised format detected, combining input and output columns
2025-04-28 23:27:00,844 - INFO - Supervised format detected, combining input and output columns
2025-04-28 23:27:01,669 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-04-28 23:27:01,670 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-04-28 23:27:11,448 - INFO - INFO: Dataset prepared with 8301 examples
2025-04-28 23:27:13,515 - INFO - INFO: Dataset prepared with 1555 examples
2025-04-28 23:27:14,288 - INFO - INFO: Dataset prepared with 522 examples
2025-04-28 23:27:14,292 - INFO - INFO: CUDA cache cleared
2025-04-28 23:27:14,422 - INFO - INFO: Garbage collector freed 85 objects
2025-04-28 23:28:28,618 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3
2025-04-28 23:28:28,620 - INFO - INFO: Model has 8030261248 parameters, 0 are trainable (0.00%)
2025-04-28 23:28:29,371 - INFO - INFO: Model has 62914560 trainable parameters after PEFT configuration
2025-04-28 23:28:29,435 - INFO - Starting model training with 8301 training examples
2025-04-28 23:28:29,435 - INFO - Using 1555 examples for validation during training
2025-04-28 23:28:29,435 - INFO - Using 522 examples for pre/final evaluation
2025-04-28 23:28:29,503 - INFO - INFO: Starting training...
2025-04-28 23:28:29,503 - INFO - Starting training...
2025-04-28 23:28:29,503 - INFO - Starting training...
2025-04-28 23:28:29,504 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 23:28:29,504 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 23:28:29,504 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 23:28:29,504 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_4/checkpoint-1000
2025-04-28 23:28:29,505 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 23:28:29,505 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 23:28:29,505 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 23:28:29,506 - INFO - INFO: Registering specific numpy components
2025-04-28 23:28:29,506 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 23:28:29,506 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 23:28:29,507 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 23:28:29,507 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 23:28:29,507 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 23:28:29,508 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 23:28:29,508 - INFO - INFO: Registering specific numpy components
2025-04-28 23:28:29,508 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 23:28:29,509 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 23:28:29,509 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 23:28:31,057 - INFO - INFO: Starting epoch 0.9385265133740028/5
2025-04-28 23:28:31,057 - INFO - Starting epoch 0.9385265133740028/5
2025-04-28 23:28:31,057 - INFO - Starting epoch 0.9385265133740028/5
2025-04-28 23:29:53,701 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 23:29:53,701 - INFO - 

==================================================
2025-04-28 23:29:53,701 - INFO - Resuming from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200
2025-04-28 23:29:53,701 - INFO - ==================================================

2025-04-28 23:29:53,702 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\logging.txt
2025-04-28 23:29:53,702 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets', 'text_column': 'input', 'use_checkpoint': True, 'checkpoint_path': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200', 'max_samples': None, 'pre_eval': False, 'eval_split': 0, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3200, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 24, 'lora_alpha': 48, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 5, 'learning_rate': 1.5e-05, 'warmup_steps': 100, 'warmup_ratio': 0.08, 'logging_steps': 10, 'save_steps': 200, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 210, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'lr_scheduler_type': 'cosine_with_restarts', 'weight_decay': 0.01, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2}}
2025-04-28 23:29:53,702 - INFO - INFO: Loading datasets from individual JSONL files
2025-04-28 23:29:53,702 - INFO - Loading datasets from individual JSONL files
2025-04-28 23:29:53,872 - INFO - INFO: Loaded 8301 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\training_set.jsonl
2025-04-28 23:29:54,061 - INFO - INFO: Loaded 1555 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\validation_set.jsonl
2025-04-28 23:29:54,091 - INFO - INFO: Loaded separate validation set with 1555 examples
2025-04-28 23:29:54,092 - INFO - Loaded separate validation set with 1555 examples
2025-04-28 23:29:54,102 - INFO - INFO: Loaded 522 examples from N:/Thesis/data_prepare/datasets_ready/supervised/multiple_chapters_format2/combined_datasets\test_set.jsonl
2025-04-28 23:29:54,114 - INFO - INFO: Loaded separate test set with 522 examples
2025-04-28 23:29:54,114 - INFO - Loaded separate test set with 522 examples
2025-04-28 23:29:54,114 - INFO - INFO: Supervised format detected, combining input and output columns
2025-04-28 23:29:54,114 - INFO - Supervised format detected, combining input and output columns
2025-04-28 23:29:54,902 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-04-28 23:29:54,903 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-04-28 23:30:04,628 - INFO - INFO: Dataset prepared with 8301 examples
2025-04-28 23:30:06,557 - INFO - INFO: Dataset prepared with 1555 examples
2025-04-28 23:30:07,322 - INFO - INFO: Dataset prepared with 522 examples
2025-04-28 23:30:07,326 - INFO - INFO: CUDA cache cleared
2025-04-28 23:30:07,447 - INFO - INFO: Garbage collector freed 85 objects
2025-04-28 23:30:15,967 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_unsupervised_3
2025-04-28 23:30:15,969 - INFO - INFO: Model has 8030261248 parameters, 0 are trainable (0.00%)
2025-04-28 23:30:16,511 - INFO - INFO: Model has 62914560 trainable parameters after PEFT configuration
2025-04-28 23:30:16,559 - INFO - Starting model training with 8301 training examples
2025-04-28 23:30:16,559 - INFO - Using 1555 examples for validation during training
2025-04-28 23:30:16,559 - INFO - Using 522 examples for pre/final evaluation
2025-04-28 23:30:16,573 - INFO - INFO: Starting training...
2025-04-28 23:30:16,573 - INFO - Starting training...
2025-04-28 23:30:16,573 - INFO - Starting training...
2025-04-28 23:30:16,573 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200
2025-04-28 23:30:16,573 - INFO - INFO: Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200
2025-04-28 23:30:16,574 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200
2025-04-28 23:30:16,574 - INFO - Resuming training from checkpoint: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5/checkpoint-3200
2025-04-28 23:30:16,574 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 23:30:16,574 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 23:30:16,574 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 23:30:16,574 - INFO - INFO: Registering specific numpy components
2025-04-28 23:30:16,575 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 23:30:16,575 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 23:30:16,575 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 23:30:16,576 - INFO - INFO: Registering numpy component classes as safe globals
2025-04-28 23:30:16,576 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-04-28 23:30:16,576 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-04-28 23:30:16,577 - INFO - INFO: Registering specific numpy components
2025-04-28 23:30:16,577 - INFO - INFO: Registering numpy array creation patterns
2025-04-28 23:30:16,577 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-04-28 23:30:16,577 - INFO - INFO: Numpy components registered as safe globals
2025-04-28 23:30:17,396 - INFO - INFO: Starting epoch 3.083965787254548/5
2025-04-28 23:30:17,396 - INFO - Starting epoch 3.083965787254548/5
2025-04-28 23:30:17,396 - INFO - Starting epoch 3.083965787254548/5
2025-04-28 23:33:38,072 - INFO - INFO: Training progress: {'loss': 0.3831, 'grad_norm': 1.4659043550491333, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.0954101915431878}
2025-04-28 23:33:38,072 - INFO - Training progress: {'loss': 0.3831, 'grad_norm': 1.4659043550491333, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.0954101915431878}
2025-04-28 23:33:38,073 - INFO - Training metrics: {'loss': 0.3831, 'grad_norm': 1.4659043550491333, 'learning_rate': 4.923966548707496e-06, 'epoch': 3.0954101915431878}
2025-04-28 23:36:52,562 - INFO - INFO: Training progress: {'loss': 0.3821, 'grad_norm': 1.9893252849578857, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.105047584628358}
2025-04-28 23:36:52,562 - INFO - Training progress: {'loss': 0.3821, 'grad_norm': 1.9893252849578857, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.105047584628358}
2025-04-28 23:36:52,562 - INFO - Training metrics: {'loss': 0.3821, 'grad_norm': 1.9893252849578857, 'learning_rate': 4.8804987565705405e-06, 'epoch': 3.105047584628358}
2025-04-28 23:40:08,109 - INFO - INFO: Training progress: {'loss': 0.3583, 'grad_norm': 1.7715368270874023, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.1146849777135284}
2025-04-28 23:40:08,109 - INFO - Training progress: {'loss': 0.3583, 'grad_norm': 1.7715368270874023, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.1146849777135284}
2025-04-28 23:40:08,109 - INFO - Training metrics: {'loss': 0.3583, 'grad_norm': 1.7715368270874023, 'learning_rate': 4.837130949481347e-06, 'epoch': 3.1146849777135284}
2025-04-28 23:43:25,223 - INFO - INFO: Training progress: {'loss': 0.3561, 'grad_norm': 1.856255292892456, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.124322370798699}
2025-04-28 23:43:25,224 - INFO - Training progress: {'loss': 0.3561, 'grad_norm': 1.856255292892456, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.124322370798699}
2025-04-28 23:43:25,224 - INFO - Training metrics: {'loss': 0.3561, 'grad_norm': 1.856255292892456, 'learning_rate': 4.793864782767343e-06, 'epoch': 3.124322370798699}
2025-04-28 23:46:40,755 - INFO - INFO: Training progress: {'loss': 0.3448, 'grad_norm': 1.6234837770462036, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1339597638838694}
2025-04-28 23:46:40,755 - INFO - Training progress: {'loss': 0.3448, 'grad_norm': 1.6234837770462036, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1339597638838694}
2025-04-28 23:46:40,755 - INFO - Training metrics: {'loss': 0.3448, 'grad_norm': 1.6234837770462036, 'learning_rate': 4.750701907876407e-06, 'epoch': 3.1339597638838694}
2025-04-28 23:49:57,181 - INFO - INFO: Training progress: {'loss': 0.3492, 'grad_norm': 1.8888949155807495, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1435971569690397}
2025-04-28 23:49:57,181 - INFO - Training progress: {'loss': 0.3492, 'grad_norm': 1.8888949155807495, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1435971569690397}
2025-04-28 23:49:57,181 - INFO - Training metrics: {'loss': 0.3492, 'grad_norm': 1.8888949155807495, 'learning_rate': 4.707643972313797e-06, 'epoch': 3.1435971569690397}
2025-04-28 23:53:12,590 - INFO - INFO: Training progress: {'loss': 0.3173, 'grad_norm': 1.8297840356826782, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.1532345500542105}
2025-04-28 23:53:12,590 - INFO - Training progress: {'loss': 0.3173, 'grad_norm': 1.8297840356826782, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.1532345500542105}
2025-04-28 23:53:12,590 - INFO - Training metrics: {'loss': 0.3173, 'grad_norm': 1.8297840356826782, 'learning_rate': 4.6646926195793155e-06, 'epoch': 3.1532345500542105}
2025-04-28 23:56:26,801 - INFO - INFO: Training progress: {'loss': 0.353, 'grad_norm': 1.5269644260406494, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.162871943139381}
2025-04-28 23:56:26,801 - INFO - Training progress: {'loss': 0.353, 'grad_norm': 1.5269644260406494, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.162871943139381}
2025-04-28 23:56:26,802 - INFO - Training metrics: {'loss': 0.353, 'grad_norm': 1.5269644260406494, 'learning_rate': 4.621849489104532e-06, 'epoch': 3.162871943139381}
2025-04-28 23:59:46,059 - INFO - INFO: Training progress: {'loss': 0.3456, 'grad_norm': 1.8247830867767334, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.172509336224551}
2025-04-28 23:59:46,059 - INFO - Training progress: {'loss': 0.3456, 'grad_norm': 1.8247830867767334, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.172509336224551}
2025-04-28 23:59:46,059 - INFO - Training metrics: {'loss': 0.3456, 'grad_norm': 1.8247830867767334, 'learning_rate': 4.579116216190238e-06, 'epoch': 3.172509336224551}
2025-04-29 00:03:03,408 - INFO - INFO: Training progress: {'loss': 0.3784, 'grad_norm': 1.7704166173934937, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.182146729309722}
2025-04-29 00:03:03,408 - INFO - Training progress: {'loss': 0.3784, 'grad_norm': 1.7704166173934937, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.182146729309722}
2025-04-29 00:03:03,408 - INFO - Training metrics: {'loss': 0.3784, 'grad_norm': 1.7704166173934937, 'learning_rate': 4.536494431944012e-06, 'epoch': 3.182146729309722}
2025-04-29 00:06:20,749 - INFO - INFO: Training progress: {'loss': 0.3337, 'grad_norm': 1.9107178449630737, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.191784122394892}
2025-04-29 00:06:20,749 - INFO - Training progress: {'loss': 0.3337, 'grad_norm': 1.9107178449630737, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.191784122394892}
2025-04-29 00:06:20,749 - INFO - Training metrics: {'loss': 0.3337, 'grad_norm': 1.9107178449630737, 'learning_rate': 4.493985763217967e-06, 'epoch': 3.191784122394892}
2025-04-29 00:09:38,023 - INFO - INFO: Training progress: {'loss': 0.3454, 'grad_norm': 1.5516648292541504, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.2014215154800625}
2025-04-29 00:09:38,023 - INFO - Training progress: {'loss': 0.3454, 'grad_norm': 1.5516648292541504, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.2014215154800625}
2025-04-29 00:09:38,023 - INFO - Training metrics: {'loss': 0.3454, 'grad_norm': 1.5516648292541504, 'learning_rate': 4.451591832546655e-06, 'epoch': 3.2014215154800625}
2025-04-29 00:12:55,897 - INFO - INFO: Training progress: {'loss': 0.3517, 'grad_norm': 1.5909719467163086, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2110589085652332}
2025-04-29 00:12:55,897 - INFO - Training progress: {'loss': 0.3517, 'grad_norm': 1.5909719467163086, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2110589085652332}
2025-04-29 00:12:55,897 - INFO - Training metrics: {'loss': 0.3517, 'grad_norm': 1.5909719467163086, 'learning_rate': 4.409314258085129e-06, 'epoch': 3.2110589085652332}
2025-04-29 00:16:14,350 - INFO - INFO: Training progress: {'loss': 0.3294, 'grad_norm': 1.6016674041748047, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.2206963016504035}
2025-04-29 00:16:14,351 - INFO - Training progress: {'loss': 0.3294, 'grad_norm': 1.6016674041748047, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.2206963016504035}
2025-04-29 00:16:14,351 - INFO - Training metrics: {'loss': 0.3294, 'grad_norm': 1.6016674041748047, 'learning_rate': 4.367154653547191e-06, 'epoch': 3.2206963016504035}
2025-04-29 00:19:31,573 - INFO - INFO: Training progress: {'loss': 0.3537, 'grad_norm': 1.8165363073349, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.230333694735574}
2025-04-29 00:19:31,573 - INFO - Training progress: {'loss': 0.3537, 'grad_norm': 1.8165363073349, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.230333694735574}
2025-04-29 00:19:31,573 - INFO - Training metrics: {'loss': 0.3537, 'grad_norm': 1.8165363073349, 'learning_rate': 4.325114628143785e-06, 'epoch': 3.230333694735574}
2025-04-29 00:22:49,868 - INFO - INFO: Training progress: {'loss': 0.3938, 'grad_norm': 1.555456280708313, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.2399710878207446}
2025-04-29 00:22:49,869 - INFO - Training progress: {'loss': 0.3938, 'grad_norm': 1.555456280708313, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.2399710878207446}
2025-04-29 00:22:49,869 - INFO - Training metrics: {'loss': 0.3938, 'grad_norm': 1.555456280708313, 'learning_rate': 4.283195786521585e-06, 'epoch': 3.2399710878207446}
2025-04-29 00:38:30,445 - INFO - INFO: Training progress: {'eval_loss': 0.35277318954467773, 'eval_runtime': 940.5715, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 3.2399710878207446}
2025-04-29 00:38:30,445 - INFO - Training progress: {'eval_loss': 0.35277318954467773, 'eval_runtime': 940.5715, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 3.2399710878207446}
2025-04-29 00:38:30,445 - INFO - Training metrics: {'eval_loss': 0.35277318954467773, 'eval_runtime': 940.5715, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 3.2399710878207446}
2025-04-29 00:41:48,928 - INFO - INFO: Training progress: {'loss': 0.3355, 'grad_norm': 1.7216427326202393, 'learning_rate': 4.241399728701733e-06, 'epoch': 3.249608480905915}
2025-04-29 00:41:48,928 - INFO - Training progress: {'loss': 0.3355, 'grad_norm': 1.7216427326202393, 'learning_rate': 4.241399728701733e-06, 'epoch': 3.249608480905915}
2025-04-29 00:41:48,928 - INFO - Training metrics: {'loss': 0.3355, 'grad_norm': 1.7216427326202393, 'learning_rate': 4.241399728701733e-06, 'epoch': 3.249608480905915}
2025-04-29 00:45:01,501 - INFO - INFO: Training progress: {'loss': 0.3666, 'grad_norm': 1.9009368419647217, 'learning_rate': 4.199728050018776e-06, 'epoch': 3.2592458739910852}
2025-04-29 00:45:01,501 - INFO - Training progress: {'loss': 0.3666, 'grad_norm': 1.9009368419647217, 'learning_rate': 4.199728050018776e-06, 'epoch': 3.2592458739910852}
2025-04-29 00:45:01,501 - INFO - Training metrics: {'loss': 0.3666, 'grad_norm': 1.9009368419647217, 'learning_rate': 4.199728050018776e-06, 'epoch': 3.2592458739910852}
2025-04-29 00:48:14,730 - INFO - INFO: Training progress: {'loss': 0.3398, 'grad_norm': 1.4997588396072388, 'learning_rate': 4.158182341059783e-06, 'epoch': 3.268883267076256}
2025-04-29 00:48:14,730 - INFO - Training progress: {'loss': 0.3398, 'grad_norm': 1.4997588396072388, 'learning_rate': 4.158182341059783e-06, 'epoch': 3.268883267076256}
2025-04-29 00:48:14,730 - INFO - Training metrics: {'loss': 0.3398, 'grad_norm': 1.4997588396072388, 'learning_rate': 4.158182341059783e-06, 'epoch': 3.268883267076256}
2025-04-29 00:51:26,732 - INFO - INFO: Training progress: {'loss': 0.3812, 'grad_norm': 1.5317809581756592, 'learning_rate': 4.11676418760361e-06, 'epoch': 3.2785206601614263}
2025-04-29 00:51:26,733 - INFO - Training progress: {'loss': 0.3812, 'grad_norm': 1.5317809581756592, 'learning_rate': 4.11676418760361e-06, 'epoch': 3.2785206601614263}
2025-04-29 00:51:26,733 - INFO - Training metrics: {'loss': 0.3812, 'grad_norm': 1.5317809581756592, 'learning_rate': 4.11676418760361e-06, 'epoch': 3.2785206601614263}
2025-04-29 00:51:27,704 - INFO - INFO: Saving checkpoint at step 3400
2025-04-29 00:51:27,704 - INFO - Saving checkpoint at step 3400
2025-04-29 00:51:27,704 - INFO - Saving checkpoint at step 3400
2025-04-29 00:54:41,308 - INFO - INFO: Training progress: {'loss': 0.3266, 'grad_norm': 1.874495267868042, 'learning_rate': 4.075475170560386e-06, 'epoch': 3.2881580532465966}
2025-04-29 00:54:41,308 - INFO - Training progress: {'loss': 0.3266, 'grad_norm': 1.874495267868042, 'learning_rate': 4.075475170560386e-06, 'epoch': 3.2881580532465966}
2025-04-29 00:54:41,308 - INFO - Training metrics: {'loss': 0.3266, 'grad_norm': 1.874495267868042, 'learning_rate': 4.075475170560386e-06, 'epoch': 3.2881580532465966}
2025-04-29 00:57:53,946 - INFO - INFO: Training progress: {'loss': 0.385, 'grad_norm': 1.9527454376220703, 'learning_rate': 4.03431686591117e-06, 'epoch': 3.2977954463317674}
2025-04-29 00:57:53,946 - INFO - Training progress: {'loss': 0.385, 'grad_norm': 1.9527454376220703, 'learning_rate': 4.03431686591117e-06, 'epoch': 3.2977954463317674}
2025-04-29 00:57:53,946 - INFO - Training metrics: {'loss': 0.385, 'grad_norm': 1.9527454376220703, 'learning_rate': 4.03431686591117e-06, 'epoch': 3.2977954463317674}
2025-04-29 01:01:06,368 - INFO - INFO: Training progress: {'loss': 0.3557, 'grad_norm': 1.74811851978302, 'learning_rate': 3.9932908446477885e-06, 'epoch': 3.3074328394169377}
2025-04-29 01:01:06,368 - INFO - Training progress: {'loss': 0.3557, 'grad_norm': 1.74811851978302, 'learning_rate': 3.9932908446477885e-06, 'epoch': 3.3074328394169377}
2025-04-29 01:01:06,368 - INFO - Training metrics: {'loss': 0.3557, 'grad_norm': 1.74811851978302, 'learning_rate': 3.9932908446477885e-06, 'epoch': 3.3074328394169377}
2025-04-29 01:04:18,134 - INFO - INFO: Training progress: {'loss': 0.3687, 'grad_norm': 2.4851481914520264, 'learning_rate': 3.952398672712892e-06, 'epoch': 3.3170702325021084}
2025-04-29 01:04:18,134 - INFO - Training progress: {'loss': 0.3687, 'grad_norm': 2.4851481914520264, 'learning_rate': 3.952398672712892e-06, 'epoch': 3.3170702325021084}
2025-04-29 01:04:18,134 - INFO - Training metrics: {'loss': 0.3687, 'grad_norm': 2.4851481914520264, 'learning_rate': 3.952398672712892e-06, 'epoch': 3.3170702325021084}
2025-04-29 01:07:38,840 - INFO - INFO: Training progress: {'loss': 0.3215, 'grad_norm': 1.857427954673767, 'learning_rate': 3.911641910940152e-06, 'epoch': 3.3267076255872787}
2025-04-29 01:07:38,841 - INFO - Training progress: {'loss': 0.3215, 'grad_norm': 1.857427954673767, 'learning_rate': 3.911641910940152e-06, 'epoch': 3.3267076255872787}
2025-04-29 01:07:38,841 - INFO - Training metrics: {'loss': 0.3215, 'grad_norm': 1.857427954673767, 'learning_rate': 3.911641910940152e-06, 'epoch': 3.3267076255872787}
2025-04-29 01:10:53,251 - INFO - INFO: Training progress: {'loss': 0.3357, 'grad_norm': 1.6425715684890747, 'learning_rate': 3.871022114994716e-06, 'epoch': 3.336345018672449}
2025-04-29 01:10:53,251 - INFO - Training progress: {'loss': 0.3357, 'grad_norm': 1.6425715684890747, 'learning_rate': 3.871022114994716e-06, 'epoch': 3.336345018672449}
2025-04-29 01:10:53,251 - INFO - Training metrics: {'loss': 0.3357, 'grad_norm': 1.6425715684890747, 'learning_rate': 3.871022114994716e-06, 'epoch': 3.336345018672449}
2025-04-29 01:14:08,613 - INFO - INFO: Training progress: {'loss': 0.3617, 'grad_norm': 1.5520312786102295, 'learning_rate': 3.830540835313808e-06, 'epoch': 3.3459824117576193}
2025-04-29 01:14:08,614 - INFO - Training progress: {'loss': 0.3617, 'grad_norm': 1.5520312786102295, 'learning_rate': 3.830540835313808e-06, 'epoch': 3.3459824117576193}
2025-04-29 01:14:08,614 - INFO - Training metrics: {'loss': 0.3617, 'grad_norm': 1.5520312786102295, 'learning_rate': 3.830540835313808e-06, 'epoch': 3.3459824117576193}
2025-04-29 01:17:21,692 - INFO - INFO: Training progress: {'loss': 0.3569, 'grad_norm': 2.1400301456451416, 'learning_rate': 3.790199617047549e-06, 'epoch': 3.35561980484279}
2025-04-29 01:17:21,692 - INFO - Training progress: {'loss': 0.3569, 'grad_norm': 2.1400301456451416, 'learning_rate': 3.790199617047549e-06, 'epoch': 3.35561980484279}
2025-04-29 01:17:21,692 - INFO - Training metrics: {'loss': 0.3569, 'grad_norm': 2.1400301456451416, 'learning_rate': 3.790199617047549e-06, 'epoch': 3.35561980484279}
2025-04-29 01:20:34,034 - INFO - INFO: Training progress: {'loss': 0.3142, 'grad_norm': 1.6797332763671875, 'learning_rate': 3.750000000000002e-06, 'epoch': 3.3652571979279604}
2025-04-29 01:20:34,035 - INFO - Training progress: {'loss': 0.3142, 'grad_norm': 1.6797332763671875, 'learning_rate': 3.750000000000002e-06, 'epoch': 3.3652571979279604}
2025-04-29 01:20:34,035 - INFO - Training metrics: {'loss': 0.3142, 'grad_norm': 1.6797332763671875, 'learning_rate': 3.750000000000002e-06, 'epoch': 3.3652571979279604}
2025-04-29 01:23:46,239 - INFO - INFO: Training progress: {'loss': 0.3187, 'grad_norm': 2.007887840270996, 'learning_rate': 3.709943518570366e-06, 'epoch': 3.374894591013131}
2025-04-29 01:23:46,239 - INFO - Training progress: {'loss': 0.3187, 'grad_norm': 2.007887840270996, 'learning_rate': 3.709943518570366e-06, 'epoch': 3.374894591013131}
2025-04-29 01:23:46,239 - INFO - Training metrics: {'loss': 0.3187, 'grad_norm': 2.007887840270996, 'learning_rate': 3.709943518570366e-06, 'epoch': 3.374894591013131}
2025-04-29 01:26:57,848 - INFO - INFO: Training progress: {'loss': 0.3485, 'grad_norm': 1.813063621520996, 'learning_rate': 3.6700317016944418e-06, 'epoch': 3.3845319840983015}
2025-04-29 01:26:57,848 - INFO - Training progress: {'loss': 0.3485, 'grad_norm': 1.813063621520996, 'learning_rate': 3.6700317016944418e-06, 'epoch': 3.3845319840983015}
2025-04-29 01:26:57,848 - INFO - Training metrics: {'loss': 0.3485, 'grad_norm': 1.813063621520996, 'learning_rate': 3.6700317016944418e-06, 'epoch': 3.3845319840983015}
2025-04-29 01:30:10,152 - INFO - INFO: Training progress: {'loss': 0.3517, 'grad_norm': 1.843682885169983, 'learning_rate': 3.6302660727862435e-06, 'epoch': 3.394169377183472}
2025-04-29 01:30:10,152 - INFO - Training progress: {'loss': 0.3517, 'grad_norm': 1.843682885169983, 'learning_rate': 3.6302660727862435e-06, 'epoch': 3.394169377183472}
2025-04-29 01:30:10,152 - INFO - Training metrics: {'loss': 0.3517, 'grad_norm': 1.843682885169983, 'learning_rate': 3.6302660727862435e-06, 'epoch': 3.394169377183472}
2025-04-29 01:33:22,616 - INFO - INFO: Training progress: {'loss': 0.3573, 'grad_norm': 1.780664324760437, 'learning_rate': 3.5906481496798698e-06, 'epoch': 3.403806770268642}
2025-04-29 01:33:22,617 - INFO - Training progress: {'loss': 0.3573, 'grad_norm': 1.780664324760437, 'learning_rate': 3.5906481496798698e-06, 'epoch': 3.403806770268642}
2025-04-29 01:33:22,617 - INFO - Training metrics: {'loss': 0.3573, 'grad_norm': 1.780664324760437, 'learning_rate': 3.5906481496798698e-06, 'epoch': 3.403806770268642}
2025-04-29 01:36:34,284 - INFO - INFO: Training progress: {'loss': 0.3126, 'grad_norm': 2.1818602085113525, 'learning_rate': 3.5511794445715604e-06, 'epoch': 3.413444163353813}
2025-04-29 01:36:34,284 - INFO - Training progress: {'loss': 0.3126, 'grad_norm': 2.1818602085113525, 'learning_rate': 3.5511794445715604e-06, 'epoch': 3.413444163353813}
2025-04-29 01:36:34,284 - INFO - Training metrics: {'loss': 0.3126, 'grad_norm': 2.1818602085113525, 'learning_rate': 3.5511794445715604e-06, 'epoch': 3.413444163353813}
2025-04-29 01:39:45,180 - INFO - INFO: Training progress: {'loss': 0.3014, 'grad_norm': 1.821070671081543, 'learning_rate': 3.5118614639619794e-06, 'epoch': 3.423081556438983}
2025-04-29 01:39:45,180 - INFO - Training progress: {'loss': 0.3014, 'grad_norm': 1.821070671081543, 'learning_rate': 3.5118614639619794e-06, 'epoch': 3.423081556438983}
2025-04-29 01:39:45,180 - INFO - Training metrics: {'loss': 0.3014, 'grad_norm': 1.821070671081543, 'learning_rate': 3.5118614639619794e-06, 'epoch': 3.423081556438983}
2025-04-29 01:42:57,971 - INFO - INFO: Training progress: {'loss': 0.3176, 'grad_norm': 1.697422981262207, 'learning_rate': 3.4726957085987167e-06, 'epoch': 3.432718949524154}
2025-04-29 01:42:57,971 - INFO - Training progress: {'loss': 0.3176, 'grad_norm': 1.697422981262207, 'learning_rate': 3.4726957085987167e-06, 'epoch': 3.432718949524154}
2025-04-29 01:42:57,971 - INFO - Training metrics: {'loss': 0.3176, 'grad_norm': 1.697422981262207, 'learning_rate': 3.4726957085987167e-06, 'epoch': 3.432718949524154}
2025-04-29 01:46:11,212 - INFO - INFO: Training progress: {'loss': 0.3495, 'grad_norm': 1.6303412914276123, 'learning_rate': 3.433683673418995e-06, 'epoch': 3.442356342609324}
2025-04-29 01:46:11,212 - INFO - Training progress: {'loss': 0.3495, 'grad_norm': 1.6303412914276123, 'learning_rate': 3.433683673418995e-06, 'epoch': 3.442356342609324}
2025-04-29 01:46:11,212 - INFO - Training metrics: {'loss': 0.3495, 'grad_norm': 1.6303412914276123, 'learning_rate': 3.433683673418995e-06, 'epoch': 3.442356342609324}
2025-04-29 02:01:13,367 - INFO - INFO: Training progress: {'eval_loss': 0.3478237986564636, 'eval_runtime': 902.153, 'eval_samples_per_second': 1.724, 'eval_steps_per_second': 1.724, 'epoch': 3.442356342609324}
2025-04-29 02:01:13,367 - INFO - Training progress: {'eval_loss': 0.3478237986564636, 'eval_runtime': 902.153, 'eval_samples_per_second': 1.724, 'eval_steps_per_second': 1.724, 'epoch': 3.442356342609324}
2025-04-29 02:01:13,367 - INFO - Training metrics: {'eval_loss': 0.3478237986564636, 'eval_runtime': 902.153, 'eval_samples_per_second': 1.724, 'eval_steps_per_second': 1.724, 'epoch': 3.442356342609324}
2025-04-29 02:04:25,508 - INFO - INFO: Training progress: {'loss': 0.32, 'grad_norm': 1.7440751791000366, 'learning_rate': 3.3948268474926227e-06, 'epoch': 3.4519937356944945}
2025-04-29 02:04:25,508 - INFO - Training progress: {'loss': 0.32, 'grad_norm': 1.7440751791000366, 'learning_rate': 3.3948268474926227e-06, 'epoch': 3.4519937356944945}
2025-04-29 02:04:25,508 - INFO - Training metrics: {'loss': 0.32, 'grad_norm': 1.7440751791000366, 'learning_rate': 3.3948268474926227e-06, 'epoch': 3.4519937356944945}
2025-04-29 02:07:37,196 - INFO - INFO: Training progress: {'loss': 0.3236, 'grad_norm': 1.668675184249878, 'learning_rate': 3.356126713965135e-06, 'epoch': 3.4616311287796653}
2025-04-29 02:07:37,196 - INFO - Training progress: {'loss': 0.3236, 'grad_norm': 1.668675184249878, 'learning_rate': 3.356126713965135e-06, 'epoch': 3.4616311287796653}
2025-04-29 02:07:37,196 - INFO - Training metrics: {'loss': 0.3236, 'grad_norm': 1.668675184249878, 'learning_rate': 3.356126713965135e-06, 'epoch': 3.4616311287796653}
2025-04-29 02:10:48,932 - INFO - INFO: Training progress: {'loss': 0.3221, 'grad_norm': 1.76789391040802, 'learning_rate': 3.3175847500012123e-06, 'epoch': 3.4712685218648356}
2025-04-29 02:10:48,932 - INFO - Training progress: {'loss': 0.3221, 'grad_norm': 1.76789391040802, 'learning_rate': 3.3175847500012123e-06, 'epoch': 3.4712685218648356}
2025-04-29 02:10:48,932 - INFO - Training metrics: {'loss': 0.3221, 'grad_norm': 1.76789391040802, 'learning_rate': 3.3175847500012123e-06, 'epoch': 3.4712685218648356}
2025-04-29 02:10:49,857 - INFO - INFO: Saving checkpoint at step 3600
2025-04-29 02:10:49,857 - INFO - Saving checkpoint at step 3600
2025-04-29 02:10:49,857 - INFO - Saving checkpoint at step 3600
2025-04-29 02:14:03,289 - INFO - INFO: Training progress: {'loss': 0.3258, 'grad_norm': 1.6934294700622559, 'learning_rate': 3.2792024267282726e-06, 'epoch': 3.480905914950006}
2025-04-29 02:14:03,289 - INFO - Training progress: {'loss': 0.3258, 'grad_norm': 1.6934294700622559, 'learning_rate': 3.2792024267282726e-06, 'epoch': 3.480905914950006}
2025-04-29 02:14:03,289 - INFO - Training metrics: {'loss': 0.3258, 'grad_norm': 1.6934294700622559, 'learning_rate': 3.2792024267282726e-06, 'epoch': 3.480905914950006}
2025-04-29 02:17:17,436 - INFO - INFO: Training progress: {'loss': 0.3549, 'grad_norm': 1.817496657371521, 'learning_rate': 3.2409812091803294e-06, 'epoch': 3.4905433080351767}
2025-04-29 02:17:17,436 - INFO - Training progress: {'loss': 0.3549, 'grad_norm': 1.817496657371521, 'learning_rate': 3.2409812091803294e-06, 'epoch': 3.4905433080351767}
2025-04-29 02:17:17,436 - INFO - Training metrics: {'loss': 0.3549, 'grad_norm': 1.817496657371521, 'learning_rate': 3.2409812091803294e-06, 'epoch': 3.4905433080351767}
2025-04-29 02:20:35,761 - INFO - INFO: Training progress: {'loss': 0.3491, 'grad_norm': 1.3867137432098389, 'learning_rate': 3.202922556242079e-06, 'epoch': 3.500180701120347}
2025-04-29 02:20:35,761 - INFO - Training progress: {'loss': 0.3491, 'grad_norm': 1.3867137432098389, 'learning_rate': 3.202922556242079e-06, 'epoch': 3.500180701120347}
2025-04-29 02:20:35,761 - INFO - Training metrics: {'loss': 0.3491, 'grad_norm': 1.3867137432098389, 'learning_rate': 3.202922556242079e-06, 'epoch': 3.500180701120347}
2025-04-29 02:23:48,999 - INFO - INFO: Training progress: {'loss': 0.3629, 'grad_norm': 1.4486719369888306, 'learning_rate': 3.1650279205932003e-06, 'epoch': 3.5098180942055173}
2025-04-29 02:23:48,999 - INFO - Training progress: {'loss': 0.3629, 'grad_norm': 1.4486719369888306, 'learning_rate': 3.1650279205932003e-06, 'epoch': 3.5098180942055173}
2025-04-29 02:23:49,000 - INFO - Training metrics: {'loss': 0.3629, 'grad_norm': 1.4486719369888306, 'learning_rate': 3.1650279205932003e-06, 'epoch': 3.5098180942055173}
2025-04-29 02:27:12,243 - INFO - INFO: Training progress: {'loss': 0.404, 'grad_norm': 1.520228624343872, 'learning_rate': 3.1272987486529165e-06, 'epoch': 3.519455487290688}
2025-04-29 02:27:12,243 - INFO - Training progress: {'loss': 0.404, 'grad_norm': 1.520228624343872, 'learning_rate': 3.1272987486529165e-06, 'epoch': 3.519455487290688}
2025-04-29 02:27:12,243 - INFO - Training metrics: {'loss': 0.404, 'grad_norm': 1.520228624343872, 'learning_rate': 3.1272987486529165e-06, 'epoch': 3.519455487290688}
2025-04-29 02:30:30,642 - INFO - INFO: Training progress: {'loss': 0.3284, 'grad_norm': 1.8413619995117188, 'learning_rate': 3.0897364805247844e-06, 'epoch': 3.5290928803758583}
2025-04-29 02:30:30,642 - INFO - Training progress: {'loss': 0.3284, 'grad_norm': 1.8413619995117188, 'learning_rate': 3.0897364805247844e-06, 'epoch': 3.5290928803758583}
2025-04-29 02:30:30,642 - INFO - Training metrics: {'loss': 0.3284, 'grad_norm': 1.8413619995117188, 'learning_rate': 3.0897364805247844e-06, 'epoch': 3.5290928803758583}
2025-04-29 02:33:54,477 - INFO - INFO: Training progress: {'loss': 0.3397, 'grad_norm': 1.6740586757659912, 'learning_rate': 3.052342549941725e-06, 'epoch': 3.5387302734610286}
2025-04-29 02:33:54,477 - INFO - Training progress: {'loss': 0.3397, 'grad_norm': 1.6740586757659912, 'learning_rate': 3.052342549941725e-06, 'epoch': 3.5387302734610286}
2025-04-29 02:33:54,477 - INFO - Training metrics: {'loss': 0.3397, 'grad_norm': 1.6740586757659912, 'learning_rate': 3.052342549941725e-06, 'epoch': 3.5387302734610286}
2025-04-29 02:37:17,420 - INFO - INFO: Training progress: {'loss': 0.3352, 'grad_norm': 1.5781826972961426, 'learning_rate': 3.015118384211306e-06, 'epoch': 3.5483676665461994}
2025-04-29 02:37:17,420 - INFO - Training progress: {'loss': 0.3352, 'grad_norm': 1.5781826972961426, 'learning_rate': 3.015118384211306e-06, 'epoch': 3.5483676665461994}
2025-04-29 02:37:17,420 - INFO - Training metrics: {'loss': 0.3352, 'grad_norm': 1.5781826972961426, 'learning_rate': 3.015118384211306e-06, 'epoch': 3.5483676665461994}
2025-04-29 02:40:42,082 - INFO - INFO: Training progress: {'loss': 0.3517, 'grad_norm': 2.183692455291748, 'learning_rate': 2.9780654041612444e-06, 'epoch': 3.5580050596313697}
2025-04-29 02:40:42,082 - INFO - Training progress: {'loss': 0.3517, 'grad_norm': 2.183692455291748, 'learning_rate': 2.9780654041612444e-06, 'epoch': 3.5580050596313697}
2025-04-29 02:40:42,082 - INFO - Training metrics: {'loss': 0.3517, 'grad_norm': 2.183692455291748, 'learning_rate': 2.9780654041612444e-06, 'epoch': 3.5580050596313697}
2025-04-29 02:44:02,689 - INFO - INFO: Training progress: {'loss': 0.3395, 'grad_norm': 1.880442500114441, 'learning_rate': 2.9411850240852016e-06, 'epoch': 3.56764245271654}
2025-04-29 02:44:02,689 - INFO - Training progress: {'loss': 0.3395, 'grad_norm': 1.880442500114441, 'learning_rate': 2.9411850240852016e-06, 'epoch': 3.56764245271654}
2025-04-29 02:44:02,689 - INFO - Training metrics: {'loss': 0.3395, 'grad_norm': 1.880442500114441, 'learning_rate': 2.9411850240852016e-06, 'epoch': 3.56764245271654}
2025-04-29 02:47:16,590 - INFO - INFO: Training progress: {'loss': 0.2967, 'grad_norm': 1.8376086950302124, 'learning_rate': 2.9044786516887636e-06, 'epoch': 3.5772798458017108}
2025-04-29 02:47:16,591 - INFO - Training progress: {'loss': 0.2967, 'grad_norm': 1.8376086950302124, 'learning_rate': 2.9044786516887636e-06, 'epoch': 3.5772798458017108}
2025-04-29 02:47:16,591 - INFO - Training metrics: {'loss': 0.2967, 'grad_norm': 1.8376086950302124, 'learning_rate': 2.9044786516887636e-06, 'epoch': 3.5772798458017108}
2025-04-29 02:50:30,332 - INFO - INFO: Training progress: {'loss': 0.3544, 'grad_norm': 1.9312324523925781, 'learning_rate': 2.867947688035751e-06, 'epoch': 3.586917238886881}
2025-04-29 02:50:30,332 - INFO - Training progress: {'loss': 0.3544, 'grad_norm': 1.9312324523925781, 'learning_rate': 2.867947688035751e-06, 'epoch': 3.586917238886881}
2025-04-29 02:50:30,332 - INFO - Training metrics: {'loss': 0.3544, 'grad_norm': 1.9312324523925781, 'learning_rate': 2.867947688035751e-06, 'epoch': 3.586917238886881}
2025-04-29 02:53:45,131 - INFO - INFO: Training progress: {'loss': 0.3371, 'grad_norm': 1.8879368305206299, 'learning_rate': 2.8315935274947084e-06, 'epoch': 3.596554631972052}
2025-04-29 02:53:45,131 - INFO - Training progress: {'loss': 0.3371, 'grad_norm': 1.8879368305206299, 'learning_rate': 2.8315935274947084e-06, 'epoch': 3.596554631972052}
2025-04-29 02:53:45,131 - INFO - Training metrics: {'loss': 0.3371, 'grad_norm': 1.8879368305206299, 'learning_rate': 2.8315935274947084e-06, 'epoch': 3.596554631972052}
2025-04-29 02:56:56,507 - INFO - INFO: Training progress: {'loss': 0.3537, 'grad_norm': 2.1263058185577393, 'learning_rate': 2.7954175576856965e-06, 'epoch': 3.606192025057222}
2025-04-29 02:56:56,507 - INFO - Training progress: {'loss': 0.3537, 'grad_norm': 2.1263058185577393, 'learning_rate': 2.7954175576856965e-06, 'epoch': 3.606192025057222}
2025-04-29 02:56:56,507 - INFO - Training metrics: {'loss': 0.3537, 'grad_norm': 2.1263058185577393, 'learning_rate': 2.7954175576856965e-06, 'epoch': 3.606192025057222}
2025-04-29 03:00:07,917 - INFO - INFO: Training progress: {'loss': 0.3151, 'grad_norm': 1.656424641609192, 'learning_rate': 2.7594211594273315e-06, 'epoch': 3.6158294181423924}
2025-04-29 03:00:07,917 - INFO - Training progress: {'loss': 0.3151, 'grad_norm': 1.656424641609192, 'learning_rate': 2.7594211594273315e-06, 'epoch': 3.6158294181423924}
2025-04-29 03:00:07,917 - INFO - Training metrics: {'loss': 0.3151, 'grad_norm': 1.656424641609192, 'learning_rate': 2.7594211594273315e-06, 'epoch': 3.6158294181423924}
2025-04-29 03:03:21,838 - INFO - INFO: Training progress: {'loss': 0.3654, 'grad_norm': 1.7129881381988525, 'learning_rate': 2.723605706684065e-06, 'epoch': 3.6254668112275628}
2025-04-29 03:03:21,838 - INFO - Training progress: {'loss': 0.3654, 'grad_norm': 1.7129881381988525, 'learning_rate': 2.723605706684065e-06, 'epoch': 3.6254668112275628}
2025-04-29 03:03:21,838 - INFO - Training metrics: {'loss': 0.3654, 'grad_norm': 1.7129881381988525, 'learning_rate': 2.723605706684065e-06, 'epoch': 3.6254668112275628}
2025-04-29 03:06:33,622 - INFO - INFO: Training progress: {'loss': 0.33, 'grad_norm': 1.5609723329544067, 'learning_rate': 2.6879725665137536e-06, 'epoch': 3.6351042043127335}
2025-04-29 03:06:33,623 - INFO - Training progress: {'loss': 0.33, 'grad_norm': 1.5609723329544067, 'learning_rate': 2.6879725665137536e-06, 'epoch': 3.6351042043127335}
2025-04-29 03:06:33,623 - INFO - Training metrics: {'loss': 0.33, 'grad_norm': 1.5609723329544067, 'learning_rate': 2.6879725665137536e-06, 'epoch': 3.6351042043127335}
2025-04-29 03:09:44,637 - INFO - INFO: Training progress: {'loss': 0.3352, 'grad_norm': 1.7538412809371948, 'learning_rate': 2.652523099015474e-06, 'epoch': 3.644741597397904}
2025-04-29 03:09:44,637 - INFO - Training progress: {'loss': 0.3352, 'grad_norm': 1.7538412809371948, 'learning_rate': 2.652523099015474e-06, 'epoch': 3.644741597397904}
2025-04-29 03:09:44,637 - INFO - Training metrics: {'loss': 0.3352, 'grad_norm': 1.7538412809371948, 'learning_rate': 2.652523099015474e-06, 'epoch': 3.644741597397904}
2025-04-29 03:24:41,323 - INFO - INFO: Training progress: {'eval_loss': 0.3439994752407074, 'eval_runtime': 896.6825, 'eval_samples_per_second': 1.734, 'eval_steps_per_second': 1.734, 'epoch': 3.644741597397904}
2025-04-29 03:24:41,323 - INFO - Training progress: {'eval_loss': 0.3439994752407074, 'eval_runtime': 896.6825, 'eval_samples_per_second': 1.734, 'eval_steps_per_second': 1.734, 'epoch': 3.644741597397904}
2025-04-29 03:24:41,323 - INFO - Training metrics: {'eval_loss': 0.3439994752407074, 'eval_runtime': 896.6825, 'eval_samples_per_second': 1.734, 'eval_steps_per_second': 1.734, 'epoch': 3.644741597397904}
2025-04-29 03:27:52,562 - INFO - INFO: Training progress: {'loss': 0.3467, 'grad_norm': 1.8066229820251465, 'learning_rate': 2.6172586572776063e-06, 'epoch': 3.6543789904830746}
2025-04-29 03:27:52,562 - INFO - Training progress: {'loss': 0.3467, 'grad_norm': 1.8066229820251465, 'learning_rate': 2.6172586572776063e-06, 'epoch': 3.6543789904830746}
2025-04-29 03:27:52,562 - INFO - Training metrics: {'loss': 0.3467, 'grad_norm': 1.8066229820251465, 'learning_rate': 2.6172586572776063e-06, 'epoch': 3.6543789904830746}
2025-04-29 03:31:03,456 - INFO - INFO: Training progress: {'loss': 0.3129, 'grad_norm': 2.060110092163086, 'learning_rate': 2.5821805873261975e-06, 'epoch': 3.664016383568245}
2025-04-29 03:31:03,457 - INFO - Training progress: {'loss': 0.3129, 'grad_norm': 2.060110092163086, 'learning_rate': 2.5821805873261975e-06, 'epoch': 3.664016383568245}
2025-04-29 03:31:03,457 - INFO - Training metrics: {'loss': 0.3129, 'grad_norm': 2.060110092163086, 'learning_rate': 2.5821805873261975e-06, 'epoch': 3.664016383568245}
2025-04-29 03:31:04,445 - INFO - INFO: Saving checkpoint at step 3800
2025-04-29 03:31:04,445 - INFO - Saving checkpoint at step 3800
2025-04-29 03:31:04,445 - INFO - Saving checkpoint at step 3800
2025-04-29 03:34:16,399 - INFO - INFO: Training progress: {'loss': 0.3659, 'grad_norm': 1.6477611064910889, 'learning_rate': 2.547290228073569e-06, 'epoch': 3.673653776653415}
2025-04-29 03:34:16,399 - INFO - Training progress: {'loss': 0.3659, 'grad_norm': 1.6477611064910889, 'learning_rate': 2.547290228073569e-06, 'epoch': 3.673653776653415}
2025-04-29 03:34:16,399 - INFO - Training metrics: {'loss': 0.3659, 'grad_norm': 1.6477611064910889, 'learning_rate': 2.547290228073569e-06, 'epoch': 3.673653776653415}
2025-04-29 03:37:28,151 - INFO - INFO: Training progress: {'loss': 0.3248, 'grad_norm': 2.1978044509887695, 'learning_rate': 2.5125889112672245e-06, 'epoch': 3.6832911697385855}
2025-04-29 03:37:28,151 - INFO - Training progress: {'loss': 0.3248, 'grad_norm': 2.1978044509887695, 'learning_rate': 2.5125889112672245e-06, 'epoch': 3.6832911697385855}
2025-04-29 03:37:28,151 - INFO - Training metrics: {'loss': 0.3248, 'grad_norm': 2.1978044509887695, 'learning_rate': 2.5125889112672245e-06, 'epoch': 3.6832911697385855}
2025-04-29 03:40:39,352 - INFO - INFO: Training progress: {'loss': 0.3516, 'grad_norm': 1.8038740158081055, 'learning_rate': 2.4780779614390058e-06, 'epoch': 3.6929285628237563}
2025-04-29 03:40:39,352 - INFO - Training progress: {'loss': 0.3516, 'grad_norm': 1.8038740158081055, 'learning_rate': 2.4780779614390058e-06, 'epoch': 3.6929285628237563}
2025-04-29 03:40:39,352 - INFO - Training metrics: {'loss': 0.3516, 'grad_norm': 1.8038740158081055, 'learning_rate': 2.4780779614390058e-06, 'epoch': 3.6929285628237563}
2025-04-29 03:43:50,747 - INFO - INFO: Training progress: {'loss': 0.342, 'grad_norm': 1.8194819688796997, 'learning_rate': 2.4437586958545537e-06, 'epoch': 3.7025659559089266}
2025-04-29 03:43:50,748 - INFO - Training progress: {'loss': 0.342, 'grad_norm': 1.8194819688796997, 'learning_rate': 2.4437586958545537e-06, 'epoch': 3.7025659559089266}
2025-04-29 03:43:50,748 - INFO - Training metrics: {'loss': 0.342, 'grad_norm': 1.8194819688796997, 'learning_rate': 2.4437586958545537e-06, 'epoch': 3.7025659559089266}
2025-04-29 03:47:02,559 - INFO - INFO: Training progress: {'loss': 0.2965, 'grad_norm': 2.281548261642456, 'learning_rate': 2.4096324244630073e-06, 'epoch': 3.7122033489940973}
2025-04-29 03:47:02,559 - INFO - Training progress: {'loss': 0.2965, 'grad_norm': 2.281548261642456, 'learning_rate': 2.4096324244630073e-06, 'epoch': 3.7122033489940973}
2025-04-29 03:47:02,559 - INFO - Training metrics: {'loss': 0.2965, 'grad_norm': 2.281548261642456, 'learning_rate': 2.4096324244630073e-06, 'epoch': 3.7122033489940973}
2025-04-29 03:50:14,444 - INFO - INFO: Training progress: {'loss': 0.3399, 'grad_norm': 1.8769280910491943, 'learning_rate': 2.375700449847016e-06, 'epoch': 3.7218407420792676}
2025-04-29 03:50:14,444 - INFO - Training progress: {'loss': 0.3399, 'grad_norm': 1.8769280910491943, 'learning_rate': 2.375700449847016e-06, 'epoch': 3.7218407420792676}
2025-04-29 03:50:14,444 - INFO - Training metrics: {'loss': 0.3399, 'grad_norm': 1.8769280910491943, 'learning_rate': 2.375700449847016e-06, 'epoch': 3.7218407420792676}
2025-04-29 03:53:25,870 - INFO - INFO: Training progress: {'loss': 0.309, 'grad_norm': 1.4945358037948608, 'learning_rate': 2.3419640671730255e-06, 'epoch': 3.731478135164438}
2025-04-29 03:53:25,870 - INFO - Training progress: {'loss': 0.309, 'grad_norm': 1.4945358037948608, 'learning_rate': 2.3419640671730255e-06, 'epoch': 3.731478135164438}
2025-04-29 03:53:25,871 - INFO - Training metrics: {'loss': 0.309, 'grad_norm': 1.4945358037948608, 'learning_rate': 2.3419640671730255e-06, 'epoch': 3.731478135164438}
2025-04-29 03:56:36,907 - INFO - INFO: Training progress: {'loss': 0.3612, 'grad_norm': 1.6831811666488647, 'learning_rate': 2.308424564141829e-06, 'epoch': 3.7411155282496082}
2025-04-29 03:56:36,908 - INFO - Training progress: {'loss': 0.3612, 'grad_norm': 1.6831811666488647, 'learning_rate': 2.308424564141829e-06, 'epoch': 3.7411155282496082}
2025-04-29 03:56:36,908 - INFO - Training metrics: {'loss': 0.3612, 'grad_norm': 1.6831811666488647, 'learning_rate': 2.308424564141829e-06, 'epoch': 3.7411155282496082}
2025-04-29 03:59:48,486 - INFO - INFO: Training progress: {'loss': 0.373, 'grad_norm': 2.4219179153442383, 'learning_rate': 2.2750832209394232e-06, 'epoch': 3.750752921334779}
2025-04-29 03:59:48,486 - INFO - Training progress: {'loss': 0.373, 'grad_norm': 2.4219179153442383, 'learning_rate': 2.2750832209394232e-06, 'epoch': 3.750752921334779}
2025-04-29 03:59:48,487 - INFO - Training metrics: {'loss': 0.373, 'grad_norm': 2.4219179153442383, 'learning_rate': 2.2750832209394232e-06, 'epoch': 3.750752921334779}
2025-04-29 04:02:59,470 - INFO - INFO: Training progress: {'loss': 0.3005, 'grad_norm': 1.6366368532180786, 'learning_rate': 2.241941310188145e-06, 'epoch': 3.7603903144199493}
2025-04-29 04:02:59,470 - INFO - Training progress: {'loss': 0.3005, 'grad_norm': 1.6366368532180786, 'learning_rate': 2.241941310188145e-06, 'epoch': 3.7603903144199493}
2025-04-29 04:02:59,470 - INFO - Training metrics: {'loss': 0.3005, 'grad_norm': 1.6366368532180786, 'learning_rate': 2.241941310188145e-06, 'epoch': 3.7603903144199493}
2025-04-29 04:06:11,100 - INFO - INFO: Training progress: {'loss': 0.3205, 'grad_norm': 1.5477968454360962, 'learning_rate': 2.209000096898091e-06, 'epoch': 3.77002770750512}
2025-04-29 04:06:11,101 - INFO - Training progress: {'loss': 0.3205, 'grad_norm': 1.5477968454360962, 'learning_rate': 2.209000096898091e-06, 'epoch': 3.77002770750512}
2025-04-29 04:06:11,101 - INFO - Training metrics: {'loss': 0.3205, 'grad_norm': 1.5477968454360962, 'learning_rate': 2.209000096898091e-06, 'epoch': 3.77002770750512}
2025-04-29 04:09:22,821 - INFO - INFO: Training progress: {'loss': 0.3532, 'grad_norm': 1.7798112630844116, 'learning_rate': 2.176260838418845e-06, 'epoch': 3.7796651005902904}
2025-04-29 04:09:22,821 - INFO - Training progress: {'loss': 0.3532, 'grad_norm': 1.7798112630844116, 'learning_rate': 2.176260838418845e-06, 'epoch': 3.7796651005902904}
2025-04-29 04:09:22,821 - INFO - Training metrics: {'loss': 0.3532, 'grad_norm': 1.7798112630844116, 'learning_rate': 2.176260838418845e-06, 'epoch': 3.7796651005902904}
2025-04-29 04:12:34,465 - INFO - INFO: Training progress: {'loss': 0.3422, 'grad_norm': 1.8107917308807373, 'learning_rate': 2.1437247843914703e-06, 'epoch': 3.7893024936754607}
2025-04-29 04:12:34,466 - INFO - Training progress: {'loss': 0.3422, 'grad_norm': 1.8107917308807373, 'learning_rate': 2.1437247843914703e-06, 'epoch': 3.7893024936754607}
2025-04-29 04:12:34,466 - INFO - Training metrics: {'loss': 0.3422, 'grad_norm': 1.8107917308807373, 'learning_rate': 2.1437247843914703e-06, 'epoch': 3.7893024936754607}
2025-04-29 04:15:46,066 - INFO - INFO: Training progress: {'loss': 0.3443, 'grad_norm': 1.6342756748199463, 'learning_rate': 2.1113931767008177e-06, 'epoch': 3.798939886760631}
2025-04-29 04:15:46,066 - INFO - Training progress: {'loss': 0.3443, 'grad_norm': 1.6342756748199463, 'learning_rate': 2.1113931767008177e-06, 'epoch': 3.798939886760631}
2025-04-29 04:15:46,066 - INFO - Training metrics: {'loss': 0.3443, 'grad_norm': 1.6342756748199463, 'learning_rate': 2.1113931767008177e-06, 'epoch': 3.798939886760631}
2025-04-29 04:18:57,924 - INFO - INFO: Training progress: {'loss': 0.3165, 'grad_norm': 1.743838906288147, 'learning_rate': 2.079267249428124e-06, 'epoch': 3.8085772798458017}
2025-04-29 04:18:57,924 - INFO - Training progress: {'loss': 0.3165, 'grad_norm': 1.743838906288147, 'learning_rate': 2.079267249428124e-06, 'epoch': 3.8085772798458017}
2025-04-29 04:18:57,924 - INFO - Training metrics: {'loss': 0.3165, 'grad_norm': 1.743838906288147, 'learning_rate': 2.079267249428124e-06, 'epoch': 3.8085772798458017}
2025-04-29 04:22:09,706 - INFO - INFO: Training progress: {'loss': 0.3276, 'grad_norm': 2.32155442237854, 'learning_rate': 2.047348228803916e-06, 'epoch': 3.818214672930972}
2025-04-29 04:22:09,706 - INFO - Training progress: {'loss': 0.3276, 'grad_norm': 2.32155442237854, 'learning_rate': 2.047348228803916e-06, 'epoch': 3.818214672930972}
2025-04-29 04:22:09,706 - INFO - Training metrics: {'loss': 0.3276, 'grad_norm': 2.32155442237854, 'learning_rate': 2.047348228803916e-06, 'epoch': 3.818214672930972}
2025-04-29 04:25:21,399 - INFO - INFO: Training progress: {'loss': 0.3361, 'grad_norm': 1.7854433059692383, 'learning_rate': 2.015637333161186e-06, 'epoch': 3.827852066016143}
2025-04-29 04:25:21,399 - INFO - Training progress: {'loss': 0.3361, 'grad_norm': 1.7854433059692383, 'learning_rate': 2.015637333161186e-06, 'epoch': 3.827852066016143}
2025-04-29 04:25:21,399 - INFO - Training metrics: {'loss': 0.3361, 'grad_norm': 1.7854433059692383, 'learning_rate': 2.015637333161186e-06, 'epoch': 3.827852066016143}
2025-04-29 04:28:33,005 - INFO - INFO: Training progress: {'loss': 0.3342, 'grad_norm': 1.5832487344741821, 'learning_rate': 1.9841357728889055e-06, 'epoch': 3.837489459101313}
2025-04-29 04:28:33,005 - INFO - Training progress: {'loss': 0.3342, 'grad_norm': 1.5832487344741821, 'learning_rate': 1.9841357728889055e-06, 'epoch': 3.837489459101313}
2025-04-29 04:28:33,005 - INFO - Training metrics: {'loss': 0.3342, 'grad_norm': 1.5832487344741821, 'learning_rate': 1.9841357728889055e-06, 'epoch': 3.837489459101313}
2025-04-29 04:31:44,750 - INFO - INFO: Training progress: {'loss': 0.3236, 'grad_norm': 1.6283159255981445, 'learning_rate': 1.9528447503858253e-06, 'epoch': 3.8471268521864834}
2025-04-29 04:31:44,750 - INFO - Training progress: {'loss': 0.3236, 'grad_norm': 1.6283159255981445, 'learning_rate': 1.9528447503858253e-06, 'epoch': 3.8471268521864834}
2025-04-29 04:31:44,751 - INFO - Training metrics: {'loss': 0.3236, 'grad_norm': 1.6283159255981445, 'learning_rate': 1.9528447503858253e-06, 'epoch': 3.8471268521864834}
2025-04-29 04:46:58,382 - INFO - INFO: Training progress: {'eval_loss': 0.34136104583740234, 'eval_runtime': 913.6272, 'eval_samples_per_second': 1.702, 'eval_steps_per_second': 1.702, 'epoch': 3.8471268521864834}
2025-04-29 04:46:58,382 - INFO - Training progress: {'eval_loss': 0.34136104583740234, 'eval_runtime': 913.6272, 'eval_samples_per_second': 1.702, 'eval_steps_per_second': 1.702, 'epoch': 3.8471268521864834}
2025-04-29 04:46:58,382 - INFO - Training metrics: {'eval_loss': 0.34136104583740234, 'eval_runtime': 913.6272, 'eval_samples_per_second': 1.702, 'eval_steps_per_second': 1.702, 'epoch': 3.8471268521864834}
2025-04-29 04:50:10,262 - INFO - INFO: Training progress: {'loss': 0.333, 'grad_norm': 2.041522741317749, 'learning_rate': 1.921765460014561e-06, 'epoch': 3.856764245271654}
2025-04-29 04:50:10,262 - INFO - Training progress: {'loss': 0.333, 'grad_norm': 2.041522741317749, 'learning_rate': 1.921765460014561e-06, 'epoch': 3.856764245271654}
2025-04-29 04:50:10,262 - INFO - Training metrics: {'loss': 0.333, 'grad_norm': 2.041522741317749, 'learning_rate': 1.921765460014561e-06, 'epoch': 3.856764245271654}
2025-04-29 04:50:11,277 - INFO - INFO: Saving checkpoint at step 4000
2025-04-29 04:50:11,277 - INFO - Saving checkpoint at step 4000
2025-04-29 04:50:11,277 - INFO - Saving checkpoint at step 4000
2025-04-29 04:53:22,797 - INFO - INFO: Training progress: {'loss': 0.3522, 'grad_norm': 1.9917643070220947, 'learning_rate': 1.890899088056034e-06, 'epoch': 3.8664016383568245}
2025-04-29 04:53:22,797 - INFO - Training progress: {'loss': 0.3522, 'grad_norm': 1.9917643070220947, 'learning_rate': 1.890899088056034e-06, 'epoch': 3.8664016383568245}
2025-04-29 04:53:22,797 - INFO - Training metrics: {'loss': 0.3522, 'grad_norm': 1.9917643070220947, 'learning_rate': 1.890899088056034e-06, 'epoch': 3.8664016383568245}
2025-04-29 04:56:34,578 - INFO - INFO: Training progress: {'loss': 0.3174, 'grad_norm': 2.037104606628418, 'learning_rate': 1.8602468126641689e-06, 'epoch': 3.876039031441995}
2025-04-29 04:56:34,578 - INFO - Training progress: {'loss': 0.3174, 'grad_norm': 2.037104606628418, 'learning_rate': 1.8602468126641689e-06, 'epoch': 3.876039031441995}
2025-04-29 04:56:34,579 - INFO - Training metrics: {'loss': 0.3174, 'grad_norm': 2.037104606628418, 'learning_rate': 1.8602468126641689e-06, 'epoch': 3.876039031441995}
2025-04-29 04:59:46,261 - INFO - INFO: Training progress: {'loss': 0.2939, 'grad_norm': 1.8295378684997559, 'learning_rate': 1.8298098038209305e-06, 'epoch': 3.8856764245271656}
2025-04-29 04:59:46,261 - INFO - Training progress: {'loss': 0.2939, 'grad_norm': 1.8295378684997559, 'learning_rate': 1.8298098038209305e-06, 'epoch': 3.8856764245271656}
2025-04-29 04:59:46,261 - INFO - Training metrics: {'loss': 0.2939, 'grad_norm': 1.8295378684997559, 'learning_rate': 1.8298098038209305e-06, 'epoch': 3.8856764245271656}
2025-04-29 05:02:57,710 - INFO - INFO: Training progress: {'loss': 0.3028, 'grad_norm': 1.967871904373169, 'learning_rate': 1.7995892232916759e-06, 'epoch': 3.895313817612336}
2025-04-29 05:02:57,711 - INFO - Training progress: {'loss': 0.3028, 'grad_norm': 1.967871904373169, 'learning_rate': 1.7995892232916759e-06, 'epoch': 3.895313817612336}
2025-04-29 05:02:57,711 - INFO - Training metrics: {'loss': 0.3028, 'grad_norm': 1.967871904373169, 'learning_rate': 1.7995892232916759e-06, 'epoch': 3.895313817612336}
2025-04-29 05:06:09,216 - INFO - INFO: Training progress: {'loss': 0.3209, 'grad_norm': 1.8053414821624756, 'learning_rate': 1.7695862245807929e-06, 'epoch': 3.904951210697506}
2025-04-29 05:06:09,216 - INFO - Training progress: {'loss': 0.3209, 'grad_norm': 1.8053414821624756, 'learning_rate': 1.7695862245807929e-06, 'epoch': 3.904951210697506}
2025-04-29 05:06:09,216 - INFO - Training metrics: {'loss': 0.3209, 'grad_norm': 1.8053414821624756, 'learning_rate': 1.7695862245807929e-06, 'epoch': 3.904951210697506}
2025-04-29 05:09:20,544 - INFO - INFO: Training progress: {'loss': 0.2957, 'grad_norm': 2.30823016166687, 'learning_rate': 1.7398019528876852e-06, 'epoch': 3.914588603782677}
2025-04-29 05:09:20,545 - INFO - Training progress: {'loss': 0.2957, 'grad_norm': 2.30823016166687, 'learning_rate': 1.7398019528876852e-06, 'epoch': 3.914588603782677}
2025-04-29 05:09:20,545 - INFO - Training metrics: {'loss': 0.2957, 'grad_norm': 2.30823016166687, 'learning_rate': 1.7398019528876852e-06, 'epoch': 3.914588603782677}
2025-04-29 05:12:32,171 - INFO - INFO: Training progress: {'loss': 0.3137, 'grad_norm': 1.7658721208572388, 'learning_rate': 1.7102375450630522e-06, 'epoch': 3.9242259968678472}
2025-04-29 05:12:32,171 - INFO - Training progress: {'loss': 0.3137, 'grad_norm': 1.7658721208572388, 'learning_rate': 1.7102375450630522e-06, 'epoch': 3.9242259968678472}
2025-04-29 05:12:32,171 - INFO - Training metrics: {'loss': 0.3137, 'grad_norm': 1.7658721208572388, 'learning_rate': 1.7102375450630522e-06, 'epoch': 3.9242259968678472}
2025-04-29 05:15:43,825 - INFO - INFO: Training progress: {'loss': 0.2963, 'grad_norm': 1.8725415468215942, 'learning_rate': 1.6808941295655072e-06, 'epoch': 3.933863389953018}
2025-04-29 05:15:43,825 - INFO - Training progress: {'loss': 0.2963, 'grad_norm': 1.8725415468215942, 'learning_rate': 1.6808941295655072e-06, 'epoch': 3.933863389953018}
2025-04-29 05:15:43,825 - INFO - Training metrics: {'loss': 0.2963, 'grad_norm': 1.8725415468215942, 'learning_rate': 1.6808941295655072e-06, 'epoch': 3.933863389953018}
2025-04-29 05:18:55,364 - INFO - INFO: Training progress: {'loss': 0.3109, 'grad_norm': 2.159302234649658, 'learning_rate': 1.6517728264184892e-06, 'epoch': 3.9435007830381883}
2025-04-29 05:18:55,365 - INFO - Training progress: {'loss': 0.3109, 'grad_norm': 2.159302234649658, 'learning_rate': 1.6517728264184892e-06, 'epoch': 3.9435007830381883}
2025-04-29 05:18:55,365 - INFO - Training metrics: {'loss': 0.3109, 'grad_norm': 2.159302234649658, 'learning_rate': 1.6517728264184892e-06, 'epoch': 3.9435007830381883}
2025-04-29 05:22:06,619 - INFO - INFO: Training progress: {'loss': 0.3527, 'grad_norm': 1.7930183410644531, 'learning_rate': 1.6228747471675238e-06, 'epoch': 3.9531381761233586}
2025-04-29 05:22:06,620 - INFO - Training progress: {'loss': 0.3527, 'grad_norm': 1.7930183410644531, 'learning_rate': 1.6228747471675238e-06, 'epoch': 3.9531381761233586}
2025-04-29 05:22:06,620 - INFO - Training metrics: {'loss': 0.3527, 'grad_norm': 1.7930183410644531, 'learning_rate': 1.6228747471675238e-06, 'epoch': 3.9531381761233586}
2025-04-29 05:25:18,220 - INFO - INFO: Training progress: {'loss': 0.2924, 'grad_norm': 2.091296911239624, 'learning_rate': 1.5942009948377962e-06, 'epoch': 3.962775569208529}
2025-04-29 05:25:18,221 - INFO - Training progress: {'loss': 0.2924, 'grad_norm': 2.091296911239624, 'learning_rate': 1.5942009948377962e-06, 'epoch': 3.962775569208529}
2025-04-29 05:25:18,221 - INFO - Training metrics: {'loss': 0.2924, 'grad_norm': 2.091296911239624, 'learning_rate': 1.5942009948377962e-06, 'epoch': 3.962775569208529}
2025-04-29 05:28:29,769 - INFO - INFO: Training progress: {'loss': 0.3611, 'grad_norm': 2.3783164024353027, 'learning_rate': 1.5657526638920352e-06, 'epoch': 3.9724129622936997}
2025-04-29 05:28:29,770 - INFO - Training progress: {'loss': 0.3611, 'grad_norm': 2.3783164024353027, 'learning_rate': 1.5657526638920352e-06, 'epoch': 3.9724129622936997}
2025-04-29 05:28:29,770 - INFO - Training metrics: {'loss': 0.3611, 'grad_norm': 2.3783164024353027, 'learning_rate': 1.5657526638920352e-06, 'epoch': 3.9724129622936997}
2025-04-29 05:31:41,362 - INFO - INFO: Training progress: {'loss': 0.3115, 'grad_norm': 1.6884304285049438, 'learning_rate': 1.5375308401887608e-06, 'epoch': 3.98205035537887}
2025-04-29 05:31:41,362 - INFO - Training progress: {'loss': 0.3115, 'grad_norm': 1.6884304285049438, 'learning_rate': 1.5375308401887608e-06, 'epoch': 3.98205035537887}
2025-04-29 05:31:41,362 - INFO - Training metrics: {'loss': 0.3115, 'grad_norm': 1.6884304285049438, 'learning_rate': 1.5375308401887608e-06, 'epoch': 3.98205035537887}
2025-04-29 05:34:53,073 - INFO - INFO: Training progress: {'loss': 0.328, 'grad_norm': 1.5795150995254517, 'learning_rate': 1.5095366009408176e-06, 'epoch': 3.9916877484640407}
2025-04-29 05:34:53,073 - INFO - Training progress: {'loss': 0.328, 'grad_norm': 1.5795150995254517, 'learning_rate': 1.5095366009408176e-06, 'epoch': 3.9916877484640407}
2025-04-29 05:34:53,073 - INFO - Training metrics: {'loss': 0.328, 'grad_norm': 1.5795150995254517, 'learning_rate': 1.5095366009408176e-06, 'epoch': 3.9916877484640407}
2025-04-29 05:37:39,051 - INFO - INFO: Starting epoch 3.999397662932177/5
2025-04-29 05:37:39,051 - INFO - Starting epoch 3.999397662932177/5
2025-04-29 05:37:39,051 - INFO - Starting epoch 3.999397662932177/5
2025-04-29 05:38:13,267 - INFO - INFO: Training progress: {'loss': 0.3868, 'grad_norm': 1.863813042640686, 'learning_rate': 1.4817710146742658e-06, 'epoch': 4.001325141549211}
2025-04-29 05:38:13,268 - INFO - Training progress: {'loss': 0.3868, 'grad_norm': 1.863813042640686, 'learning_rate': 1.4817710146742658e-06, 'epoch': 4.001325141549211}
2025-04-29 05:38:13,268 - INFO - Training metrics: {'loss': 0.3868, 'grad_norm': 1.863813042640686, 'learning_rate': 1.4817710146742658e-06, 'epoch': 4.001325141549211}
2025-04-29 05:41:24,953 - INFO - INFO: Training progress: {'loss': 0.3208, 'grad_norm': 2.0021207332611084, 'learning_rate': 1.454235141187601e-06, 'epoch': 4.010962534634381}
2025-04-29 05:41:24,953 - INFO - Training progress: {'loss': 0.3208, 'grad_norm': 2.0021207332611084, 'learning_rate': 1.454235141187601e-06, 'epoch': 4.010962534634381}
2025-04-29 05:41:24,953 - INFO - Training metrics: {'loss': 0.3208, 'grad_norm': 2.0021207332611084, 'learning_rate': 1.454235141187601e-06, 'epoch': 4.010962534634381}
2025-04-29 05:44:36,272 - INFO - INFO: Training progress: {'loss': 0.2945, 'grad_norm': 1.7686975002288818, 'learning_rate': 1.4269300315112924e-06, 'epoch': 4.020599927719552}
2025-04-29 05:44:36,272 - INFO - Training progress: {'loss': 0.2945, 'grad_norm': 1.7686975002288818, 'learning_rate': 1.4269300315112924e-06, 'epoch': 4.020599927719552}
2025-04-29 05:44:36,272 - INFO - Training metrics: {'loss': 0.2945, 'grad_norm': 1.7686975002288818, 'learning_rate': 1.4269300315112924e-06, 'epoch': 4.020599927719552}
2025-04-29 05:47:47,820 - INFO - INFO: Training progress: {'loss': 0.3189, 'grad_norm': 2.073173999786377, 'learning_rate': 1.3998567278676736e-06, 'epoch': 4.030237320804722}
2025-04-29 05:47:47,820 - INFO - Training progress: {'loss': 0.3189, 'grad_norm': 2.073173999786377, 'learning_rate': 1.3998567278676736e-06, 'epoch': 4.030237320804722}
2025-04-29 05:47:47,820 - INFO - Training metrics: {'loss': 0.3189, 'grad_norm': 2.073173999786377, 'learning_rate': 1.3998567278676736e-06, 'epoch': 4.030237320804722}
2025-04-29 05:50:59,309 - INFO - INFO: Training progress: {'loss': 0.3242, 'grad_norm': 2.061025381088257, 'learning_rate': 1.373016263631154e-06, 'epoch': 4.039874713889893}
2025-04-29 05:50:59,309 - INFO - Training progress: {'loss': 0.3242, 'grad_norm': 2.061025381088257, 'learning_rate': 1.373016263631154e-06, 'epoch': 4.039874713889893}
2025-04-29 05:50:59,309 - INFO - Training metrics: {'loss': 0.3242, 'grad_norm': 2.061025381088257, 'learning_rate': 1.373016263631154e-06, 'epoch': 4.039874713889893}
2025-04-29 05:54:10,998 - INFO - INFO: Training progress: {'loss': 0.3207, 'grad_norm': 2.198110580444336, 'learning_rate': 1.3464096632887859e-06, 'epoch': 4.0495121069750635}
2025-04-29 05:54:10,999 - INFO - Training progress: {'loss': 0.3207, 'grad_norm': 2.198110580444336, 'learning_rate': 1.3464096632887859e-06, 'epoch': 4.0495121069750635}
2025-04-29 05:54:10,999 - INFO - Training metrics: {'loss': 0.3207, 'grad_norm': 2.198110580444336, 'learning_rate': 1.3464096632887859e-06, 'epoch': 4.0495121069750635}
2025-04-29 06:09:16,181 - INFO - INFO: Training progress: {'eval_loss': 0.33917009830474854, 'eval_runtime': 905.1794, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.0495121069750635}
2025-04-29 06:09:16,181 - INFO - Training progress: {'eval_loss': 0.33917009830474854, 'eval_runtime': 905.1794, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.0495121069750635}
2025-04-29 06:09:16,181 - INFO - Training metrics: {'eval_loss': 0.33917009830474854, 'eval_runtime': 905.1794, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.0495121069750635}
2025-04-29 06:09:17,246 - INFO - INFO: Saving checkpoint at step 4200
2025-04-29 06:09:17,247 - INFO - Saving checkpoint at step 4200
2025-04-29 06:09:17,247 - INFO - Saving checkpoint at step 4200
2025-04-29 06:12:28,848 - INFO - INFO: Training progress: {'loss': 0.318, 'grad_norm': 2.037433385848999, 'learning_rate': 1.3200379424011496e-06, 'epoch': 4.059149500060234}
2025-04-29 06:12:28,848 - INFO - Training progress: {'loss': 0.318, 'grad_norm': 2.037433385848999, 'learning_rate': 1.3200379424011496e-06, 'epoch': 4.059149500060234}
2025-04-29 06:12:28,848 - INFO - Training metrics: {'loss': 0.318, 'grad_norm': 2.037433385848999, 'learning_rate': 1.3200379424011496e-06, 'epoch': 4.059149500060234}
2025-04-29 06:15:42,335 - INFO - INFO: Training progress: {'loss': 0.3071, 'grad_norm': 2.2232155799865723, 'learning_rate': 1.2939021075635953e-06, 'epoch': 4.068786893145404}
2025-04-29 06:15:42,336 - INFO - Training progress: {'loss': 0.3071, 'grad_norm': 2.2232155799865723, 'learning_rate': 1.2939021075635953e-06, 'epoch': 4.068786893145404}
2025-04-29 06:15:42,336 - INFO - Training metrics: {'loss': 0.3071, 'grad_norm': 2.2232155799865723, 'learning_rate': 1.2939021075635953e-06, 'epoch': 4.068786893145404}
2025-04-29 06:18:53,913 - INFO - INFO: Training progress: {'loss': 0.3512, 'grad_norm': 2.061988592147827, 'learning_rate': 1.2680031563678215e-06, 'epoch': 4.078424286230574}
2025-04-29 06:18:53,914 - INFO - Training progress: {'loss': 0.3512, 'grad_norm': 2.061988592147827, 'learning_rate': 1.2680031563678215e-06, 'epoch': 4.078424286230574}
2025-04-29 06:18:53,914 - INFO - Training metrics: {'loss': 0.3512, 'grad_norm': 2.061988592147827, 'learning_rate': 1.2680031563678215e-06, 'epoch': 4.078424286230574}
2025-04-29 06:22:05,211 - INFO - INFO: Training progress: {'loss': 0.3578, 'grad_norm': 2.0408310890197754, 'learning_rate': 1.2423420773637958e-06, 'epoch': 4.088061679315745}
2025-04-29 06:22:05,212 - INFO - Training progress: {'loss': 0.3578, 'grad_norm': 2.0408310890197754, 'learning_rate': 1.2423420773637958e-06, 'epoch': 4.088061679315745}
2025-04-29 06:22:05,212 - INFO - Training metrics: {'loss': 0.3578, 'grad_norm': 2.0408310890197754, 'learning_rate': 1.2423420773637958e-06, 'epoch': 4.088061679315745}
2025-04-29 06:25:17,067 - INFO - INFO: Training progress: {'loss': 0.3182, 'grad_norm': 1.9298837184906006, 'learning_rate': 1.2169198500220279e-06, 'epoch': 4.097699072400916}
2025-04-29 06:25:17,068 - INFO - Training progress: {'loss': 0.3182, 'grad_norm': 1.9298837184906006, 'learning_rate': 1.2169198500220279e-06, 'epoch': 4.097699072400916}
2025-04-29 06:25:17,068 - INFO - Training metrics: {'loss': 0.3182, 'grad_norm': 1.9298837184906006, 'learning_rate': 1.2169198500220279e-06, 'epoch': 4.097699072400916}
2025-04-29 06:28:28,628 - INFO - INFO: Training progress: {'loss': 0.3335, 'grad_norm': 1.960965871810913, 'learning_rate': 1.1917374446961745e-06, 'epoch': 4.107336465486086}
2025-04-29 06:28:28,628 - INFO - Training progress: {'loss': 0.3335, 'grad_norm': 1.960965871810913, 'learning_rate': 1.1917374446961745e-06, 'epoch': 4.107336465486086}
2025-04-29 06:28:28,628 - INFO - Training metrics: {'loss': 0.3335, 'grad_norm': 1.960965871810913, 'learning_rate': 1.1917374446961745e-06, 'epoch': 4.107336465486086}
2025-04-29 06:31:40,381 - INFO - INFO: Training progress: {'loss': 0.3075, 'grad_norm': 1.970415472984314, 'learning_rate': 1.1667958225860063e-06, 'epoch': 4.1169738585712565}
2025-04-29 06:31:40,382 - INFO - Training progress: {'loss': 0.3075, 'grad_norm': 1.970415472984314, 'learning_rate': 1.1667958225860063e-06, 'epoch': 4.1169738585712565}
2025-04-29 06:31:40,382 - INFO - Training metrics: {'loss': 0.3075, 'grad_norm': 1.970415472984314, 'learning_rate': 1.1667958225860063e-06, 'epoch': 4.1169738585712565}
2025-04-29 06:34:51,929 - INFO - INFO: Training progress: {'loss': 0.3254, 'grad_norm': 2.696686267852783, 'learning_rate': 1.1420959357007243e-06, 'epoch': 4.126611251656427}
2025-04-29 06:34:51,929 - INFO - Training progress: {'loss': 0.3254, 'grad_norm': 2.696686267852783, 'learning_rate': 1.1420959357007243e-06, 'epoch': 4.126611251656427}
2025-04-29 06:34:51,929 - INFO - Training metrics: {'loss': 0.3254, 'grad_norm': 2.696686267852783, 'learning_rate': 1.1420959357007243e-06, 'epoch': 4.126611251656427}
2025-04-29 06:38:03,347 - INFO - INFO: Training progress: {'loss': 0.3061, 'grad_norm': 1.7000699043273926, 'learning_rate': 1.1176387268226138e-06, 'epoch': 4.136248644741597}
2025-04-29 06:38:03,347 - INFO - Training progress: {'loss': 0.3061, 'grad_norm': 1.7000699043273926, 'learning_rate': 1.1176387268226138e-06, 'epoch': 4.136248644741597}
2025-04-29 06:38:03,347 - INFO - Training metrics: {'loss': 0.3061, 'grad_norm': 1.7000699043273926, 'learning_rate': 1.1176387268226138e-06, 'epoch': 4.136248644741597}
2025-04-29 06:41:14,755 - INFO - INFO: Training progress: {'loss': 0.3356, 'grad_norm': 2.099199056625366, 'learning_rate': 1.093425129471062e-06, 'epoch': 4.1458860378267675}
2025-04-29 06:41:14,755 - INFO - Training progress: {'loss': 0.3356, 'grad_norm': 2.099199056625366, 'learning_rate': 1.093425129471062e-06, 'epoch': 4.1458860378267675}
2025-04-29 06:41:14,756 - INFO - Training metrics: {'loss': 0.3356, 'grad_norm': 2.099199056625366, 'learning_rate': 1.093425129471062e-06, 'epoch': 4.1458860378267675}
2025-04-29 06:44:26,211 - INFO - INFO: Training progress: {'loss': 0.3312, 'grad_norm': 1.883004069328308, 'learning_rate': 1.0694560678669258e-06, 'epoch': 4.155523430911939}
2025-04-29 06:44:26,211 - INFO - Training progress: {'loss': 0.3312, 'grad_norm': 1.883004069328308, 'learning_rate': 1.0694560678669258e-06, 'epoch': 4.155523430911939}
2025-04-29 06:44:26,211 - INFO - Training metrics: {'loss': 0.3312, 'grad_norm': 1.883004069328308, 'learning_rate': 1.0694560678669258e-06, 'epoch': 4.155523430911939}
2025-04-29 06:47:37,507 - INFO - INFO: Training progress: {'loss': 0.3093, 'grad_norm': 2.6730103492736816, 'learning_rate': 1.04573245689726e-06, 'epoch': 4.165160823997109}
2025-04-29 06:47:37,507 - INFO - Training progress: {'loss': 0.3093, 'grad_norm': 2.6730103492736816, 'learning_rate': 1.04573245689726e-06, 'epoch': 4.165160823997109}
2025-04-29 06:47:37,508 - INFO - Training metrics: {'loss': 0.3093, 'grad_norm': 2.6730103492736816, 'learning_rate': 1.04573245689726e-06, 'epoch': 4.165160823997109}
2025-04-29 06:50:49,062 - INFO - INFO: Training progress: {'loss': 0.2901, 'grad_norm': 2.1346166133880615, 'learning_rate': 1.022255202080387e-06, 'epoch': 4.174798217082279}
2025-04-29 06:50:49,062 - INFO - Training progress: {'loss': 0.2901, 'grad_norm': 2.1346166133880615, 'learning_rate': 1.022255202080387e-06, 'epoch': 4.174798217082279}
2025-04-29 06:50:49,062 - INFO - Training metrics: {'loss': 0.2901, 'grad_norm': 2.1346166133880615, 'learning_rate': 1.022255202080387e-06, 'epoch': 4.174798217082279}
2025-04-29 06:54:00,501 - INFO - INFO: Training progress: {'loss': 0.3068, 'grad_norm': 1.8670754432678223, 'learning_rate': 9.990251995313407e-07, 'epoch': 4.18443561016745}
2025-04-29 06:54:00,502 - INFO - Training progress: {'loss': 0.3068, 'grad_norm': 1.8670754432678223, 'learning_rate': 9.990251995313407e-07, 'epoch': 4.18443561016745}
2025-04-29 06:54:00,502 - INFO - Training metrics: {'loss': 0.3068, 'grad_norm': 1.8670754432678223, 'learning_rate': 9.990251995313407e-07, 'epoch': 4.18443561016745}
2025-04-29 06:57:11,884 - INFO - INFO: Training progress: {'loss': 0.3462, 'grad_norm': 1.681915044784546, 'learning_rate': 9.760433359276589e-07, 'epoch': 4.19407300325262}
2025-04-29 06:57:11,884 - INFO - Training progress: {'loss': 0.3462, 'grad_norm': 1.681915044784546, 'learning_rate': 9.760433359276589e-07, 'epoch': 4.19407300325262}
2025-04-29 06:57:11,884 - INFO - Training metrics: {'loss': 0.3462, 'grad_norm': 1.681915044784546, 'learning_rate': 9.760433359276589e-07, 'epoch': 4.19407300325262}
2025-04-29 07:00:23,757 - INFO - INFO: Training progress: {'loss': 0.2968, 'grad_norm': 2.0293846130371094, 'learning_rate': 9.533104884755392e-07, 'epoch': 4.20371039633779}
2025-04-29 07:00:23,758 - INFO - Training progress: {'loss': 0.2968, 'grad_norm': 2.0293846130371094, 'learning_rate': 9.533104884755392e-07, 'epoch': 4.20371039633779}
2025-04-29 07:00:23,758 - INFO - Training metrics: {'loss': 0.2968, 'grad_norm': 2.0293846130371094, 'learning_rate': 9.533104884755392e-07, 'epoch': 4.20371039633779}
2025-04-29 07:03:35,300 - INFO - INFO: Training progress: {'loss': 0.2922, 'grad_norm': 2.7475156784057617, 'learning_rate': 9.308275248763634e-07, 'epoch': 4.213347789422961}
2025-04-29 07:03:35,300 - INFO - Training progress: {'loss': 0.2922, 'grad_norm': 2.7475156784057617, 'learning_rate': 9.308275248763634e-07, 'epoch': 4.213347789422961}
2025-04-29 07:03:35,301 - INFO - Training metrics: {'loss': 0.2922, 'grad_norm': 2.7475156784057617, 'learning_rate': 9.308275248763634e-07, 'epoch': 4.213347789422961}
2025-04-29 07:06:46,525 - INFO - INFO: Training progress: {'loss': 0.2846, 'grad_norm': 2.1058590412139893, 'learning_rate': 9.085953032935668e-07, 'epoch': 4.222985182508132}
2025-04-29 07:06:46,525 - INFO - Training progress: {'loss': 0.2846, 'grad_norm': 2.1058590412139893, 'learning_rate': 9.085953032935668e-07, 'epoch': 4.222985182508132}
2025-04-29 07:06:46,525 - INFO - Training metrics: {'loss': 0.2846, 'grad_norm': 2.1058590412139893, 'learning_rate': 9.085953032935668e-07, 'epoch': 4.222985182508132}
2025-04-29 07:09:58,184 - INFO - INFO: Training progress: {'loss': 0.2976, 'grad_norm': 1.7982521057128906, 'learning_rate': 8.866146723198862e-07, 'epoch': 4.232622575593302}
2025-04-29 07:09:58,184 - INFO - Training progress: {'loss': 0.2976, 'grad_norm': 1.7982521057128906, 'learning_rate': 8.866146723198862e-07, 'epoch': 4.232622575593302}
2025-04-29 07:09:58,184 - INFO - Training metrics: {'loss': 0.2976, 'grad_norm': 1.7982521057128906, 'learning_rate': 8.866146723198862e-07, 'epoch': 4.232622575593302}
2025-04-29 07:13:09,641 - INFO - INFO: Training progress: {'loss': 0.3131, 'grad_norm': 1.720468282699585, 'learning_rate': 8.648864709449794e-07, 'epoch': 4.242259968678472}
2025-04-29 07:13:09,641 - INFO - Training progress: {'loss': 0.3131, 'grad_norm': 1.720468282699585, 'learning_rate': 8.648864709449794e-07, 'epoch': 4.242259968678472}
2025-04-29 07:13:09,641 - INFO - Training metrics: {'loss': 0.3131, 'grad_norm': 1.720468282699585, 'learning_rate': 8.648864709449794e-07, 'epoch': 4.242259968678472}
2025-04-29 07:13:10,628 - INFO - INFO: Saving checkpoint at step 4400
2025-04-29 07:13:10,628 - INFO - Saving checkpoint at step 4400
2025-04-29 07:13:10,628 - INFO - Saving checkpoint at step 4400
2025-04-29 07:16:22,145 - INFO - INFO: Training progress: {'loss': 0.3296, 'grad_norm': 1.9648951292037964, 'learning_rate': 8.434115285233823e-07, 'epoch': 4.251897361763643}
2025-04-29 07:16:22,145 - INFO - Training progress: {'loss': 0.3296, 'grad_norm': 1.9648951292037964, 'learning_rate': 8.434115285233823e-07, 'epoch': 4.251897361763643}
2025-04-29 07:16:22,145 - INFO - Training metrics: {'loss': 0.3296, 'grad_norm': 1.9648951292037964, 'learning_rate': 8.434115285233823e-07, 'epoch': 4.251897361763643}
2025-04-29 07:31:34,182 - INFO - INFO: Training progress: {'eval_loss': 0.3375541567802429, 'eval_runtime': 912.0338, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705, 'epoch': 4.251897361763643}
2025-04-29 07:31:34,182 - INFO - Training progress: {'eval_loss': 0.3375541567802429, 'eval_runtime': 912.0338, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705, 'epoch': 4.251897361763643}
2025-04-29 07:31:34,183 - INFO - Training metrics: {'eval_loss': 0.3375541567802429, 'eval_runtime': 912.0338, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705, 'epoch': 4.251897361763643}
2025-04-29 07:34:45,763 - INFO - INFO: Training progress: {'loss': 0.3049, 'grad_norm': 2.0120413303375244, 'learning_rate': 8.221906647428726e-07, 'epoch': 4.261534754848814}
2025-04-29 07:34:45,763 - INFO - Training progress: {'loss': 0.3049, 'grad_norm': 2.0120413303375244, 'learning_rate': 8.221906647428726e-07, 'epoch': 4.261534754848814}
2025-04-29 07:34:45,763 - INFO - Training metrics: {'loss': 0.3049, 'grad_norm': 2.0120413303375244, 'learning_rate': 8.221906647428726e-07, 'epoch': 4.261534754848814}
2025-04-29 07:37:57,563 - INFO - INFO: Training progress: {'loss': 0.3469, 'grad_norm': 1.7895346879959106, 'learning_rate': 8.01224689593168e-07, 'epoch': 4.271172147933984}
2025-04-29 07:37:57,563 - INFO - Training progress: {'loss': 0.3469, 'grad_norm': 1.7895346879959106, 'learning_rate': 8.01224689593168e-07, 'epoch': 4.271172147933984}
2025-04-29 07:37:57,564 - INFO - Training metrics: {'loss': 0.3469, 'grad_norm': 1.7895346879959106, 'learning_rate': 8.01224689593168e-07, 'epoch': 4.271172147933984}
2025-04-29 07:41:08,982 - INFO - INFO: Training progress: {'loss': 0.2888, 'grad_norm': 1.9405667781829834, 'learning_rate': 7.805144033350195e-07, 'epoch': 4.2808095410191545}
2025-04-29 07:41:08,983 - INFO - Training progress: {'loss': 0.2888, 'grad_norm': 1.9405667781829834, 'learning_rate': 7.805144033350195e-07, 'epoch': 4.2808095410191545}
2025-04-29 07:41:08,983 - INFO - Training metrics: {'loss': 0.2888, 'grad_norm': 1.9405667781829834, 'learning_rate': 7.805144033350195e-07, 'epoch': 4.2808095410191545}
2025-04-29 07:44:19,895 - INFO - INFO: Training progress: {'loss': 0.3554, 'grad_norm': 1.8589634895324707, 'learning_rate': 7.600605964696583e-07, 'epoch': 4.290446934104325}
2025-04-29 07:44:19,896 - INFO - Training progress: {'loss': 0.3554, 'grad_norm': 1.8589634895324707, 'learning_rate': 7.600605964696583e-07, 'epoch': 4.290446934104325}
2025-04-29 07:44:19,896 - INFO - Training metrics: {'loss': 0.3554, 'grad_norm': 1.8589634895324707, 'learning_rate': 7.600605964696583e-07, 'epoch': 4.290446934104325}
2025-04-29 07:47:31,550 - INFO - INFO: Training progress: {'loss': 0.3637, 'grad_norm': 1.775505781173706, 'learning_rate': 7.398640497086273e-07, 'epoch': 4.300084327189495}
2025-04-29 07:47:31,551 - INFO - Training progress: {'loss': 0.3637, 'grad_norm': 1.775505781173706, 'learning_rate': 7.398640497086273e-07, 'epoch': 4.300084327189495}
2025-04-29 07:47:31,551 - INFO - Training metrics: {'loss': 0.3637, 'grad_norm': 1.775505781173706, 'learning_rate': 7.398640497086273e-07, 'epoch': 4.300084327189495}
2025-04-29 07:50:43,040 - INFO - INFO: Training progress: {'loss': 0.312, 'grad_norm': 2.1135427951812744, 'learning_rate': 7.199255339439801e-07, 'epoch': 4.309721720274665}
2025-04-29 07:50:43,040 - INFO - Training progress: {'loss': 0.312, 'grad_norm': 2.1135427951812744, 'learning_rate': 7.199255339439801e-07, 'epoch': 4.309721720274665}
2025-04-29 07:50:43,040 - INFO - Training metrics: {'loss': 0.312, 'grad_norm': 2.1135427951812744, 'learning_rate': 7.199255339439801e-07, 'epoch': 4.309721720274665}
2025-04-29 07:53:54,752 - INFO - INFO: Training progress: {'loss': 0.3178, 'grad_norm': 2.7598142623901367, 'learning_rate': 7.002458102188533e-07, 'epoch': 4.319359113359837}
2025-04-29 07:53:54,752 - INFO - Training progress: {'loss': 0.3178, 'grad_norm': 2.7598142623901367, 'learning_rate': 7.002458102188533e-07, 'epoch': 4.319359113359837}
2025-04-29 07:53:54,752 - INFO - Training metrics: {'loss': 0.3178, 'grad_norm': 2.7598142623901367, 'learning_rate': 7.002458102188533e-07, 'epoch': 4.319359113359837}
2025-04-29 07:57:06,114 - INFO - INFO: Training progress: {'loss': 0.3224, 'grad_norm': 1.766969919204712, 'learning_rate': 6.808256296984289e-07, 'epoch': 4.328996506445007}
2025-04-29 07:57:06,114 - INFO - Training progress: {'loss': 0.3224, 'grad_norm': 1.766969919204712, 'learning_rate': 6.808256296984289e-07, 'epoch': 4.328996506445007}
2025-04-29 07:57:06,114 - INFO - Training metrics: {'loss': 0.3224, 'grad_norm': 1.766969919204712, 'learning_rate': 6.808256296984289e-07, 'epoch': 4.328996506445007}
2025-04-29 08:00:17,616 - INFO - INFO: Training progress: {'loss': 0.2973, 'grad_norm': 1.6067396402359009, 'learning_rate': 6.616657336412496e-07, 'epoch': 4.338633899530177}
2025-04-29 08:00:17,617 - INFO - Training progress: {'loss': 0.2973, 'grad_norm': 1.6067396402359009, 'learning_rate': 6.616657336412496e-07, 'epoch': 4.338633899530177}
2025-04-29 08:00:17,617 - INFO - Training metrics: {'loss': 0.2973, 'grad_norm': 1.6067396402359009, 'learning_rate': 6.616657336412496e-07, 'epoch': 4.338633899530177}
2025-04-29 08:03:29,257 - INFO - INFO: Training progress: {'loss': 0.3158, 'grad_norm': 1.7624315023422241, 'learning_rate': 6.427668533709288e-07, 'epoch': 4.3482712926153475}
2025-04-29 08:03:29,257 - INFO - Training progress: {'loss': 0.3158, 'grad_norm': 1.7624315023422241, 'learning_rate': 6.427668533709288e-07, 'epoch': 4.3482712926153475}
2025-04-29 08:03:29,257 - INFO - Training metrics: {'loss': 0.3158, 'grad_norm': 1.7624315023422241, 'learning_rate': 6.427668533709288e-07, 'epoch': 4.3482712926153475}
2025-04-29 08:06:41,020 - INFO - INFO: Training progress: {'loss': 0.3453, 'grad_norm': 1.9269906282424927, 'learning_rate': 6.241297102482468e-07, 'epoch': 4.357908685700518}
2025-04-29 08:06:41,020 - INFO - Training progress: {'loss': 0.3453, 'grad_norm': 1.9269906282424927, 'learning_rate': 6.241297102482468e-07, 'epoch': 4.357908685700518}
2025-04-29 08:06:41,021 - INFO - Training metrics: {'loss': 0.3453, 'grad_norm': 1.9269906282424927, 'learning_rate': 6.241297102482468e-07, 'epoch': 4.357908685700518}
2025-04-29 08:09:52,749 - INFO - INFO: Training progress: {'loss': 0.349, 'grad_norm': 2.061445713043213, 'learning_rate': 6.057550156435962e-07, 'epoch': 4.367546078785688}
2025-04-29 08:09:52,749 - INFO - Training progress: {'loss': 0.349, 'grad_norm': 2.061445713043213, 'learning_rate': 6.057550156435962e-07, 'epoch': 4.367546078785688}
2025-04-29 08:09:52,749 - INFO - Training metrics: {'loss': 0.349, 'grad_norm': 2.061445713043213, 'learning_rate': 6.057550156435962e-07, 'epoch': 4.367546078785688}
2025-04-29 08:13:04,267 - INFO - INFO: Training progress: {'loss': 0.3177, 'grad_norm': 1.7051280736923218, 'learning_rate': 5.876434709098527e-07, 'epoch': 4.377183471870859}
2025-04-29 08:13:04,267 - INFO - Training progress: {'loss': 0.3177, 'grad_norm': 1.7051280736923218, 'learning_rate': 5.876434709098527e-07, 'epoch': 4.377183471870859}
2025-04-29 08:13:04,267 - INFO - Training metrics: {'loss': 0.3177, 'grad_norm': 1.7051280736923218, 'learning_rate': 5.876434709098527e-07, 'epoch': 4.377183471870859}
2025-04-29 08:16:15,299 - INFO - INFO: Training progress: {'loss': 0.3265, 'grad_norm': 2.432436466217041, 'learning_rate': 5.697957673555859e-07, 'epoch': 4.38682086495603}
2025-04-29 08:16:15,299 - INFO - Training progress: {'loss': 0.3265, 'grad_norm': 2.432436466217041, 'learning_rate': 5.697957673555859e-07, 'epoch': 4.38682086495603}
2025-04-29 08:16:15,299 - INFO - Training metrics: {'loss': 0.3265, 'grad_norm': 2.432436466217041, 'learning_rate': 5.697957673555859e-07, 'epoch': 4.38682086495603}
2025-04-29 08:19:27,113 - INFO - INFO: Training progress: {'loss': 0.3533, 'grad_norm': 1.8809049129486084, 'learning_rate': 5.522125862186861e-07, 'epoch': 4.3964582580412}
2025-04-29 08:19:27,114 - INFO - Training progress: {'loss': 0.3533, 'grad_norm': 1.8809049129486084, 'learning_rate': 5.522125862186861e-07, 'epoch': 4.3964582580412}
2025-04-29 08:19:27,114 - INFO - Training metrics: {'loss': 0.3533, 'grad_norm': 1.8809049129486084, 'learning_rate': 5.522125862186861e-07, 'epoch': 4.3964582580412}
2025-04-29 08:22:38,446 - INFO - INFO: Training progress: {'loss': 0.3172, 'grad_norm': 1.7070297002792358, 'learning_rate': 5.348945986403508e-07, 'epoch': 4.40609565112637}
2025-04-29 08:22:38,447 - INFO - Training progress: {'loss': 0.3172, 'grad_norm': 1.7070297002792358, 'learning_rate': 5.348945986403508e-07, 'epoch': 4.40609565112637}
2025-04-29 08:22:38,447 - INFO - Training metrics: {'loss': 0.3172, 'grad_norm': 1.7070297002792358, 'learning_rate': 5.348945986403508e-07, 'epoch': 4.40609565112637}
2025-04-29 08:25:50,259 - INFO - INFO: Training progress: {'loss': 0.3069, 'grad_norm': 1.5557066202163696, 'learning_rate': 5.17842465639474e-07, 'epoch': 4.415733044211541}
2025-04-29 08:25:50,259 - INFO - Training progress: {'loss': 0.3069, 'grad_norm': 1.5557066202163696, 'learning_rate': 5.17842465639474e-07, 'epoch': 4.415733044211541}
2025-04-29 08:25:50,259 - INFO - Training metrics: {'loss': 0.3069, 'grad_norm': 1.5557066202163696, 'learning_rate': 5.17842465639474e-07, 'epoch': 4.415733044211541}
2025-04-29 08:29:02,092 - INFO - INFO: Training progress: {'loss': 0.3068, 'grad_norm': 1.7442982196807861, 'learning_rate': 5.010568380874139e-07, 'epoch': 4.425370437296711}
2025-04-29 08:29:02,092 - INFO - Training progress: {'loss': 0.3068, 'grad_norm': 1.7442982196807861, 'learning_rate': 5.010568380874139e-07, 'epoch': 4.425370437296711}
2025-04-29 08:29:02,092 - INFO - Training metrics: {'loss': 0.3068, 'grad_norm': 1.7442982196807861, 'learning_rate': 5.010568380874139e-07, 'epoch': 4.425370437296711}
2025-04-29 08:32:13,935 - INFO - INFO: Training progress: {'loss': 0.3556, 'grad_norm': 1.97244131565094, 'learning_rate': 4.845383566831488e-07, 'epoch': 4.435007830381882}
2025-04-29 08:32:13,935 - INFO - Training progress: {'loss': 0.3556, 'grad_norm': 1.97244131565094, 'learning_rate': 4.845383566831488e-07, 'epoch': 4.435007830381882}
2025-04-29 08:32:13,935 - INFO - Training metrics: {'loss': 0.3556, 'grad_norm': 1.97244131565094, 'learning_rate': 4.845383566831488e-07, 'epoch': 4.435007830381882}
2025-04-29 08:32:14,906 - INFO - INFO: Saving checkpoint at step 4600
2025-04-29 08:32:14,906 - INFO - Saving checkpoint at step 4600
2025-04-29 08:32:14,906 - INFO - Saving checkpoint at step 4600
2025-04-29 08:35:26,485 - INFO - INFO: Training progress: {'loss': 0.2991, 'grad_norm': 1.8119336366653442, 'learning_rate': 4.6828765192882565e-07, 'epoch': 4.444645223467052}
2025-04-29 08:35:26,485 - INFO - Training progress: {'loss': 0.2991, 'grad_norm': 1.8119336366653442, 'learning_rate': 4.6828765192882565e-07, 'epoch': 4.444645223467052}
2025-04-29 08:35:26,485 - INFO - Training metrics: {'loss': 0.2991, 'grad_norm': 1.8119336366653442, 'learning_rate': 4.6828765192882565e-07, 'epoch': 4.444645223467052}
2025-04-29 08:38:37,856 - INFO - INFO: Training progress: {'loss': 0.3177, 'grad_norm': 2.1075937747955322, 'learning_rate': 4.5230534410568764e-07, 'epoch': 4.454282616552223}
2025-04-29 08:38:37,856 - INFO - Training progress: {'loss': 0.3177, 'grad_norm': 2.1075937747955322, 'learning_rate': 4.5230534410568764e-07, 'epoch': 4.454282616552223}
2025-04-29 08:38:37,857 - INFO - Training metrics: {'loss': 0.3177, 'grad_norm': 2.1075937747955322, 'learning_rate': 4.5230534410568764e-07, 'epoch': 4.454282616552223}
2025-04-29 08:53:42,886 - INFO - INFO: Training progress: {'eval_loss': 0.3367793560028076, 'eval_runtime': 905.0264, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.454282616552223}
2025-04-29 08:53:42,887 - INFO - Training progress: {'eval_loss': 0.3367793560028076, 'eval_runtime': 905.0264, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.454282616552223}
2025-04-29 08:53:42,887 - INFO - Training metrics: {'eval_loss': 0.3367793560028076, 'eval_runtime': 905.0264, 'eval_samples_per_second': 1.718, 'eval_steps_per_second': 1.718, 'epoch': 4.454282616552223}
2025-04-29 08:56:54,922 - INFO - INFO: Training progress: {'loss': 0.317, 'grad_norm': 1.9493789672851562, 'learning_rate': 4.3659204325040334e-07, 'epoch': 4.463920009637393}
2025-04-29 08:56:54,922 - INFO - Training progress: {'loss': 0.317, 'grad_norm': 1.9493789672851562, 'learning_rate': 4.3659204325040334e-07, 'epoch': 4.463920009637393}
2025-04-29 08:56:54,922 - INFO - Training metrics: {'loss': 0.317, 'grad_norm': 1.9493789672851562, 'learning_rate': 4.3659204325040334e-07, 'epoch': 4.463920009637393}
2025-04-29 09:00:06,558 - INFO - INFO: Training progress: {'loss': 0.3535, 'grad_norm': 1.9549263715744019, 'learning_rate': 4.2114834913178e-07, 'epoch': 4.473557402722563}
2025-04-29 09:00:06,558 - INFO - Training progress: {'loss': 0.3535, 'grad_norm': 1.9549263715744019, 'learning_rate': 4.2114834913178e-07, 'epoch': 4.473557402722563}
2025-04-29 09:00:06,558 - INFO - Training metrics: {'loss': 0.3535, 'grad_norm': 1.9549263715744019, 'learning_rate': 4.2114834913178e-07, 'epoch': 4.473557402722563}
2025-04-29 09:03:17,880 - INFO - INFO: Training progress: {'loss': 0.3298, 'grad_norm': 1.7249912023544312, 'learning_rate': 4.0597485122786965e-07, 'epoch': 4.483194795807734}
2025-04-29 09:03:17,881 - INFO - Training progress: {'loss': 0.3298, 'grad_norm': 1.7249912023544312, 'learning_rate': 4.0597485122786965e-07, 'epoch': 4.483194795807734}
2025-04-29 09:03:17,881 - INFO - Training metrics: {'loss': 0.3298, 'grad_norm': 1.7249912023544312, 'learning_rate': 4.0597485122786965e-07, 'epoch': 4.483194795807734}
2025-04-29 09:06:29,208 - INFO - INFO: Training progress: {'loss': 0.3098, 'grad_norm': 2.0371103286743164, 'learning_rate': 3.9107212870347437e-07, 'epoch': 4.492832188892905}
2025-04-29 09:06:29,208 - INFO - Training progress: {'loss': 0.3098, 'grad_norm': 2.0371103286743164, 'learning_rate': 3.9107212870347437e-07, 'epoch': 4.492832188892905}
2025-04-29 09:06:29,208 - INFO - Training metrics: {'loss': 0.3098, 'grad_norm': 2.0371103286743164, 'learning_rate': 3.9107212870347437e-07, 'epoch': 4.492832188892905}
2025-04-29 09:09:40,816 - INFO - INFO: Training progress: {'loss': 0.3382, 'grad_norm': 2.396721839904785, 'learning_rate': 3.76440750388031e-07, 'epoch': 4.502469581978075}
2025-04-29 09:09:40,816 - INFO - Training progress: {'loss': 0.3382, 'grad_norm': 2.396721839904785, 'learning_rate': 3.76440750388031e-07, 'epoch': 4.502469581978075}
2025-04-29 09:09:40,817 - INFO - Training metrics: {'loss': 0.3382, 'grad_norm': 2.396721839904785, 'learning_rate': 3.76440750388031e-07, 'epoch': 4.502469581978075}
2025-04-29 09:12:52,624 - INFO - INFO: Training progress: {'loss': 0.3023, 'grad_norm': 1.4825191497802734, 'learning_rate': 3.6208127475390875e-07, 'epoch': 4.512106975063245}
2025-04-29 09:12:52,624 - INFO - Training progress: {'loss': 0.3023, 'grad_norm': 1.4825191497802734, 'learning_rate': 3.6208127475390875e-07, 'epoch': 4.512106975063245}
2025-04-29 09:12:52,624 - INFO - Training metrics: {'loss': 0.3023, 'grad_norm': 1.4825191497802734, 'learning_rate': 3.6208127475390875e-07, 'epoch': 4.512106975063245}
2025-04-29 09:16:04,152 - INFO - INFO: Training progress: {'loss': 0.3234, 'grad_norm': 2.1647796630859375, 'learning_rate': 3.479942498950853e-07, 'epoch': 4.521744368148416}
2025-04-29 09:16:04,153 - INFO - Training progress: {'loss': 0.3234, 'grad_norm': 2.1647796630859375, 'learning_rate': 3.479942498950853e-07, 'epoch': 4.521744368148416}
2025-04-29 09:16:04,153 - INFO - Training metrics: {'loss': 0.3234, 'grad_norm': 2.1647796630859375, 'learning_rate': 3.479942498950853e-07, 'epoch': 4.521744368148416}
2025-04-29 09:19:15,615 - INFO - INFO: Training progress: {'loss': 0.2893, 'grad_norm': 1.4332855939865112, 'learning_rate': 3.3418021350622773e-07, 'epoch': 4.531381761233586}
2025-04-29 09:19:15,615 - INFO - Training progress: {'loss': 0.2893, 'grad_norm': 1.4332855939865112, 'learning_rate': 3.3418021350622773e-07, 'epoch': 4.531381761233586}
2025-04-29 09:19:15,615 - INFO - Training metrics: {'loss': 0.2893, 'grad_norm': 1.4332855939865112, 'learning_rate': 3.3418021350622773e-07, 'epoch': 4.531381761233586}
2025-04-29 09:22:27,272 - INFO - INFO: Training progress: {'loss': 0.3263, 'grad_norm': 2.4769160747528076, 'learning_rate': 3.2063969286217407e-07, 'epoch': 4.541019154318757}
2025-04-29 09:22:27,272 - INFO - Training progress: {'loss': 0.3263, 'grad_norm': 2.4769160747528076, 'learning_rate': 3.2063969286217407e-07, 'epoch': 4.541019154318757}
2025-04-29 09:22:27,272 - INFO - Training metrics: {'loss': 0.3263, 'grad_norm': 2.4769160747528076, 'learning_rate': 3.2063969286217407e-07, 'epoch': 4.541019154318757}
2025-04-29 09:25:38,698 - INFO - INFO: Training progress: {'loss': 0.2937, 'grad_norm': 1.7046033143997192, 'learning_rate': 3.073732047977984e-07, 'epoch': 4.5506565474039276}
2025-04-29 09:25:38,698 - INFO - Training progress: {'loss': 0.2937, 'grad_norm': 1.7046033143997192, 'learning_rate': 3.073732047977984e-07, 'epoch': 4.5506565474039276}
2025-04-29 09:25:38,698 - INFO - Training metrics: {'loss': 0.2937, 'grad_norm': 1.7046033143997192, 'learning_rate': 3.073732047977984e-07, 'epoch': 4.5506565474039276}
2025-04-29 09:28:50,369 - INFO - INFO: Training progress: {'loss': 0.3288, 'grad_norm': 1.8347855806350708, 'learning_rate': 2.9438125568829576e-07, 'epoch': 4.560293940489098}
2025-04-29 09:28:50,370 - INFO - Training progress: {'loss': 0.3288, 'grad_norm': 1.8347855806350708, 'learning_rate': 2.9438125568829576e-07, 'epoch': 4.560293940489098}
2025-04-29 09:28:50,370 - INFO - Training metrics: {'loss': 0.3288, 'grad_norm': 1.8347855806350708, 'learning_rate': 2.9438125568829576e-07, 'epoch': 4.560293940489098}
2025-04-29 09:32:02,189 - INFO - INFO: Training progress: {'loss': 0.3407, 'grad_norm': 1.909644365310669, 'learning_rate': 2.8166434142984034e-07, 'epoch': 4.569931333574268}
2025-04-29 09:32:02,190 - INFO - Training progress: {'loss': 0.3407, 'grad_norm': 1.909644365310669, 'learning_rate': 2.8166434142984034e-07, 'epoch': 4.569931333574268}
2025-04-29 09:32:02,190 - INFO - Training metrics: {'loss': 0.3407, 'grad_norm': 1.909644365310669, 'learning_rate': 2.8166434142984034e-07, 'epoch': 4.569931333574268}
2025-04-29 09:35:13,938 - INFO - INFO: Training progress: {'loss': 0.3185, 'grad_norm': 1.7650705575942993, 'learning_rate': 2.692229474206678e-07, 'epoch': 4.5795687266594385}
2025-04-29 09:35:13,939 - INFO - Training progress: {'loss': 0.3185, 'grad_norm': 1.7650705575942993, 'learning_rate': 2.692229474206678e-07, 'epoch': 4.5795687266594385}
2025-04-29 09:35:13,939 - INFO - Training metrics: {'loss': 0.3185, 'grad_norm': 1.7650705575942993, 'learning_rate': 2.692229474206678e-07, 'epoch': 4.5795687266594385}
2025-04-29 09:38:25,442 - INFO - INFO: Training progress: {'loss': 0.2906, 'grad_norm': 2.051654577255249, 'learning_rate': 2.57057548542543e-07, 'epoch': 4.589206119744609}
2025-04-29 09:38:25,443 - INFO - Training progress: {'loss': 0.2906, 'grad_norm': 2.051654577255249, 'learning_rate': 2.57057548542543e-07, 'epoch': 4.589206119744609}
2025-04-29 09:38:25,443 - INFO - Training metrics: {'loss': 0.2906, 'grad_norm': 2.051654577255249, 'learning_rate': 2.57057548542543e-07, 'epoch': 4.589206119744609}
2025-04-29 09:41:39,293 - INFO - INFO: Training progress: {'loss': 0.316, 'grad_norm': 1.803717017173767, 'learning_rate': 2.45168609142635e-07, 'epoch': 4.598843512829779}
2025-04-29 09:41:39,293 - INFO - Training progress: {'loss': 0.316, 'grad_norm': 1.803717017173767, 'learning_rate': 2.45168609142635e-07, 'epoch': 4.598843512829779}
2025-04-29 09:41:39,293 - INFO - Training metrics: {'loss': 0.316, 'grad_norm': 1.803717017173767, 'learning_rate': 2.45168609142635e-07, 'epoch': 4.598843512829779}
2025-04-29 09:45:06,691 - INFO - INFO: Training progress: {'loss': 0.2822, 'grad_norm': 1.8769830465316772, 'learning_rate': 2.3355658301579542e-07, 'epoch': 4.60848090591495}
2025-04-29 09:45:06,691 - INFO - Training progress: {'loss': 0.2822, 'grad_norm': 1.8769830465316772, 'learning_rate': 2.3355658301579542e-07, 'epoch': 4.60848090591495}
2025-04-29 09:45:06,691 - INFO - Training metrics: {'loss': 0.2822, 'grad_norm': 1.8769830465316772, 'learning_rate': 2.3355658301579542e-07, 'epoch': 4.60848090591495}
2025-04-29 09:48:33,452 - INFO - INFO: Training progress: {'loss': 0.313, 'grad_norm': 1.8490101099014282, 'learning_rate': 2.2222191338723164e-07, 'epoch': 4.618118299000121}
2025-04-29 09:48:33,453 - INFO - Training progress: {'loss': 0.313, 'grad_norm': 1.8490101099014282, 'learning_rate': 2.2222191338723164e-07, 'epoch': 4.618118299000121}
2025-04-29 09:48:33,453 - INFO - Training metrics: {'loss': 0.313, 'grad_norm': 1.8490101099014282, 'learning_rate': 2.2222191338723164e-07, 'epoch': 4.618118299000121}
2025-04-29 09:52:03,555 - INFO - INFO: Training progress: {'loss': 0.3147, 'grad_norm': 2.32517409324646, 'learning_rate': 2.1116503289559608e-07, 'epoch': 4.627755692085291}
2025-04-29 09:52:03,555 - INFO - Training progress: {'loss': 0.3147, 'grad_norm': 2.32517409324646, 'learning_rate': 2.1116503289559608e-07, 'epoch': 4.627755692085291}
2025-04-29 09:52:03,555 - INFO - Training metrics: {'loss': 0.3147, 'grad_norm': 2.32517409324646, 'learning_rate': 2.1116503289559608e-07, 'epoch': 4.627755692085291}
2025-04-29 09:52:04,635 - INFO - INFO: Saving checkpoint at step 4800
2025-04-29 09:52:04,636 - INFO - Saving checkpoint at step 4800
2025-04-29 09:52:04,636 - INFO - Saving checkpoint at step 4800
2025-04-29 09:55:29,164 - INFO - INFO: Training progress: {'loss': 0.3165, 'grad_norm': 1.8011208772659302, 'learning_rate': 2.0038636357646773e-07, 'epoch': 4.637393085170461}
2025-04-29 09:55:29,164 - INFO - Training progress: {'loss': 0.3165, 'grad_norm': 1.8011208772659302, 'learning_rate': 2.0038636357646773e-07, 'epoch': 4.637393085170461}
2025-04-29 09:55:29,165 - INFO - Training metrics: {'loss': 0.3165, 'grad_norm': 1.8011208772659302, 'learning_rate': 2.0038636357646773e-07, 'epoch': 4.637393085170461}
2025-04-29 09:59:01,739 - INFO - INFO: Training progress: {'loss': 0.3265, 'grad_norm': 2.1230242252349854, 'learning_rate': 1.8988631684624335e-07, 'epoch': 4.6470304782556315}
2025-04-29 09:59:01,740 - INFO - Training progress: {'loss': 0.3265, 'grad_norm': 2.1230242252349854, 'learning_rate': 1.8988631684624335e-07, 'epoch': 4.6470304782556315}
2025-04-29 09:59:01,740 - INFO - Training metrics: {'loss': 0.3265, 'grad_norm': 2.1230242252349854, 'learning_rate': 1.8988631684624335e-07, 'epoch': 4.6470304782556315}
2025-04-29 10:02:24,775 - INFO - INFO: Training progress: {'loss': 0.2936, 'grad_norm': 1.8799244165420532, 'learning_rate': 1.7966529348643844e-07, 'epoch': 4.656667871340803}
2025-04-29 10:02:24,775 - INFO - Training progress: {'loss': 0.2936, 'grad_norm': 1.8799244165420532, 'learning_rate': 1.7966529348643844e-07, 'epoch': 4.656667871340803}
2025-04-29 10:02:24,775 - INFO - Training metrics: {'loss': 0.2936, 'grad_norm': 1.8799244165420532, 'learning_rate': 1.7966529348643844e-07, 'epoch': 4.656667871340803}
2025-04-29 10:17:33,151 - INFO - INFO: Training progress: {'eval_loss': 0.3362503945827484, 'eval_runtime': 908.3721, 'eval_samples_per_second': 1.712, 'eval_steps_per_second': 1.712, 'epoch': 4.656667871340803}
2025-04-29 10:17:33,151 - INFO - Training progress: {'eval_loss': 0.3362503945827484, 'eval_runtime': 908.3721, 'eval_samples_per_second': 1.712, 'eval_steps_per_second': 1.712, 'epoch': 4.656667871340803}
2025-04-29 10:17:33,151 - INFO - Training metrics: {'eval_loss': 0.3362503945827484, 'eval_runtime': 908.3721, 'eval_samples_per_second': 1.712, 'eval_steps_per_second': 1.712, 'epoch': 4.656667871340803}
2025-04-29 10:20:47,745 - INFO - INFO: Training progress: {'loss': 0.3001, 'grad_norm': 2.0542049407958984, 'learning_rate': 1.6972368362838275e-07, 'epoch': 4.666305264425973}
2025-04-29 10:20:47,745 - INFO - Training progress: {'loss': 0.3001, 'grad_norm': 2.0542049407958984, 'learning_rate': 1.6972368362838275e-07, 'epoch': 4.666305264425973}
2025-04-29 10:20:47,745 - INFO - Training metrics: {'loss': 0.3001, 'grad_norm': 2.0542049407958984, 'learning_rate': 1.6972368362838275e-07, 'epoch': 4.666305264425973}
2025-04-29 10:23:58,043 - INFO - INFO: Training progress: {'loss': 0.3168, 'grad_norm': 1.840254783630371, 'learning_rate': 1.6006186673833716e-07, 'epoch': 4.675942657511143}
2025-04-29 10:23:58,043 - INFO - Training progress: {'loss': 0.3168, 'grad_norm': 1.840254783630371, 'learning_rate': 1.6006186673833716e-07, 'epoch': 4.675942657511143}
2025-04-29 10:23:58,043 - INFO - Training metrics: {'loss': 0.3168, 'grad_norm': 1.840254783630371, 'learning_rate': 1.6006186673833716e-07, 'epoch': 4.675942657511143}
2025-04-29 10:27:08,029 - INFO - INFO: Training progress: {'loss': 0.3037, 'grad_norm': 1.8790203332901, 'learning_rate': 1.5068021160300288e-07, 'epoch': 4.685580050596314}
2025-04-29 10:27:08,029 - INFO - Training progress: {'loss': 0.3037, 'grad_norm': 1.8790203332901, 'learning_rate': 1.5068021160300288e-07, 'epoch': 4.685580050596314}
2025-04-29 10:27:08,029 - INFO - Training metrics: {'loss': 0.3037, 'grad_norm': 1.8790203332901, 'learning_rate': 1.5068021160300288e-07, 'epoch': 4.685580050596314}
2025-04-29 10:30:19,249 - INFO - INFO: Training progress: {'loss': 0.3095, 'grad_norm': 2.226567506790161, 'learning_rate': 1.4157907631544675e-07, 'epoch': 4.695217443681484}
2025-04-29 10:30:19,249 - INFO - Training progress: {'loss': 0.3095, 'grad_norm': 2.226567506790161, 'learning_rate': 1.4157907631544675e-07, 'epoch': 4.695217443681484}
2025-04-29 10:30:19,249 - INFO - Training metrics: {'loss': 0.3095, 'grad_norm': 2.226567506790161, 'learning_rate': 1.4157907631544675e-07, 'epoch': 4.695217443681484}
2025-04-29 10:33:29,663 - INFO - INFO: Training progress: {'loss': 0.3203, 'grad_norm': 2.0670177936553955, 'learning_rate': 1.3275880826143642e-07, 'epoch': 4.704854836766654}
2025-04-29 10:33:29,663 - INFO - Training progress: {'loss': 0.3203, 'grad_norm': 2.0670177936553955, 'learning_rate': 1.3275880826143642e-07, 'epoch': 4.704854836766654}
2025-04-29 10:33:29,663 - INFO - Training metrics: {'loss': 0.3203, 'grad_norm': 2.0670177936553955, 'learning_rate': 1.3275880826143642e-07, 'epoch': 4.704854836766654}
2025-04-29 10:36:40,108 - INFO - INFO: Training progress: {'loss': 0.3441, 'grad_norm': 1.9492332935333252, 'learning_rate': 1.2421974410617432e-07, 'epoch': 4.714492229851825}
2025-04-29 10:36:40,108 - INFO - Training progress: {'loss': 0.3441, 'grad_norm': 1.9492332935333252, 'learning_rate': 1.2421974410617432e-07, 'epoch': 4.714492229851825}
2025-04-29 10:36:40,108 - INFO - Training metrics: {'loss': 0.3441, 'grad_norm': 1.9492332935333252, 'learning_rate': 1.2421974410617432e-07, 'epoch': 4.714492229851825}
2025-04-29 10:39:50,215 - INFO - INFO: Training progress: {'loss': 0.3596, 'grad_norm': 2.403475284576416, 'learning_rate': 1.1596220978145611e-07, 'epoch': 4.724129622936996}
2025-04-29 10:39:50,215 - INFO - Training progress: {'loss': 0.3596, 'grad_norm': 2.403475284576416, 'learning_rate': 1.1596220978145611e-07, 'epoch': 4.724129622936996}
2025-04-29 10:39:50,215 - INFO - Training metrics: {'loss': 0.3596, 'grad_norm': 2.403475284576416, 'learning_rate': 1.1596220978145611e-07, 'epoch': 4.724129622936996}
2025-04-29 10:43:00,682 - INFO - INFO: Training progress: {'loss': 0.3387, 'grad_norm': 2.373251438140869, 'learning_rate': 1.0798652047322084e-07, 'epoch': 4.733767016022166}
2025-04-29 10:43:00,682 - INFO - Training progress: {'loss': 0.3387, 'grad_norm': 2.373251438140869, 'learning_rate': 1.0798652047322084e-07, 'epoch': 4.733767016022166}
2025-04-29 10:43:00,682 - INFO - Training metrics: {'loss': 0.3387, 'grad_norm': 2.373251438140869, 'learning_rate': 1.0798652047322084e-07, 'epoch': 4.733767016022166}
2025-04-29 10:46:10,615 - INFO - INFO: Training progress: {'loss': 0.3123, 'grad_norm': 1.7479740381240845, 'learning_rate': 1.002929806095279e-07, 'epoch': 4.743404409107336}
2025-04-29 10:46:10,615 - INFO - Training progress: {'loss': 0.3123, 'grad_norm': 1.7479740381240845, 'learning_rate': 1.002929806095279e-07, 'epoch': 4.743404409107336}
2025-04-29 10:46:10,615 - INFO - Training metrics: {'loss': 0.3123, 'grad_norm': 1.7479740381240845, 'learning_rate': 1.002929806095279e-07, 'epoch': 4.743404409107336}
2025-04-29 10:49:20,429 - INFO - INFO: Training progress: {'loss': 0.3303, 'grad_norm': 1.6468331813812256, 'learning_rate': 9.288188384893231e-08, 'epoch': 4.753041802192507}
2025-04-29 10:49:20,430 - INFO - Training progress: {'loss': 0.3303, 'grad_norm': 1.6468331813812256, 'learning_rate': 9.288188384893231e-08, 'epoch': 4.753041802192507}
2025-04-29 10:49:20,430 - INFO - Training metrics: {'loss': 0.3303, 'grad_norm': 1.6468331813812256, 'learning_rate': 9.288188384893231e-08, 'epoch': 4.753041802192507}
2025-04-29 10:52:30,478 - INFO - INFO: Training progress: {'loss': 0.3097, 'grad_norm': 1.851346731185913, 'learning_rate': 8.575351306927686e-08, 'epoch': 4.762679195277677}
2025-04-29 10:52:30,478 - INFO - Training progress: {'loss': 0.3097, 'grad_norm': 1.851346731185913, 'learning_rate': 8.575351306927686e-08, 'epoch': 4.762679195277677}
2025-04-29 10:52:30,478 - INFO - Training metrics: {'loss': 0.3097, 'grad_norm': 1.851346731185913, 'learning_rate': 8.575351306927686e-08, 'epoch': 4.762679195277677}
2025-04-29 10:55:40,860 - INFO - INFO: Training progress: {'loss': 0.3314, 'grad_norm': 2.0666213035583496, 'learning_rate': 7.890814035689753e-08, 'epoch': 4.772316588362848}
2025-04-29 10:55:40,861 - INFO - Training progress: {'loss': 0.3314, 'grad_norm': 2.0666213035583496, 'learning_rate': 7.890814035689753e-08, 'epoch': 4.772316588362848}
2025-04-29 10:55:40,861 - INFO - Training metrics: {'loss': 0.3314, 'grad_norm': 2.0666213035583496, 'learning_rate': 7.890814035689753e-08, 'epoch': 4.772316588362848}
2025-04-29 10:58:51,201 - INFO - INFO: Training progress: {'loss': 0.3154, 'grad_norm': 1.8403369188308716, 'learning_rate': 7.234602699623422e-08, 'epoch': 4.7819539814480185}
2025-04-29 10:58:51,201 - INFO - Training progress: {'loss': 0.3154, 'grad_norm': 1.8403369188308716, 'learning_rate': 7.234602699623422e-08, 'epoch': 4.7819539814480185}
2025-04-29 10:58:51,202 - INFO - Training metrics: {'loss': 0.3154, 'grad_norm': 1.8403369188308716, 'learning_rate': 7.234602699623422e-08, 'epoch': 4.7819539814480185}
2025-04-29 11:02:01,624 - INFO - INFO: Training progress: {'loss': 0.347, 'grad_norm': 2.246767044067383, 'learning_rate': 6.606742345986127e-08, 'epoch': 4.791591374533189}
2025-04-29 11:02:01,624 - INFO - Training progress: {'loss': 0.347, 'grad_norm': 2.246767044067383, 'learning_rate': 6.606742345986127e-08, 'epoch': 4.791591374533189}
2025-04-29 11:02:01,625 - INFO - Training metrics: {'loss': 0.347, 'grad_norm': 2.246767044067383, 'learning_rate': 6.606742345986127e-08, 'epoch': 4.791591374533189}
2025-04-29 11:05:12,215 - INFO - INFO: Training progress: {'loss': 0.3756, 'grad_norm': 1.9690196514129639, 'learning_rate': 6.00725693989243e-08, 'epoch': 4.801228767618359}
2025-04-29 11:05:12,215 - INFO - Training progress: {'loss': 0.3756, 'grad_norm': 1.9690196514129639, 'learning_rate': 6.00725693989243e-08, 'epoch': 4.801228767618359}
2025-04-29 11:05:12,215 - INFO - Training metrics: {'loss': 0.3756, 'grad_norm': 1.9690196514129639, 'learning_rate': 6.00725693989243e-08, 'epoch': 4.801228767618359}
2025-04-29 11:08:22,698 - INFO - INFO: Training progress: {'loss': 0.3387, 'grad_norm': 1.5252366065979004, 'learning_rate': 5.436169363399413e-08, 'epoch': 4.8108661607035295}
2025-04-29 11:08:22,699 - INFO - Training progress: {'loss': 0.3387, 'grad_norm': 1.5252366065979004, 'learning_rate': 5.436169363399413e-08, 'epoch': 4.8108661607035295}
2025-04-29 11:08:22,699 - INFO - Training metrics: {'loss': 0.3387, 'grad_norm': 1.5252366065979004, 'learning_rate': 5.436169363399413e-08, 'epoch': 4.8108661607035295}
2025-04-29 11:11:32,870 - INFO - INFO: Training progress: {'loss': 0.3185, 'grad_norm': 1.9546831846237183, 'learning_rate': 4.8935014146332114e-08, 'epoch': 4.8205035537887}
2025-04-29 11:11:32,870 - INFO - Training progress: {'loss': 0.3185, 'grad_norm': 1.9546831846237183, 'learning_rate': 4.8935014146332114e-08, 'epoch': 4.8205035537887}
2025-04-29 11:11:32,870 - INFO - Training metrics: {'loss': 0.3185, 'grad_norm': 1.9546831846237183, 'learning_rate': 4.8935014146332114e-08, 'epoch': 4.8205035537887}
2025-04-29 11:11:33,826 - INFO - INFO: Saving checkpoint at step 5000
2025-04-29 11:11:33,826 - INFO - Saving checkpoint at step 5000
2025-04-29 11:11:33,826 - INFO - Saving checkpoint at step 5000
2025-04-29 11:14:44,221 - INFO - INFO: Training progress: {'loss': 0.324, 'grad_norm': 1.536105751991272, 'learning_rate': 4.3792738069571014e-08, 'epoch': 4.830140946873871}
2025-04-29 11:14:44,221 - INFO - Training progress: {'loss': 0.324, 'grad_norm': 1.536105751991272, 'learning_rate': 4.3792738069571014e-08, 'epoch': 4.830140946873871}
2025-04-29 11:14:44,221 - INFO - Training metrics: {'loss': 0.324, 'grad_norm': 1.536105751991272, 'learning_rate': 4.3792738069571014e-08, 'epoch': 4.830140946873871}
2025-04-29 11:17:54,445 - INFO - INFO: Training progress: {'loss': 0.3463, 'grad_norm': 1.8710566759109497, 'learning_rate': 3.8935061681807935e-08, 'epoch': 4.839778339959041}
2025-04-29 11:17:54,445 - INFO - Training progress: {'loss': 0.3463, 'grad_norm': 1.8710566759109497, 'learning_rate': 3.8935061681807935e-08, 'epoch': 4.839778339959041}
2025-04-29 11:17:54,445 - INFO - Training metrics: {'loss': 0.3463, 'grad_norm': 1.8710566759109497, 'learning_rate': 3.8935061681807935e-08, 'epoch': 4.839778339959041}
2025-04-29 11:21:04,675 - INFO - INFO: Training progress: {'loss': 0.2953, 'grad_norm': 1.687036156654358, 'learning_rate': 3.436217039811201e-08, 'epoch': 4.849415733044212}
2025-04-29 11:21:04,675 - INFO - Training progress: {'loss': 0.2953, 'grad_norm': 1.687036156654358, 'learning_rate': 3.436217039811201e-08, 'epoch': 4.849415733044212}
2025-04-29 11:21:04,675 - INFO - Training metrics: {'loss': 0.2953, 'grad_norm': 1.687036156654358, 'learning_rate': 3.436217039811201e-08, 'epoch': 4.849415733044212}
2025-04-29 11:24:14,977 - INFO - INFO: Training progress: {'loss': 0.3292, 'grad_norm': 1.8037517070770264, 'learning_rate': 3.007423876344922e-08, 'epoch': 4.859053126129382}
2025-04-29 11:24:14,977 - INFO - Training progress: {'loss': 0.3292, 'grad_norm': 1.8037517070770264, 'learning_rate': 3.007423876344922e-08, 'epoch': 4.859053126129382}
2025-04-29 11:24:14,977 - INFO - Training metrics: {'loss': 0.3292, 'grad_norm': 1.8037517070770264, 'learning_rate': 3.007423876344922e-08, 'epoch': 4.859053126129382}
2025-04-29 11:39:08,143 - INFO - INFO: Training progress: {'eval_loss': 0.33604466915130615, 'eval_runtime': 893.1637, 'eval_samples_per_second': 1.741, 'eval_steps_per_second': 1.741, 'epoch': 4.859053126129382}
2025-04-29 11:39:08,144 - INFO - Training progress: {'eval_loss': 0.33604466915130615, 'eval_runtime': 893.1637, 'eval_samples_per_second': 1.741, 'eval_steps_per_second': 1.741, 'epoch': 4.859053126129382}
2025-04-29 11:39:08,144 - INFO - Training metrics: {'eval_loss': 0.33604466915130615, 'eval_runtime': 893.1637, 'eval_samples_per_second': 1.741, 'eval_steps_per_second': 1.741, 'epoch': 4.859053126129382}
2025-04-29 11:42:18,067 - INFO - INFO: Training progress: {'loss': 0.3175, 'grad_norm': 1.8481535911560059, 'learning_rate': 2.607143044601773e-08, 'epoch': 4.868690519214552}
2025-04-29 11:42:18,067 - INFO - Training progress: {'loss': 0.3175, 'grad_norm': 1.8481535911560059, 'learning_rate': 2.607143044601773e-08, 'epoch': 4.868690519214552}
2025-04-29 11:42:18,067 - INFO - Training metrics: {'loss': 0.3175, 'grad_norm': 1.8481535911560059, 'learning_rate': 2.607143044601773e-08, 'epoch': 4.868690519214552}
2025-04-29 11:45:28,340 - INFO - INFO: Training progress: {'loss': 0.3446, 'grad_norm': 1.985181450843811, 'learning_rate': 2.2353898231002035e-08, 'epoch': 4.8783279122997225}
2025-04-29 11:45:28,340 - INFO - Training progress: {'loss': 0.3446, 'grad_norm': 1.985181450843811, 'learning_rate': 2.2353898231002035e-08, 'epoch': 4.8783279122997225}
2025-04-29 11:45:28,340 - INFO - Training metrics: {'loss': 0.3446, 'grad_norm': 1.985181450843811, 'learning_rate': 2.2353898231002035e-08, 'epoch': 4.8783279122997225}
2025-04-29 11:48:38,577 - INFO - INFO: Training progress: {'loss': 0.2891, 'grad_norm': 2.0455574989318848, 'learning_rate': 1.892178401474015e-08, 'epoch': 4.887965305384894}
2025-04-29 11:48:38,577 - INFO - Training progress: {'loss': 0.2891, 'grad_norm': 2.0455574989318848, 'learning_rate': 1.892178401474015e-08, 'epoch': 4.887965305384894}
2025-04-29 11:48:38,577 - INFO - Training metrics: {'loss': 0.2891, 'grad_norm': 2.0455574989318848, 'learning_rate': 1.892178401474015e-08, 'epoch': 4.887965305384894}
2025-04-29 11:51:48,989 - INFO - INFO: Training progress: {'loss': 0.2981, 'grad_norm': 2.856595039367676, 'learning_rate': 1.577521879931043e-08, 'epoch': 4.897602698470064}
2025-04-29 11:51:48,989 - INFO - Training progress: {'loss': 0.2981, 'grad_norm': 2.856595039367676, 'learning_rate': 1.577521879931043e-08, 'epoch': 4.897602698470064}
2025-04-29 11:51:48,989 - INFO - Training metrics: {'loss': 0.2981, 'grad_norm': 2.856595039367676, 'learning_rate': 1.577521879931043e-08, 'epoch': 4.897602698470064}
2025-04-29 11:54:59,433 - INFO - INFO: Training progress: {'loss': 0.3268, 'grad_norm': 2.050170660018921, 'learning_rate': 1.2914322687527236e-08, 'epoch': 4.907240091555234}
2025-04-29 11:54:59,433 - INFO - Training progress: {'loss': 0.3268, 'grad_norm': 2.050170660018921, 'learning_rate': 1.2914322687527236e-08, 'epoch': 4.907240091555234}
2025-04-29 11:54:59,433 - INFO - Training metrics: {'loss': 0.3268, 'grad_norm': 2.050170660018921, 'learning_rate': 1.2914322687527236e-08, 'epoch': 4.907240091555234}
2025-04-29 11:58:09,921 - INFO - INFO: Training progress: {'loss': 0.3067, 'grad_norm': 1.4779871702194214, 'learning_rate': 1.033920487836043e-08, 'epoch': 4.916877484640405}
2025-04-29 11:58:09,921 - INFO - Training progress: {'loss': 0.3067, 'grad_norm': 1.4779871702194214, 'learning_rate': 1.033920487836043e-08, 'epoch': 4.916877484640405}
2025-04-29 11:58:09,921 - INFO - Training metrics: {'loss': 0.3067, 'grad_norm': 1.4779871702194214, 'learning_rate': 1.033920487836043e-08, 'epoch': 4.916877484640405}
2025-04-29 12:01:20,538 - INFO - INFO: Training progress: {'loss': 0.3111, 'grad_norm': 2.391282320022583, 'learning_rate': 8.04996366276206e-09, 'epoch': 4.926514877725575}
2025-04-29 12:01:20,538 - INFO - Training progress: {'loss': 0.3111, 'grad_norm': 2.391282320022583, 'learning_rate': 8.04996366276206e-09, 'epoch': 4.926514877725575}
2025-04-29 12:01:20,538 - INFO - Training metrics: {'loss': 0.3111, 'grad_norm': 2.391282320022583, 'learning_rate': 8.04996366276206e-09, 'epoch': 4.926514877725575}
2025-04-29 12:04:30,660 - INFO - INFO: Training progress: {'loss': 0.369, 'grad_norm': 2.0689730644226074, 'learning_rate': 6.046686419922675e-09, 'epoch': 4.936152270810746}
2025-04-29 12:04:30,660 - INFO - Training progress: {'loss': 0.369, 'grad_norm': 2.0689730644226074, 'learning_rate': 6.046686419922675e-09, 'epoch': 4.936152270810746}
2025-04-29 12:04:30,660 - INFO - Training metrics: {'loss': 0.369, 'grad_norm': 2.0689730644226074, 'learning_rate': 6.046686419922675e-09, 'epoch': 4.936152270810746}
2025-04-29 12:07:40,880 - INFO - INFO: Training progress: {'loss': 0.2903, 'grad_norm': 1.6097025871276855, 'learning_rate': 4.329449613927339e-09, 'epoch': 4.9457896638959165}
2025-04-29 12:07:40,880 - INFO - Training progress: {'loss': 0.2903, 'grad_norm': 1.6097025871276855, 'learning_rate': 4.329449613927339e-09, 'epoch': 4.9457896638959165}
2025-04-29 12:07:40,880 - INFO - Training metrics: {'loss': 0.2903, 'grad_norm': 1.6097025871276855, 'learning_rate': 4.329449613927339e-09, 'epoch': 4.9457896638959165}
2025-04-29 12:10:50,951 - INFO - INFO: Training progress: {'loss': 0.3079, 'grad_norm': 2.0786056518554688, 'learning_rate': 2.898318790844623e-09, 'epoch': 4.955427056981087}
2025-04-29 12:10:50,951 - INFO - Training progress: {'loss': 0.3079, 'grad_norm': 2.0786056518554688, 'learning_rate': 2.898318790844623e-09, 'epoch': 4.955427056981087}
2025-04-29 12:10:50,951 - INFO - Training metrics: {'loss': 0.3079, 'grad_norm': 2.0786056518554688, 'learning_rate': 2.898318790844623e-09, 'epoch': 4.955427056981087}
2025-04-29 12:14:01,034 - INFO - INFO: Training progress: {'loss': 0.3168, 'grad_norm': 2.3452484607696533, 'learning_rate': 1.753348576219449e-09, 'epoch': 4.965064450066257}
2025-04-29 12:14:01,034 - INFO - Training progress: {'loss': 0.3168, 'grad_norm': 2.3452484607696533, 'learning_rate': 1.753348576219449e-09, 'epoch': 4.965064450066257}
2025-04-29 12:14:01,034 - INFO - Training metrics: {'loss': 0.3168, 'grad_norm': 2.3452484607696533, 'learning_rate': 1.753348576219449e-09, 'epoch': 4.965064450066257}
2025-04-29 12:17:11,362 - INFO - INFO: Training progress: {'loss': 0.3387, 'grad_norm': 1.9096589088439941, 'learning_rate': 8.945826729897521e-10, 'epoch': 4.974701843151427}
2025-04-29 12:17:11,362 - INFO - Training progress: {'loss': 0.3387, 'grad_norm': 1.9096589088439941, 'learning_rate': 8.945826729897521e-10, 'epoch': 4.974701843151427}
2025-04-29 12:17:11,362 - INFO - Training metrics: {'loss': 0.3387, 'grad_norm': 1.9096589088439941, 'learning_rate': 8.945826729897521e-10, 'epoch': 4.974701843151427}
2025-04-29 12:20:21,941 - INFO - INFO: Training progress: {'loss': 0.303, 'grad_norm': 1.9012329578399658, 'learning_rate': 3.220538598194822e-10, 'epoch': 4.984339236236598}
2025-04-29 12:20:21,941 - INFO - Training progress: {'loss': 0.303, 'grad_norm': 1.9012329578399658, 'learning_rate': 3.220538598194822e-10, 'epoch': 4.984339236236598}
2025-04-29 12:20:21,941 - INFO - Training metrics: {'loss': 0.303, 'grad_norm': 1.9012329578399658, 'learning_rate': 3.220538598194822e-10, 'epoch': 4.984339236236598}
2025-04-29 12:23:31,637 - INFO - INFO: Training progress: {'loss': 0.3127, 'grad_norm': 2.217454433441162, 'learning_rate': 3.5783989845439294e-11, 'epoch': 4.993976629321768}
2025-04-29 12:23:31,637 - INFO - Training progress: {'loss': 0.3127, 'grad_norm': 2.217454433441162, 'learning_rate': 3.5783989845439294e-11, 'epoch': 4.993976629321768}
2025-04-29 12:23:31,637 - INFO - Training metrics: {'loss': 0.3127, 'grad_norm': 2.217454433441162, 'learning_rate': 3.5783989845439294e-11, 'epoch': 4.993976629321768}
2025-04-29 12:25:07,469 - INFO - INFO: Saving checkpoint at step 5185
2025-04-29 12:25:07,469 - INFO - Saving checkpoint at step 5185
2025-04-29 12:25:07,469 - INFO - Saving checkpoint at step 5185
2025-04-29 12:25:07,469 - INFO - INFO: Training progress: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:07,470 - INFO - Training progress: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:07,470 - INFO - Training metrics: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:08,139 - INFO - INFO: Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\final_model
2025-04-29 12:25:08,139 - INFO - Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\final_model
2025-04-29 12:25:08,139 - INFO - Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\final_model
2025-04-29 12:25:08,709 - INFO - INFO: CUDA cache cleared
2025-04-29 12:25:08,834 - INFO - INFO: Garbage collector freed 1763 objects
2025-04-29 12:25:08,835 - INFO - INFO: Training completed successfully! Model saved to: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_supervised_5\final_model
2025-04-29 12:25:08,835 - INFO - INFO: Training metrics: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:08,835 - INFO - INFO: Final training metrics: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:08,836 - INFO - Final training metrics: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:08,836 - INFO - Final training metrics: {'train_runtime': 46490.0789, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.112, 'total_flos': 6.027148249792512e+18, 'train_loss': 0.1260578737194536, 'epoch': 4.998795325864354}
2025-04-29 12:25:08,836 - INFO - INFO: Running final evaluation on test dataset...
2025-04-29 12:25:08,836 - INFO - Running final evaluation on test dataset...
2025-04-29 12:25:08,836 - INFO - Running final evaluation on test dataset...
2025-04-29 12:30:13,159 - INFO - INFO: Training progress: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,159 - INFO - Training progress: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,159 - INFO - Training metrics: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,186 - INFO - INFO: CUDA cache cleared
2025-04-29 12:30:13,312 - INFO - INFO: Garbage collector freed 9 objects
2025-04-29 12:30:13,312 - INFO - INFO: Final test metrics: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,313 - INFO - INFO: Final test metrics: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,313 - INFO - Final test metrics: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,313 - INFO - Final test metrics: {'eval_loss': 0.32241225242614746, 'eval_runtime': 304.3195, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 1.715, 'epoch': 4.998795325864354}
2025-04-29 12:30:13,313 - INFO - Training complete!
