2025-06-02 02:37:35,203 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\logging.txt
2025-06-02 02:37:35,203 - INFO - 


Starting new Fine-tuning run:


2025-06-02 02:37:35,203 - INFO - INFO: File logger setup to write to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\logging.txt
2025-06-02 02:37:35,203 - INFO - Starting supervised fine-tuning with parameters: {'mode': 'supervised', 'data_path': 'N:/Thesis/data_prepare/datasets_ready/supervised_coherence/combined_datasets', 'text_column': 'input', 'use_checkpoint': False, 'checkpoint_path': None, 'max_samples': None, 'pre_eval': True, 'freeze_partly': False, 'freeze_partly_layers': 0, 'unfreeze_specific': True, 'unfreeze_specific_layers': [8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25], 'eval_split': 0.05, 'model_path': 'C:/Users/Paul/.cache/merged_models/llama3_german_merged_V3_1', 'output_dir': 'C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2', 'logging_dir': None, 'use_flash_attention': True, 'max_length': 3100, 'chunk_size': None, 'quantization_config': {'load_in_8bit': True}, 'peft_config': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'inference_mode': False, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'w1', 'w2', 'w3']}, 'training_config': {'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 1, 'learning_rate': 5e-06, 'warmup_steps': 100, 'warmup_ratio': 0.15, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 5, 'eval_strategy': 'steps', 'eval_steps': 50, 'per_device_eval_batch_size': 1, 'eval_accumulation_steps': 4, 'fp16': True, 'bf16': False, 'lr_scheduler_type': 'constant_with_warmup', 'weight_decay': 0.005, 'gradient_checkpointing': True, 'report_to': 'none', 'disable_tqdm': False, 'max_grad_norm': 0.3, 'dataloader_num_workers': 2, 'optim': 'adamw_torch'}}
2025-06-02 02:37:35,203 - INFO - INFO: Loading datasets from individual JSONL files
2025-06-02 02:37:35,203 - INFO - Loading datasets from individual JSONL files
2025-06-02 02:37:35,289 - INFO - INFO: Loaded 2100 examples from N:/Thesis/data_prepare/datasets_ready/supervised_coherence/combined_datasets\training_set.jsonl
2025-06-02 02:37:35,363 - INFO - INFO: Loaded 540 examples from N:/Thesis/data_prepare/datasets_ready/supervised_coherence/combined_datasets\validation_set.jsonl
2025-06-02 02:37:35,375 - INFO - INFO: Loaded separate validation set with 540 examples
2025-06-02 02:37:35,376 - INFO - Loaded separate validation set with 540 examples
2025-06-02 02:37:35,394 - INFO - INFO: Loaded 360 examples from N:/Thesis/data_prepare/datasets_ready/supervised_coherence/combined_datasets\test_set.jsonl
2025-06-02 02:37:35,405 - INFO - INFO: Loaded separate test set with 360 examples
2025-06-02 02:37:35,405 - INFO - Loaded separate test set with 360 examples
2025-06-02 02:37:35,405 - INFO - INFO: Supervised format detected, combining input and output columns
2025-06-02 02:37:35,405 - INFO - Supervised format detected, combining input and output columns
2025-06-02 02:37:35,997 - INFO - INFO: Tokenizer vocabulary size: 128256
2025-06-02 02:37:36,002 - INFO - INFO: Model max length: 1000000000000000019884624838656
2025-06-02 02:37:38,524 - INFO - INFO: Dataset prepared with 2100 examples
2025-06-02 02:37:39,281 - INFO - INFO: Dataset prepared with 540 examples
2025-06-02 02:37:39,838 - INFO - INFO: Dataset prepared with 360 examples
2025-06-02 02:37:39,838 - INFO - INFO: CUDA cache cleared
2025-06-02 02:37:39,976 - INFO - INFO: Garbage collector freed 51 objects
2025-06-02 02:37:48,641 - INFO - INFO: Total transformer layers in model: 32
2025-06-02 02:37:48,641 - INFO - INFO: Keeping only specified layers trainable: [8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25]
2025-06-02 02:37:48,641 - INFO - INFO: Freezing layer 0/32
2025-06-02 02:37:48,641 - INFO - INFO: Freezing layer 1/32
2025-06-02 02:37:48,641 - INFO - INFO: Freezing layer 2/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 3/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 4/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 5/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 6/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 7/32
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 8/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 9/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 10/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 11/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 12/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 13/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 14/32 trainable
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 15/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 16/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 17/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 18/32
2025-06-02 02:37:48,645 - INFO - INFO: Freezing layer 19/32
2025-06-02 02:37:48,645 - INFO - INFO: Keeping layer 20/32 trainable
2025-06-02 02:37:48,650 - INFO - INFO: Keeping layer 21/32 trainable
2025-06-02 02:37:48,651 - INFO - INFO: Keeping layer 22/32 trainable
2025-06-02 02:37:48,651 - INFO - INFO: Keeping layer 23/32 trainable
2025-06-02 02:37:48,651 - INFO - INFO: Keeping layer 24/32 trainable
2025-06-02 02:37:48,651 - INFO - INFO: Keeping layer 25/32 trainable
2025-06-02 02:37:48,651 - INFO - INFO: Freezing layer 26/32
2025-06-02 02:37:48,651 - INFO - INFO: Freezing layer 27/32
2025-06-02 02:37:48,652 - INFO - INFO: Freezing layer 28/32
2025-06-02 02:37:48,652 - INFO - INFO: Freezing layer 29/32
2025-06-02 02:37:48,652 - INFO - INFO: Freezing layer 30/32
2025-06-02 02:37:48,653 - INFO - INFO: Freezing layer 31/32
2025-06-02 02:37:48,653 - INFO - INFO: Ensuring LM head is trainable
2025-06-02 02:37:48,655 - INFO - INFO: Selective layer training: 7504924672/8030261248 parameters frozen (93.46%)
2025-06-02 02:37:48,655 - INFO - INFO: Trainable: 525336576/8030261248 parameters (6.54%)
2025-06-02 02:37:48,656 - INFO - INFO: Trainable layers: [8, 9, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25] out of 32 layers plus LM head
2025-06-02 02:37:48,656 - INFO - INFO: Model loaded from C:/Users/Paul/.cache/merged_models/llama3_german_merged_V3_1
2025-06-02 02:37:48,656 - INFO - INFO: Model has 8030261248 parameters, 525336576 are trainable (6.54%)
2025-06-02 02:37:49,031 - INFO - INFO: Model has 20971520 trainable parameters after PEFT configuration
2025-06-02 02:37:49,033 - INFO - INFO: Using fp16 (float16) mixed precision training
2025-06-02 02:37:49,093 - INFO - Starting model training with 2100 training examples
2025-06-02 02:37:49,093 - INFO - Using 540 examples for validation during training
2025-06-02 02:37:49,093 - INFO - Using 360 examples for pre/final evaluation
2025-06-02 02:37:49,111 - INFO - INFO: Testing evaluation with current settings...
2025-06-02 02:37:49,112 - INFO - INFO: Test dataset size: 360
2025-06-02 02:37:49,127 - INFO - INFO: CUDA cache cleared
2025-06-02 02:37:49,251 - INFO - INFO: Garbage collector freed 50 objects
2025-06-02 02:41:30,321 - INFO - INFO: Training progress: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,321 - INFO - Training progress: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,321 - INFO - Training metrics: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,331 - INFO - INFO: CUDA cache cleared
2025-06-02 02:41:30,462 - INFO - INFO: Garbage collector freed 19 objects
2025-06-02 02:41:30,462 - INFO - INFO: 
Evaluation successful!
2025-06-02 02:41:30,462 - INFO - INFO: Metrics: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,462 - INFO - INFO: Pre-training evaluation successful. Metrics: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,462 - INFO - Pre-training evaluation successful. Metrics: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,462 - INFO - Pre-training evaluation metrics: {'eval_loss': 0.7259912490844727, 'eval_model_preparation_time': 0.0, 'eval_runtime': 221.0702, 'eval_samples_per_second': 1.628, 'eval_steps_per_second': 1.628}
2025-06-02 02:41:30,462 - INFO - INFO: 
GPU Memory Summary:
2025-06-02 02:41:30,462 - INFO - INFO: Allocated: 10.54 GB
2025-06-02 02:41:30,462 - INFO - INFO: Cached: 10.67 GB
2025-06-02 02:41:30,462 - INFO - INFO: GPU Memory: Allocated 10.54 GB, Cached 10.67 GB
2025-06-02 02:41:30,462 - INFO - GPU Memory: Allocated 10.54 GB, Cached 10.67 GB
2025-06-02 02:41:30,462 - INFO - GPU Memory: Allocated 10.54 GB, Cached 10.67 GB
2025-06-02 02:41:30,462 - INFO - INFO: Starting training...
2025-06-02 02:41:30,462 - INFO - Starting training...
2025-06-02 02:41:30,462 - INFO - Starting training...
2025-06-02 02:41:30,462 - INFO - INFO: Registering numpy component classes as safe globals
2025-06-02 02:41:30,462 - INFO - INFO: Registering module names in the PyTorch safe registry
2025-06-02 02:41:30,462 - INFO - WARNING: torch.serialization.safe_registry not available, using alternative registration
2025-06-02 02:41:30,462 - INFO - INFO: Registering specific numpy components
2025-06-02 02:41:30,469 - INFO - INFO: Registering numpy array creation patterns
2025-06-02 02:41:30,469 - INFO - INFO: Set up safe dtype handler for array reconstruction
2025-06-02 02:41:30,469 - INFO - INFO: Numpy components registered as safe globals
2025-06-02 02:41:30,653 - INFO - INFO: Starting epoch 0/1
2025-06-02 02:41:30,653 - INFO - Starting epoch 0/1
2025-06-02 02:41:30,653 - INFO - Starting epoch 0/1
2025-06-02 02:44:46,404 - INFO - INFO: Training progress: {'loss': 0.7114, 'grad_norm': 0.7957862615585327, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0380952380952381}
2025-06-02 02:44:46,404 - INFO - Training progress: {'loss': 0.7114, 'grad_norm': 0.7957862615585327, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0380952380952381}
2025-06-02 02:44:46,404 - INFO - Training metrics: {'loss': 0.7114, 'grad_norm': 0.7957862615585327, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0380952380952381}
2025-06-02 02:47:57,316 - INFO - INFO: Training progress: {'loss': 0.6862, 'grad_norm': 0.6281007528305054, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0761904761904762}
2025-06-02 02:47:57,316 - INFO - Training progress: {'loss': 0.6862, 'grad_norm': 0.6281007528305054, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0761904761904762}
2025-06-02 02:47:57,316 - INFO - Training metrics: {'loss': 0.6862, 'grad_norm': 0.6281007528305054, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0761904761904762}
2025-06-02 02:51:10,938 - INFO - INFO: Training progress: {'loss': 0.6522, 'grad_norm': 0.48638901114463806, 'learning_rate': 1.5e-06, 'epoch': 0.11428571428571428}
2025-06-02 02:51:10,939 - INFO - Training progress: {'loss': 0.6522, 'grad_norm': 0.48638901114463806, 'learning_rate': 1.5e-06, 'epoch': 0.11428571428571428}
2025-06-02 02:51:10,939 - INFO - Training metrics: {'loss': 0.6522, 'grad_norm': 0.48638901114463806, 'learning_rate': 1.5e-06, 'epoch': 0.11428571428571428}
2025-06-02 02:54:29,024 - INFO - INFO: Training progress: {'loss': 0.6578, 'grad_norm': 0.5696474313735962, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1523809523809524}
2025-06-02 02:54:29,024 - INFO - Training progress: {'loss': 0.6578, 'grad_norm': 0.5696474313735962, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1523809523809524}
2025-06-02 02:54:29,024 - INFO - Training metrics: {'loss': 0.6578, 'grad_norm': 0.5696474313735962, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1523809523809524}
2025-06-02 02:57:43,551 - INFO - INFO: Training progress: {'loss': 0.6912, 'grad_norm': 0.7362064123153687, 'learning_rate': 2.5e-06, 'epoch': 0.19047619047619047}
2025-06-02 02:57:43,552 - INFO - Training progress: {'loss': 0.6912, 'grad_norm': 0.7362064123153687, 'learning_rate': 2.5e-06, 'epoch': 0.19047619047619047}
2025-06-02 02:57:43,552 - INFO - Training metrics: {'loss': 0.6912, 'grad_norm': 0.7362064123153687, 'learning_rate': 2.5e-06, 'epoch': 0.19047619047619047}
2025-06-02 03:03:10,485 - INFO - INFO: Training progress: {'eval_loss': 0.6741752624511719, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.9306, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 0.19047619047619047}
2025-06-02 03:03:10,486 - INFO - Training progress: {'eval_loss': 0.6741752624511719, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.9306, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 0.19047619047619047}
2025-06-02 03:03:10,486 - INFO - Training metrics: {'eval_loss': 0.6741752624511719, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.9306, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 0.19047619047619047}
2025-06-02 03:06:23,244 - INFO - INFO: Training progress: {'loss': 0.6518, 'grad_norm': 0.7563308477401733, 'learning_rate': 3e-06, 'epoch': 0.22857142857142856}
2025-06-02 03:06:23,244 - INFO - Training progress: {'loss': 0.6518, 'grad_norm': 0.7563308477401733, 'learning_rate': 3e-06, 'epoch': 0.22857142857142856}
2025-06-02 03:06:23,244 - INFO - Training metrics: {'loss': 0.6518, 'grad_norm': 0.7563308477401733, 'learning_rate': 3e-06, 'epoch': 0.22857142857142856}
2025-06-02 03:09:37,060 - INFO - INFO: Training progress: {'loss': 0.6438, 'grad_norm': 0.5021886229515076, 'learning_rate': 3.5e-06, 'epoch': 0.26666666666666666}
2025-06-02 03:09:37,060 - INFO - Training progress: {'loss': 0.6438, 'grad_norm': 0.5021886229515076, 'learning_rate': 3.5e-06, 'epoch': 0.26666666666666666}
2025-06-02 03:09:37,060 - INFO - Training metrics: {'loss': 0.6438, 'grad_norm': 0.5021886229515076, 'learning_rate': 3.5e-06, 'epoch': 0.26666666666666666}
2025-06-02 03:13:00,984 - INFO - INFO: Training progress: {'loss': 0.5914, 'grad_norm': 0.47478216886520386, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.3047619047619048}
2025-06-02 03:13:00,984 - INFO - Training progress: {'loss': 0.5914, 'grad_norm': 0.47478216886520386, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.3047619047619048}
2025-06-02 03:13:00,984 - INFO - Training metrics: {'loss': 0.5914, 'grad_norm': 0.47478216886520386, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.3047619047619048}
2025-06-02 03:16:26,533 - INFO - INFO: Training progress: {'loss': 0.5743, 'grad_norm': 0.43913397192955017, 'learning_rate': 4.5e-06, 'epoch': 0.34285714285714286}
2025-06-02 03:16:26,533 - INFO - Training progress: {'loss': 0.5743, 'grad_norm': 0.43913397192955017, 'learning_rate': 4.5e-06, 'epoch': 0.34285714285714286}
2025-06-02 03:16:26,533 - INFO - Training metrics: {'loss': 0.5743, 'grad_norm': 0.43913397192955017, 'learning_rate': 4.5e-06, 'epoch': 0.34285714285714286}
2025-06-02 03:19:47,570 - INFO - INFO: Training progress: {'loss': 0.5508, 'grad_norm': 0.42791393399238586, 'learning_rate': 5e-06, 'epoch': 0.38095238095238093}
2025-06-02 03:19:47,570 - INFO - Training progress: {'loss': 0.5508, 'grad_norm': 0.42791393399238586, 'learning_rate': 5e-06, 'epoch': 0.38095238095238093}
2025-06-02 03:19:47,570 - INFO - Training metrics: {'loss': 0.5508, 'grad_norm': 0.42791393399238586, 'learning_rate': 5e-06, 'epoch': 0.38095238095238093}
2025-06-02 03:25:14,070 - INFO - INFO: Training progress: {'eval_loss': 0.5360752940177917, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.4997, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 0.38095238095238093}
2025-06-02 03:25:14,070 - INFO - Training progress: {'eval_loss': 0.5360752940177917, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.4997, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 0.38095238095238093}
2025-06-02 03:25:14,070 - INFO - Training metrics: {'eval_loss': 0.5360752940177917, 'eval_model_preparation_time': 0.0, 'eval_runtime': 326.4997, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 0.38095238095238093}
2025-06-02 03:25:14,561 - INFO - INFO: Saving checkpoint at step 100
2025-06-02 03:25:14,561 - INFO - Saving checkpoint at step 100
2025-06-02 03:25:14,561 - INFO - Saving checkpoint at step 100
2025-06-02 03:28:29,982 - INFO - INFO: Training progress: {'loss': 0.5197, 'grad_norm': 0.38259267807006836, 'learning_rate': 5e-06, 'epoch': 0.41904761904761906}
2025-06-02 03:28:29,982 - INFO - Training progress: {'loss': 0.5197, 'grad_norm': 0.38259267807006836, 'learning_rate': 5e-06, 'epoch': 0.41904761904761906}
2025-06-02 03:28:29,982 - INFO - Training metrics: {'loss': 0.5197, 'grad_norm': 0.38259267807006836, 'learning_rate': 5e-06, 'epoch': 0.41904761904761906}
2025-06-02 03:31:43,798 - INFO - INFO: Training progress: {'loss': 0.5381, 'grad_norm': 0.4930500090122223, 'learning_rate': 5e-06, 'epoch': 0.45714285714285713}
2025-06-02 03:31:43,798 - INFO - Training progress: {'loss': 0.5381, 'grad_norm': 0.4930500090122223, 'learning_rate': 5e-06, 'epoch': 0.45714285714285713}
2025-06-02 03:31:43,798 - INFO - Training metrics: {'loss': 0.5381, 'grad_norm': 0.4930500090122223, 'learning_rate': 5e-06, 'epoch': 0.45714285714285713}
2025-06-02 03:35:08,182 - INFO - INFO: Training progress: {'loss': 0.5489, 'grad_norm': 0.3639441430568695, 'learning_rate': 5e-06, 'epoch': 0.49523809523809526}
2025-06-02 03:35:08,182 - INFO - Training progress: {'loss': 0.5489, 'grad_norm': 0.3639441430568695, 'learning_rate': 5e-06, 'epoch': 0.49523809523809526}
2025-06-02 03:35:08,182 - INFO - Training metrics: {'loss': 0.5489, 'grad_norm': 0.3639441430568695, 'learning_rate': 5e-06, 'epoch': 0.49523809523809526}
2025-06-02 03:38:19,610 - INFO - INFO: Training progress: {'loss': 0.4767, 'grad_norm': 0.37687748670578003, 'learning_rate': 5e-06, 'epoch': 0.5333333333333333}
2025-06-02 03:38:19,610 - INFO - Training progress: {'loss': 0.4767, 'grad_norm': 0.37687748670578003, 'learning_rate': 5e-06, 'epoch': 0.5333333333333333}
2025-06-02 03:38:19,610 - INFO - Training metrics: {'loss': 0.4767, 'grad_norm': 0.37687748670578003, 'learning_rate': 5e-06, 'epoch': 0.5333333333333333}
2025-06-02 03:41:30,522 - INFO - INFO: Training progress: {'loss': 0.509, 'grad_norm': 0.39374423027038574, 'learning_rate': 5e-06, 'epoch': 0.5714285714285714}
2025-06-02 03:41:30,523 - INFO - Training progress: {'loss': 0.509, 'grad_norm': 0.39374423027038574, 'learning_rate': 5e-06, 'epoch': 0.5714285714285714}
2025-06-02 03:41:30,523 - INFO - Training metrics: {'loss': 0.509, 'grad_norm': 0.39374423027038574, 'learning_rate': 5e-06, 'epoch': 0.5714285714285714}
2025-06-02 03:46:46,677 - INFO - INFO: Training progress: {'eval_loss': 0.4805620610713959, 'eval_model_preparation_time': 0.0, 'eval_runtime': 316.1508, 'eval_samples_per_second': 1.708, 'eval_steps_per_second': 1.708, 'epoch': 0.5714285714285714}
2025-06-02 03:46:46,677 - INFO - Training progress: {'eval_loss': 0.4805620610713959, 'eval_model_preparation_time': 0.0, 'eval_runtime': 316.1508, 'eval_samples_per_second': 1.708, 'eval_steps_per_second': 1.708, 'epoch': 0.5714285714285714}
2025-06-02 03:46:46,677 - INFO - Training metrics: {'eval_loss': 0.4805620610713959, 'eval_model_preparation_time': 0.0, 'eval_runtime': 316.1508, 'eval_samples_per_second': 1.708, 'eval_steps_per_second': 1.708, 'epoch': 0.5714285714285714}
2025-06-02 03:49:57,744 - INFO - INFO: Training progress: {'loss': 0.502, 'grad_norm': 0.36424800753593445, 'learning_rate': 5e-06, 'epoch': 0.6095238095238096}
2025-06-02 03:49:57,745 - INFO - Training progress: {'loss': 0.502, 'grad_norm': 0.36424800753593445, 'learning_rate': 5e-06, 'epoch': 0.6095238095238096}
2025-06-02 03:49:57,745 - INFO - Training metrics: {'loss': 0.502, 'grad_norm': 0.36424800753593445, 'learning_rate': 5e-06, 'epoch': 0.6095238095238096}
2025-06-02 03:53:07,443 - INFO - INFO: Training progress: {'loss': 0.5196, 'grad_norm': 0.3552367389202118, 'learning_rate': 5e-06, 'epoch': 0.6476190476190476}
2025-06-02 03:53:07,443 - INFO - Training progress: {'loss': 0.5196, 'grad_norm': 0.3552367389202118, 'learning_rate': 5e-06, 'epoch': 0.6476190476190476}
2025-06-02 03:53:07,443 - INFO - Training metrics: {'loss': 0.5196, 'grad_norm': 0.3552367389202118, 'learning_rate': 5e-06, 'epoch': 0.6476190476190476}
2025-06-02 03:56:18,669 - INFO - INFO: Training progress: {'loss': 0.4572, 'grad_norm': 0.3936401307582855, 'learning_rate': 5e-06, 'epoch': 0.6857142857142857}
2025-06-02 03:56:18,669 - INFO - Training progress: {'loss': 0.4572, 'grad_norm': 0.3936401307582855, 'learning_rate': 5e-06, 'epoch': 0.6857142857142857}
2025-06-02 03:56:18,669 - INFO - Training metrics: {'loss': 0.4572, 'grad_norm': 0.3936401307582855, 'learning_rate': 5e-06, 'epoch': 0.6857142857142857}
2025-06-02 03:59:29,472 - INFO - INFO: Training progress: {'loss': 0.483, 'grad_norm': 0.4061882495880127, 'learning_rate': 5e-06, 'epoch': 0.7238095238095238}
2025-06-02 03:59:29,472 - INFO - Training progress: {'loss': 0.483, 'grad_norm': 0.4061882495880127, 'learning_rate': 5e-06, 'epoch': 0.7238095238095238}
2025-06-02 03:59:29,472 - INFO - Training metrics: {'loss': 0.483, 'grad_norm': 0.4061882495880127, 'learning_rate': 5e-06, 'epoch': 0.7238095238095238}
2025-06-02 04:02:42,000 - INFO - INFO: Training progress: {'loss': 0.4945, 'grad_norm': 0.45331618189811707, 'learning_rate': 5e-06, 'epoch': 0.7619047619047619}
2025-06-02 04:02:42,000 - INFO - Training progress: {'loss': 0.4945, 'grad_norm': 0.45331618189811707, 'learning_rate': 5e-06, 'epoch': 0.7619047619047619}
2025-06-02 04:02:42,000 - INFO - Training metrics: {'loss': 0.4945, 'grad_norm': 0.45331618189811707, 'learning_rate': 5e-06, 'epoch': 0.7619047619047619}
2025-06-02 04:07:59,541 - INFO - INFO: Training progress: {'eval_loss': 0.4606260657310486, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.5409, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.7619047619047619}
2025-06-02 04:07:59,541 - INFO - Training progress: {'eval_loss': 0.4606260657310486, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.5409, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.7619047619047619}
2025-06-02 04:07:59,541 - INFO - Training metrics: {'eval_loss': 0.4606260657310486, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.5409, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.7619047619047619}
2025-06-02 04:08:00,053 - INFO - INFO: Saving checkpoint at step 200
2025-06-02 04:08:00,053 - INFO - Saving checkpoint at step 200
2025-06-02 04:08:00,053 - INFO - Saving checkpoint at step 200
2025-06-02 04:11:11,935 - INFO - INFO: Training progress: {'loss': 0.4774, 'grad_norm': 0.43458276987075806, 'learning_rate': 5e-06, 'epoch': 0.8}
2025-06-02 04:11:11,935 - INFO - Training progress: {'loss': 0.4774, 'grad_norm': 0.43458276987075806, 'learning_rate': 5e-06, 'epoch': 0.8}
2025-06-02 04:11:11,935 - INFO - Training metrics: {'loss': 0.4774, 'grad_norm': 0.43458276987075806, 'learning_rate': 5e-06, 'epoch': 0.8}
2025-06-02 04:14:23,143 - INFO - INFO: Training progress: {'loss': 0.4356, 'grad_norm': 0.4342232942581177, 'learning_rate': 5e-06, 'epoch': 0.8380952380952381}
2025-06-02 04:14:23,143 - INFO - Training progress: {'loss': 0.4356, 'grad_norm': 0.4342232942581177, 'learning_rate': 5e-06, 'epoch': 0.8380952380952381}
2025-06-02 04:14:23,143 - INFO - Training metrics: {'loss': 0.4356, 'grad_norm': 0.4342232942581177, 'learning_rate': 5e-06, 'epoch': 0.8380952380952381}
2025-06-02 04:17:35,897 - INFO - INFO: Training progress: {'loss': 0.4565, 'grad_norm': 0.4816364049911499, 'learning_rate': 5e-06, 'epoch': 0.8761904761904762}
2025-06-02 04:17:35,897 - INFO - Training progress: {'loss': 0.4565, 'grad_norm': 0.4816364049911499, 'learning_rate': 5e-06, 'epoch': 0.8761904761904762}
2025-06-02 04:17:35,897 - INFO - Training metrics: {'loss': 0.4565, 'grad_norm': 0.4816364049911499, 'learning_rate': 5e-06, 'epoch': 0.8761904761904762}
2025-06-02 04:20:47,255 - INFO - INFO: Training progress: {'loss': 0.48, 'grad_norm': 0.3939545452594757, 'learning_rate': 5e-06, 'epoch': 0.9142857142857143}
2025-06-02 04:20:47,255 - INFO - Training progress: {'loss': 0.48, 'grad_norm': 0.3939545452594757, 'learning_rate': 5e-06, 'epoch': 0.9142857142857143}
2025-06-02 04:20:47,255 - INFO - Training metrics: {'loss': 0.48, 'grad_norm': 0.3939545452594757, 'learning_rate': 5e-06, 'epoch': 0.9142857142857143}
2025-06-02 04:23:56,483 - INFO - INFO: Training progress: {'loss': 0.4517, 'grad_norm': 0.3857594132423401, 'learning_rate': 5e-06, 'epoch': 0.9523809523809523}
2025-06-02 04:23:56,483 - INFO - Training progress: {'loss': 0.4517, 'grad_norm': 0.3857594132423401, 'learning_rate': 5e-06, 'epoch': 0.9523809523809523}
2025-06-02 04:23:56,483 - INFO - Training metrics: {'loss': 0.4517, 'grad_norm': 0.3857594132423401, 'learning_rate': 5e-06, 'epoch': 0.9523809523809523}
2025-06-02 04:29:13,877 - INFO - INFO: Training progress: {'eval_loss': 0.44975602626800537, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.3778, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.9523809523809523}
2025-06-02 04:29:13,877 - INFO - Training progress: {'eval_loss': 0.44975602626800537, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.3778, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.9523809523809523}
2025-06-02 04:29:13,877 - INFO - Training metrics: {'eval_loss': 0.44975602626800537, 'eval_model_preparation_time': 0.0, 'eval_runtime': 317.3778, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 1.701, 'epoch': 0.9523809523809523}
2025-06-02 04:32:25,954 - INFO - INFO: Training progress: {'loss': 0.4282, 'grad_norm': 0.6704389452934265, 'learning_rate': 5e-06, 'epoch': 0.9904761904761905}
2025-06-02 04:32:25,954 - INFO - Training progress: {'loss': 0.4282, 'grad_norm': 0.6704389452934265, 'learning_rate': 5e-06, 'epoch': 0.9904761904761905}
2025-06-02 04:32:25,954 - INFO - Training metrics: {'loss': 0.4282, 'grad_norm': 0.6704389452934265, 'learning_rate': 5e-06, 'epoch': 0.9904761904761905}
2025-06-02 04:33:05,132 - INFO - INFO: Saving checkpoint at step 262
2025-06-02 04:33:05,132 - INFO - Saving checkpoint at step 262
2025-06-02 04:33:05,132 - INFO - Saving checkpoint at step 262
2025-06-02 04:33:05,132 - INFO - INFO: Training progress: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:05,132 - INFO - Training progress: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:05,132 - INFO - Training metrics: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:05,747 - INFO - INFO: Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\final_model
2025-06-02 04:33:05,748 - INFO - Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\final_model
2025-06-02 04:33:05,748 - INFO - Training complete, saving model to C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\final_model
2025-06-02 04:33:06,191 - INFO - INFO: CUDA cache cleared
2025-06-02 04:33:06,326 - INFO - INFO: Garbage collector freed 1775 objects
2025-06-02 04:33:06,326 - INFO - INFO: Training completed successfully! Model saved to: C:/Users/Paul/.cache/training_output/checkpoints_llama3_german_V3_2\final_model
2025-06-02 04:33:06,326 - INFO - INFO: Training metrics: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:06,327 - INFO - INFO: Final training metrics: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:06,327 - INFO - Final training metrics: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:06,327 - INFO - Final training metrics: {'train_runtime': 6694.485, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 2.934015785828352e+17, 'train_loss': 0.5455646938040056, 'epoch': 0.9980952380952381}
2025-06-02 04:33:06,327 - INFO - INFO: Running final evaluation on test dataset...
2025-06-02 04:33:06,327 - INFO - Running final evaluation on test dataset...
2025-06-02 04:33:06,327 - INFO - Running final evaluation on test dataset...
2025-06-02 04:36:41,762 - INFO - INFO: Training progress: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,762 - INFO - Training progress: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,762 - INFO - Training metrics: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,763 - INFO - INFO: CUDA cache cleared
2025-06-02 04:36:41,910 - INFO - INFO: Garbage collector freed 9 objects
2025-06-02 04:36:41,910 - INFO - INFO: Final test metrics: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,910 - INFO - INFO: Final test metrics: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,911 - INFO - Final test metrics: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,911 - INFO - Final test metrics: {'eval_loss': 0.44940245151519775, 'eval_model_preparation_time': 0.0, 'eval_runtime': 215.4337, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.9980952380952381}
2025-06-02 04:36:41,911 - INFO - Training complete!
